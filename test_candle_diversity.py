#!/usr/bin/env python
"""ìº”ë“¤ ë‹¤ì–‘ì„± í…ŒìŠ¤íŠ¸ - ê° ì „ëµì´ ë‹¤ë¥¸ ìº”ë“¤ ì‚¬ìš©í•˜ëŠ”ì§€ ê²€ì¦"""
import sys
sys.path.append('/workspace')

import os
import sqlite3
import pandas as pd
from collections import Counter

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì • (1 ì—í”¼ì†Œë“œë§Œ)
os.environ['PREDICTIVE_SELFPLAY_EPISODES'] = '1'
os.environ['PREDICTIVE_SELFPLAY_MIN_EPISODES'] = '1'

print("=" * 80)
print("ìº”ë“¤ ë‹¤ì–‘ì„± í…ŒìŠ¤íŠ¸")
print("=" * 80)
print()

from rl_pipeline.pipelines.orchestrator import IntegratedPipelineOrchestrator

# Orchestrator ì´ˆê¸°í™”
orch = IntegratedPipelineOrchestrator(session_id="test_candle_diversity")

# ADA-15m 1 ì—í”¼ì†Œë“œë§Œ ì‹¤í–‰
print("ğŸ“Š ADA-15m ì˜ˆì¸¡ Self-play ì‹œì‘ (1 ì—í”¼ì†Œë“œ)")
print()

try:
    results = orch.run_predictive_selfplay_for_coin(
        coin="ADA",
        intervals=["15m"]
    )

    print()
    print("=" * 80)
    print("âœ… Self-play ì™„ë£Œ")
    print("=" * 80)
    print()

    if results and results.get('success'):
        print(f"  - í‰ê·  ì •í™•ë„: {results.get('avg_accuracy', 0)*100:.1f}%")
        print(f"  - ì—í”¼ì†Œë“œ ìˆ˜: {results.get('total_episodes', 0)}")

    print()
    print("=" * 80)
    print("ğŸ“Š DB ê²€ì¦ ì‹œì‘")
    print("=" * 80)
    print()

    # DB ì—°ê²°
    db_path = '/workspace/data_storage/rl_strategies.db'
    conn = sqlite3.connect(db_path)

    # 1. ìµœê·¼ ì˜ˆì¸¡ ë°ì´í„° ì¡°íšŒ
    query = """
    SELECT
        episode_id,
        strategy_id,
        entry_price,
        predicted_conf,
        horizon_k,
        ts_entry
    FROM rl_episodes
    WHERE coin = 'ADA' AND interval = '15m'
    ORDER BY ts_entry DESC
    LIMIT 200
    """

    df = pd.read_sql_query(query, conn)

    print(f"1. ì¡°íšŒëœ ì˜ˆì¸¡ ìˆ˜: {len(df)}ê°œ")
    print()

    # 2. entry_price ë‹¤ì–‘ì„± ê²€ì¦
    unique_prices = df['entry_price'].nunique()
    price_counts = Counter(df['entry_price'].values)

    print(f"2. entry_price ë‹¤ì–‘ì„±:")
    print(f"   - ê³ ìœ  ê°€ê²© ìˆ˜: {unique_prices}ê°œ (ì „ì²´ {len(df)}ê°œ ì¤‘)")
    print(f"   - ë‹¤ì–‘ì„± ë¹„ìœ¨: {unique_prices / len(df) * 100:.1f}%")
    print()

    if unique_prices <= 2:
        print("   âŒ ì‹¤íŒ¨: entry_priceê°€ ë„ˆë¬´ ì ìŒ (ëŒ€ë¶€ë¶„ ë™ì¼)")
        print(f"   - ê°€ê²© ë¶„í¬: {dict(list(price_counts.most_common(5)))}")
    elif unique_prices < len(df) * 0.3:
        print("   âš ï¸ ê²½ê³ : entry_price ë‹¤ì–‘ì„± ë¶€ì¡±")
        print(f"   - ê°€ê²© ë¶„í¬ (ìƒìœ„ 10ê°œ): {dict(list(price_counts.most_common(10)))}")
    else:
        print("   âœ… í†µê³¼: entry_priceê°€ ë‹¤ì–‘í•¨")
        print(f"   - ê°€ê²© ë²”ìœ„: {df['entry_price'].min():.4f} ~ {df['entry_price'].max():.4f}")
        print(f"   - ìƒìœ„ 5ê°œ ê°€ê²©: {dict(list(price_counts.most_common(5)))}")

    print()

    # 3. timestamp ë‹¤ì–‘ì„± ê²€ì¦
    unique_timestamps = df['ts_entry'].nunique()
    ts_counts = Counter(df['ts_entry'].values)

    print(f"3. timestamp ë‹¤ì–‘ì„±:")
    print(f"   - ê³ ìœ  íƒ€ì„ìŠ¤íƒ¬í”„ ìˆ˜: {unique_timestamps}ê°œ (ì „ì²´ {len(df)}ê°œ ì¤‘)")
    print(f"   - ë‹¤ì–‘ì„± ë¹„ìœ¨: {unique_timestamps / len(df) * 100:.1f}%")
    print()

    if unique_timestamps <= 2:
        print("   âŒ ì‹¤íŒ¨: timestampê°€ ë„ˆë¬´ ì ìŒ (ëŒ€ë¶€ë¶„ ë™ì¼)")
        print(f"   - íƒ€ì„ìŠ¤íƒ¬í”„ ë¶„í¬: {dict(list(ts_counts.most_common(5)))}")
    elif unique_timestamps < len(df) * 0.3:
        print("   âš ï¸ ê²½ê³ : timestamp ë‹¤ì–‘ì„± ë¶€ì¡±")
        print(f"   - íƒ€ì„ìŠ¤íƒ¬í”„ ë¶„í¬ (ìƒìœ„ 10ê°œ): {dict(list(ts_counts.most_common(10)))}")
    else:
        print("   âœ… í†µê³¼: timestampê°€ ë‹¤ì–‘í•¨")
        # ì‹œê°„ ê°„ê²© ê³„ì‚°
        timestamps_sorted = sorted(df['ts_entry'].unique())
        if len(timestamps_sorted) > 1:
            time_diffs = [timestamps_sorted[i+1] - timestamps_sorted[i] for i in range(len(timestamps_sorted)-1)]
            avg_diff = sum(time_diffs) / len(time_diffs)
            print(f"   - í‰ê·  ì‹œê°„ ê°„ê²©: {avg_diff / 60:.1f}ë¶„")
            print(f"   - ìµœì†Œ/ìµœëŒ€ ê°„ê²©: {min(time_diffs) / 60:.1f}ë¶„ / {max(time_diffs) / 60:.1f}ë¶„")

    print()

    # 4. ì „ëµë³„ ë¶„ì‚°ë„ í™•ì¸
    strategy_price_diversity = df.groupby('strategy_id')['entry_price'].nunique()

    print(f"4. ì „ëµë³„ ê°€ê²© ë‹¤ì–‘ì„±:")
    print(f"   - í‰ê·  ê³ ìœ  ê°€ê²© ìˆ˜/ì „ëµ: {strategy_price_diversity.mean():.2f}")

    if strategy_price_diversity.mean() > 1.0:
        print("   âš ï¸ ì£¼ì˜: ê°™ì€ ì „ëµì´ ì—¬ëŸ¬ ê°€ê²© ì‚¬ìš© (ì—í”¼ì†Œë“œë§ˆë‹¤ ë‹¤ë¥¸ ìº”ë“¤?)")
    else:
        print("   âœ… ì •ìƒ: ê° ì „ëµì´ 1ê°œ ê°€ê²© ì‚¬ìš© (ì—í”¼ì†Œë“œ ë‚´ì—ì„œ ì¼ê´€ì„± ìœ ì§€)")

    print()

    # 5. ìµœì¢… ê²°ê³¼ ìš”ì•½
    query_summary = """
    SELECT
        MIN(entry_price) as min_price,
        MAX(entry_price) as max_price,
        AVG(entry_price) as avg_price,
        MIN(ts_entry) as min_ts,
        MAX(ts_entry) as max_ts
    FROM rl_episodes
    WHERE coin = 'ADA' AND interval = '15m'
    """

    summary = pd.read_sql_query(query_summary, conn)

    print("=" * 80)
    print("ğŸ“Š ìµœì¢… ê²€ì¦ ê²°ê³¼")
    print("=" * 80)
    print()
    print(f"ê°€ê²© ë²”ìœ„: {summary['min_price'].iloc[0]:.4f} ~ {summary['max_price'].iloc[0]:.4f}")
    print(f"íƒ€ì„ìŠ¤íƒ¬í”„ ë²”ìœ„: {pd.to_datetime(summary['min_ts'].iloc[0], unit='s')} ~ {pd.to_datetime(summary['max_ts'].iloc[0], unit='s')}")
    print()

    # ì „ì²´ í‰ê°€
    if unique_prices >= len(df) * 0.3 and unique_timestamps >= len(df) * 0.3:
        print("âœ… ì „ì²´ í†µê³¼: ìº”ë“¤ ë‹¤ì–‘ì„± í™•ë³´ë¨")
    elif unique_prices <= 2 or unique_timestamps <= 2:
        print("âŒ ì „ì²´ ì‹¤íŒ¨: ìº”ë“¤ ë‹¤ì–‘ì„± ì—†ìŒ (ëŒ€ë¶€ë¶„ ë™ì¼í•œ ìº”ë“¤ ì‚¬ìš©)")
    else:
        print("âš ï¸ ë¶€ë¶„ í†µê³¼: ì¼ë¶€ ë‹¤ì–‘ì„± ìˆìœ¼ë‚˜ ê°œì„  í•„ìš”")

    print("=" * 80)

    conn.close()

except Exception as e:
    print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
    import traceback
    traceback.print_exc()
    sys.exit(1)
