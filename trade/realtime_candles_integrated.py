"""
ì´ íŒŒì¼ì€ ì „ëµì§€í‘œ(ê¸°ìˆ /íŒŒë™/íŒ¨í„´)ë¥¼ DBì˜ OHLCV+ê¸°ì´ˆì§€í‘œë§Œìœ¼ë¡œ ì§ì ‘ ê³„ì‚°í•˜ì—¬
íŒŒë™, íŒ¨í„´, í”„ë™íƒˆ, í†µí•©ë¶„ì„ ì»¬ëŸ¼(wave_step, structure_score, pattern_class, volatility_level, risk_level, integrated_direction, integrated_strength ë“±)
ëª¨ë‘ë¥¼ ê°±ì‹ í•©ë‹ˆë‹¤.

ì‹¤í–‰ ì‹œ DBì˜ ëª¨ë“  ì½”ì¸/ì¸í„°ë²Œì— ëŒ€í•´ zigzag/wave/pattern/í”„ë™íƒˆ/í†µí•©ë¶„ì„ì„ ìë™ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
"""
import sqlite3
import pandas as pd
import numpy as np
from collections import Counter
from typing import List, Tuple, Dict, Optional
from dataclasses import dataclass
from enum import Enum
import traceback

# pandas FutureWarning í•´ê²°ì„ ìœ„í•œ ì„¤ì •
pd.set_option('future.no_silent_downcasting', True)

# ë°ì´í„°ë² ì´ìŠ¤ ê²½ë¡œ ì„¤ì •
DB_PATH = "/workspace/data_storage/realtime_candles.db"

# ğŸš€ ë ˆì§ ê³„ì‚° ìƒìˆ˜ ì •ì˜
REGIME_STAGES = {
    1: "extreme_bearish",    # RSI < 20, ê¸‰ê²©í•œ í•˜ë½
    2: "bearish",           # RSI 20-40, í•˜ë½ ì¶”ì„¸
    3: "sideways_bearish",  # RSI 40-50, ì•½í•œ í•˜ë½
    4: "neutral",           # RSI 45-55, íš¡ë³´
    5: "sideways_bullish",  # RSI 50-60, ì•½í•œ ìƒìŠ¹
    6: "bullish",           # RSI 60-80, ìƒìŠ¹ ì¶”ì„¸
    7: "extreme_bullish"    # RSI > 80, ê¸‰ê²©í•œ ìƒìŠ¹
}

REGIME_LABELS = {v: k for k, v in REGIME_STAGES.items()}

# ì¸í„°ë²Œë³„ ë ˆì§ ê³„ì‚° ê¸°ì¤€
REGIME_CRITERIA = {
    '15m': {
        'rsi_weight': 0.4,      # ê· í˜•
        'macd_weight': 0.3,
        'volume_weight': 0.3,
        'volatility_threshold': 0.025,
        'lookback_period': 15
    },
    '30m': {
        'rsi_weight': 0.5,      # RSI ì‹ ë¢°ë„ ë†’ìŒ
        'macd_weight': 0.3,
        'volume_weight': 0.2,
        'volatility_threshold': 0.02,
        'lookback_period': 20
    },
    '240m': {
        'rsi_weight': 0.6,      # ê°€ì¥ ì‹ ë¢°ë„ ë†’ìŒ
        'macd_weight': 0.2,
        'volume_weight': 0.2,
        'volatility_threshold': 0.015,
        'lookback_period': 30,
        'is_primary': True      # ë©”ì¸ ë ˆì§ ê²°ì •ì
    },
    '1d': {
        'rsi_weight': 0.7,
        'macd_weight': 0.2,
        'volume_weight': 0.1,
        'volatility_threshold': 0.01,
        'lookback_period': 60
    }
}

# ë ˆì§ ì•ˆì •í™” ê¸°ë³¸ íŒŒë¼ë¯¸í„° (ê°œì„ : ë¯¼ê°ë„ ì¦ê°€)
REGIME_MIN_STAY = 2            # ìµœì†Œ ì²´ë¥˜ ìº”ë“¤ ìˆ˜ (3 â†’ 2ë¡œ ì™„í™”)
REGIME_CONF_GATE = 0.4         # ì „í™˜ í—ˆìš© ì‹ ë¢°ë„ ì„ê³„ê°’ (0.5 â†’ 0.4ë¡œ ì™„í™”)

# ğŸš€ ì‹¬ë¦¬ë„ ê³„ì‚° ìœ í‹¸ í•¨ìˆ˜ë“¤
def _compute_sentiment_series(df: pd.DataFrame) -> pd.Series:
    """ì‹¬ë¦¬ë„ ì ìˆ˜ ê³„ì‚° (-1 ~ +1)"""
    rsi = df.get('rsi', pd.Series(50.0, index=df.index)).fillna(50.0)
    macd = df.get('macd', pd.Series(0.0, index=df.index)).fillna(0.0)
    macd_sig = df.get('macd_signal', pd.Series(0.0, index=df.index)).fillna(0.0)
    volr = df.get('volume_ratio', pd.Series(1.0, index=df.index)).fillna(1.0)
    vol = df.get('atr', pd.Series(0.02, index=df.index)).fillna(0.02)
    wave_phase = df.get('wave_phase', pd.Series('unknown', index=df.index)).fillna('unknown')
    patt_conf = df.get('pattern_confidence', pd.Series(0.5, index=df.index)).fillna(0.5)

    # MACD ì„±ë¶„ ê³„ì‚°
    macd_mag = (macd.abs() / (macd.abs().ewm(span=20, min_periods=1).mean() + 1e-9)).clip(0, 1)
    macd_side = np.sign(macd - macd_sig).astype(float)

    # ì‹¬ë¦¬ë„ ì ìˆ˜ ê³„ì‚° (ê°€ì¤‘í•©)
    sent = (
        0.35 * np.tanh((rsi - 50.0) / 10.0) +
        0.25 * (macd_side * macd_mag) +
        0.20 * np.clip(np.log(volr.replace(0, 1e-9)), -1, 1) +
        0.20 * (wave_phase.isin(['impulse']).astype(float)
                - wave_phase.isin(['correction']).astype(float)) * 0.8 +
        0.10 * (patt_conf - 0.5) -
        0.15 * vol.clip(0, 1)
    )
    return sent.clip(-1, 1)

def _label_sentiment(v: float) -> str:
    """ì‹¬ë¦¬ë„ ì ìˆ˜ë¥¼ ë¼ë²¨ë¡œ ë³€í™˜"""
    if v >= 0.6:   return 'very_bullish'
    if v >= 0.3:   return 'bullish'
    if v <= -0.6:  return 'very_bearish'
    if v <= -0.3:  return 'bearish'
    return 'neutral'

# -------------------- ë¶„ì„ íŒŒë¼ë¯¸í„° --------------------
ZIGZAG_LOOKBACK_MAP = {
    # 5m ì œê±°
    '15m': 3,
    '30m': 3,
    '240m': 2,
    '1d': 2,
    '1w': 1
}
MIN_REQUIREMENTS = {
    'min_zigzag_points': 1,  # ìµœì†Œ 1ê°œë¡œ ì™„í™”
    'min_unique_pivots': 1,
    'min_wave_progress': 0.001,  # ê¸°ì¤€ ì™„í™”
    'min_pattern_confidence': 0.01  # ê¸°ì¤€ ì™„í™”
}

# -------------------- ë¶„ì„ í•¨ìˆ˜ (rl_candles_integrated.py ë°©ì‹) --------------------
def validate_zigzag_data(df: pd.DataFrame, interval: str) -> bool:
    if 'zigzag_direction' not in df.columns:
        return False
    non_zero_directions = (df['zigzag_direction'] != 0).sum()
    if non_zero_directions < MIN_REQUIREMENTS['min_zigzag_points']:
        return False
    return True

def add_zigzag(df: pd.DataFrame, interval: str) -> pd.DataFrame:
    lookback = ZIGZAG_LOOKBACK_MAP.get(interval, 2)  # ê¸°ë³¸ê°’ 2ë¡œ ë³€ê²½
    close = df['close'].values
    zz_direction = [0] * len(close)
    zz_pivot_price = [np.nan] * len(close)  # ì „í™˜ì  ê°€ê²©ë§Œ ì €ì¥
    
    # None ê°’ ì²˜ë¦¬: Noneì„ np.nanìœ¼ë¡œ ë³€í™˜
    close = np.array([np.nan if x is None else x for x in close])
    
    change_count = 0
    
    for i in range(lookback, len(close) - lookback):
        window = close[i - lookback:i + lookback + 1]
        center = close[i]
        
        # None/nan ê°’ ì²´í¬
        if pd.isna(center) or pd.isna(window).all():
            continue
            
        # windowì—ì„œ nan ê°’ ì œê±° í›„ ìµœëŒ€/ìµœì†Œ ê³„ì‚°
        valid_window = window[~pd.isna(window)]
        if len(valid_window) == 0:
            continue
            
        window_max = valid_window.max()
        window_min = valid_window.min()
        
        if center == window_max:
            zz_direction[i] = 1
            zz_pivot_price[i] = center  # ì „í™˜ì  ê°€ê²© ì €ì¥
            change_count += 1
        elif center == window_min:
            zz_direction[i] = -1
            zz_pivot_price[i] = center  # ì „í™˜ì  ê°€ê²© ì €ì¥
            change_count += 1
    
    # ì „í™˜ì  ë¶€ì¡± ì‹œ ëŒ€ì²´ ê³„ì‚°
    if change_count < 1:
        # ëŒ€ì²´ ê³„ì‚° ë°©ì‹: ë‹¨ìˆœí•œ ê³ ì /ì €ì  ì°¾ê¸°
        zz_direction = [0] * len(close)
        zz_pivot_price = [np.nan] * len(close)
        
        for i in range(1, len(close) - 1):
            current = close[i]
            prev = close[i-1]
            next_val = close[i+1]
            
            if not (pd.isna(current) or pd.isna(prev) or pd.isna(next_val)):
                # ê³ ì  íŒë‹¨ (ì´ì „ê³¼ ë‹¤ìŒë³´ë‹¤ ë†’ìŒ)
                if current > prev and current > next_val:
                    zz_direction[i] = 1
                    zz_pivot_price[i] = current
                    change_count += 1
                # ì €ì  íŒë‹¨ (ì´ì „ê³¼ ë‹¤ìŒë³´ë‹¤ ë‚®ìŒ)
                elif current < prev and current < next_val:
                    zz_direction[i] = -1
                    zz_pivot_price[i] = current
                    change_count += 1
    
    # zigzag_directionì´ ëª¨ë‘ 0ì¸ì§€ í™•ì¸
    non_zero_directions = sum(1 for d in zz_direction if d != 0)
    if non_zero_directions == 0:
        # ìµœì†Œí•œì˜ ì „í™˜ì  ìƒì„± (ì²« ë²ˆì§¸ì™€ ë§ˆì§€ë§‰ ìº”ë“¤)
        if len(close) >= 2:
            zz_direction[0] = 1  # ì²« ë²ˆì§¸ë¥¼ ê³ ì ìœ¼ë¡œ
            zz_pivot_price[0] = close[0]
            zz_direction[-1] = -1  # ë§ˆì§€ë§‰ì„ ì €ì ìœ¼ë¡œ
            zz_pivot_price[-1] = close[-1]
            change_count = 2
    
    df['zigzag_direction'] = zz_direction
    df['zigzag_pivot_price'] = zz_pivot_price
    return df

def analyze_wave_structure_new(df: pd.DataFrame, interval: str) -> pd.DataFrame:
    zigzag_valid = validate_zigzag_data(df, interval)
    if not zigzag_valid:
        df['wave_number'] = 0
        df['wave_progress'] = 0.5  # ê¸°ë³¸ê°’ 0.5ë¡œ ë³€ê²½
        df['wave_phase'] = 'unknown'
        df['integrated_wave_phase'] = 'unknown'
        df['structure_score'] = 0.5
        df['wave_step'] = 0
        df['pattern_class'] = 'unknown'
        return df
    wave_numbers = []
    current_wave = 0
    last_direction = 0
    for i, direction in enumerate(df['zigzag_direction']):
        if direction != 0 and direction != last_direction:
            current_wave += 1
            last_direction = direction
        wave_numbers.append(current_wave)
    df['wave_number'] = wave_numbers
    wave_progress = []
    lookback = ZIGZAG_LOOKBACK_MAP.get(interval, 2)  # ê¸°ë³¸ê°’ 2ë¡œ ë³€ê²½
    for i in range(len(df)):
        if i < lookback:
            wave_progress.append(0.5)  # ê¸°ë³¸ê°’ 0.5ë¡œ ë³€ê²½
            continue
        current_direction = df['zigzag_direction'].iloc[i]
        if current_direction != 0:
            prev_pivot_idx = i - 1
            while prev_pivot_idx >= 0 and df['zigzag_direction'].iloc[prev_pivot_idx] == 0:
                prev_pivot_idx -= 1
            if prev_pivot_idx >= 0:
                prev_price = df['zigzag_pivot_price'].iloc[prev_pivot_idx]
                current_price = df['close'].iloc[i]
                if pd.notna(prev_price) and prev_price != 0:
                    if current_direction == 1:
                        progress = (current_price - prev_price) / (df['high'].iloc[i] - prev_price + 1e-9)
                    else:
                        progress = (prev_price - current_price) / (prev_price - df['low'].iloc[i] + 1e-9)
                    wave_progress.append(progress.clip(0, 1))
                else:
                    wave_progress.append(0.5)  # ê¸°ë³¸ê°’ 0.5ë¡œ ë³€ê²½
            else:
                wave_progress.append(0.5)  # ê¸°ë³¸ê°’ 0.5ë¡œ ë³€ê²½
        else:
            wave_progress.append(0.5)  # ê¸°ë³¸ê°’ 0.5ë¡œ ë³€ê²½
    df['wave_progress'] = wave_progress
    wave_phases = []
    for i in range(len(df)):
        direction = df['zigzag_direction'].iloc[i]
        progress = df['wave_progress'].iloc[i]
        if direction == 1:
            if progress > 0.7:
                wave_phases.append('impulse')
            elif progress > 0.3:
                wave_phases.append('correction')
            else:
                wave_phases.append('consolidation')
        elif direction == -1:
            if progress > 0.7:
                wave_phases.append('correction')
            elif progress > 0.3:
                wave_phases.append('impulse')
            else:
                wave_phases.append('consolidation')
        else:
            wave_phases.append('unknown')
    df['wave_phase'] = wave_phases
    df['integrated_wave_phase'] = wave_phases
    return df

def analyze_pattern_structure_new(df: pd.DataFrame, interval: str) -> pd.DataFrame:
    zigzag_valid = validate_zigzag_data(df, interval)
    lookback = ZIGZAG_LOOKBACK_MAP.get(interval, 6)
    pattern_types, pattern_qualities, pattern_directions = [], [], []
    pattern_volume_ratios, pattern_pivot_strengths, pattern_start_indices, pattern_end_indices = [], [], [], []
    for i in range(len(df)):
        if i < lookback:
            pattern_types.append('none')
            pattern_qualities.append(0.0)
            pattern_directions.append('neutral')
            pattern_volume_ratios.append(1.0)
            pattern_pivot_strengths.append(0.0)
            pattern_start_indices.append(0)
            pattern_end_indices.append(0)
            continue
        current_direction = df['zigzag_direction'].iloc[i]
        if current_direction != 0 and zigzag_valid:
            pattern_end = i
            pattern_start = i - 1
            while pattern_start >= 0 and df['zigzag_direction'].iloc[pattern_start] == 0:
                pattern_start -= 1
            if pattern_start >= 0:
                pattern_data = df.iloc[pattern_start:pattern_end+1]
                if current_direction == 1:
                    pattern_type = 'ascending_triangle' if len(pattern_data) >= 3 else 'uptrend'
                    pattern_direction = 'bullish'
                elif current_direction == -1:
                    pattern_type = 'descending_triangle' if len(pattern_data) >= 3 else 'downtrend'
                    pattern_direction = 'bearish'
                else:
                    pattern_type = 'sideways'
                    pattern_direction = 'neutral'
                avg_volume_ratio = pattern_data['volume_ratio'].mean() if 'volume_ratio' in pattern_data.columns else 1.0
                avg_volatility = (pattern_data['atr'] / pattern_data['close']).mean() if 'atr' in pattern_data.columns and 'close' in pattern_data.columns else 0.02
                pattern_quality = avg_volume_ratio * avg_volatility * 5  # pattern_confidence ëŒ€ì‹  pattern_qualityë§Œ ê³„ì‚°
                pivot_strength = pattern_data['pivot_point'].sum() / len(pattern_data) if 'pivot_point' in pattern_data.columns else 0.0
                pattern_types.append(pattern_type)
                pattern_qualities.append(pattern_quality)
                pattern_directions.append(pattern_direction)
                pattern_volume_ratios.append(avg_volume_ratio)
                pattern_pivot_strengths.append(pivot_strength)
                pattern_start_indices.append(pattern_start)
                pattern_end_indices.append(pattern_end)
            else:
                pattern_types.append('none')
                pattern_qualities.append(0.0)
                pattern_directions.append('neutral')
                pattern_volume_ratios.append(1.0)
                pattern_pivot_strengths.append(0.0)
                pattern_start_indices.append(0)
                pattern_end_indices.append(0)
        else:
            pattern_types.append('none')
            pattern_qualities.append(0.0)
            pattern_directions.append('neutral')
            pattern_volume_ratios.append(1.0)
            pattern_pivot_strengths.append(0.0)
            pattern_start_indices.append(0)
            pattern_end_indices.append(0)
    df['pattern_type'] = pattern_types
    # âœ… pattern_confidenceëŠ” realtime_candles_calculate.pyì—ì„œ ê³„ì‚°ë¨
    df['pattern_quality'] = pattern_qualities
    df['pattern_direction'] = pattern_directions
    df['pattern_volume_ratio'] = pattern_volume_ratios
    df['pattern_pivot_strength'] = pattern_pivot_strengths
    df['pattern_start_idx'] = pattern_start_indices
    df['pattern_end_idx'] = pattern_end_indices
    return df

def compute_wave_step(df: pd.DataFrame) -> pd.Series:
    """ğŸš€ ë²¡í„°í™” ì—°ì‚°ìœ¼ë¡œ ìµœì í™”ëœ íŒŒë™ ë‹¨ê³„ ê³„ì‚°"""
    # ê¸°ë³¸ê°’ ì„¤ì •
    wave_num = df.get('wave_number', pd.Series(0, index=df.index))
    wave_progress = df.get('wave_progress', pd.Series(0.0, index=df.index))
    zigzag_direction = df.get('zigzag_direction', pd.Series(0, index=df.index))
    
    # NaN ê°’ ì²˜ë¦¬
    wave_progress = wave_progress.fillna(0.0)
    wave_num = wave_num.fillna(0)
    zigzag_direction = zigzag_direction.fillna(0)
    
    # ğŸš€ ë²¡í„°í™”ëœ íŒŒë™ ë‹¨ê³„ ê³„ì‚°
    wave_step = pd.cut(wave_progress, 
                      bins=[0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],
                      labels=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10],
                      include_lowest=True).astype(int)
    
    # ğŸš€ ë²¡í„°í™”ëœ ë°©í–¥ ì¡°ì •
    wave_step = np.where(zigzag_direction == -1, 11 - wave_step, wave_step)
    
    # ğŸš€ ìœ íš¨í•˜ì§€ ì•Šì€ íŒŒë™ì€ 0ìœ¼ë¡œ ì„¤ì •
    wave_step = np.where((wave_num == 0) | pd.isna(wave_progress), 0, wave_step)
    
    return pd.Series(wave_step, index=df.index)

def evaluate_fractal_structure(df: pd.DataFrame) -> pd.Series:
    structure_scores = []
    for i in range(len(df)):
        base_score = 0.0
        wave_num = df['wave_number'].iloc[i] if 'wave_number' in df.columns else 0
        if wave_num > 0:
            if wave_num <= 3:
                wave_continuity = wave_num / 3.0
            elif wave_num <= 7:
                wave_continuity = 0.5 + (wave_num - 3) / 8.0
            else:
                wave_continuity = 0.75 + (wave_num - 7) / 20.0
            wave_continuity = min(wave_continuity, 1.0)
            base_score += 0.25 * wave_continuity
        wave_progress = df['wave_progress'].iloc[i] if 'wave_progress' in df.columns else 0.0
        if pd.notna(wave_progress):
            if wave_progress < 0.2:
                progress_score = wave_progress * 2.5
            elif wave_progress < 0.4:
                progress_score = 0.5 + (wave_progress - 0.2) * 1.25
            elif wave_progress < 0.6:
                progress_score = 0.75 + (wave_progress - 0.4) * 0.625
            elif wave_progress < 0.8:
                progress_score = 0.875 + (wave_progress - 0.6) * 0.625
            else:
                progress_score = 1.0 - (wave_progress - 0.8) * 2.5
            progress_score = max(0.0, min(1.0, progress_score))
            base_score += 0.2 * progress_score
        if 'pattern_confidence' in df.columns:
            pattern_conf = df['pattern_confidence'].iloc[i]
            if pd.notna(pattern_conf):
                if pattern_conf < 0.3:
                    pattern_score = pattern_conf * 1.5
                elif pattern_conf < 0.6:
                    pattern_score = 0.45 + (pattern_conf - 0.3) * 0.5
                elif pattern_conf < 0.8:
                    pattern_score = 0.6 + (pattern_conf - 0.6) * 1.0
                else:
                    pattern_score = 0.8 + (pattern_conf - 0.8) * 1.0
                base_score += 0.25 * pattern_score
        if 'volume_ratio' in df.columns:
            volume_ratio = df['volume_ratio'].iloc[i]
            if pd.notna(volume_ratio):
                if volume_ratio < 0.5:
                    volume_score = volume_ratio * 1.0
                elif volume_ratio < 1.0:
                    volume_score = 0.5 + (volume_ratio - 0.5) * 0.5
                elif volume_ratio < 2.0:
                    volume_score = 0.75 + (volume_ratio - 1.0) * 0.25
                else:
                    volume_score = 1.0 - (volume_ratio - 2.0) * 0.1
                volume_score = max(0.0, min(1.0, volume_score))
                base_score += 0.15 * volume_score
        if 'rsi' in df.columns:
            rsi = df['rsi'].iloc[i]
            if pd.notna(rsi):
                if rsi < 20:
                    rsi_score = 0.2
                elif rsi < 30:
                    rsi_score = 0.4
                elif rsi < 45:
                    rsi_score = 0.6
                elif rsi < 55:
                    rsi_score = 0.8
                elif rsi < 70:
                    rsi_score = 0.6
                elif rsi < 80:
                    rsi_score = 0.4
                else:
                    rsi_score = 0.2
                base_score += 0.15 * rsi_score
        structure_scores.append(min(1.0, base_score))
    return pd.Series(structure_scores, index=df.index)

def classify_pattern_structure(df: pd.DataFrame) -> pd.Series:
    pattern_classes = []
    for i in range(len(df)):
        pattern_class = 'unknown'
        pattern_direction = df.get('pattern_direction', pd.Series(['neutral']*len(df))).iloc[i]
        wave_phase = df.get('wave_phase', pd.Series(['unknown']*len(df))).iloc[i]
        
        # âœ… None ê°’ ì²˜ë¦¬ ê°•í™”
        if pd.isna(pattern_direction) or pattern_direction is None:
            pattern_direction = 'neutral'
        if pd.isna(wave_phase) or wave_phase is None:
            wave_phase = 'unknown'
        # âœ… wave_step None ê°’ ì²˜ë¦¬ ê°•í™”
        if 'wave_step' in df.columns:
            wave_step_val = df['wave_step'].iloc[i]
            if pd.isna(wave_step_val) or wave_step_val is None:
                wave_step = 0
            else:
                try:
                    wave_step = int(wave_step_val)
                except (ValueError, TypeError):
                    wave_step = 0
        else:
            wave_step = 0
        pattern_type = df.get('pattern_type', pd.Series(['none']*len(df))).iloc[i]
        pattern_confidence = df.get('pattern_confidence', pd.Series([0.0]*len(df))).iloc[i]
        
        # âœ… None ê°’ ì²˜ë¦¬ ê°•í™”
        if pd.isna(pattern_confidence) or pattern_confidence is None:
            pattern_confidence = 0.0
        else:
            try:
                pattern_confidence = float(pattern_confidence)
            except (ValueError, TypeError):
                pattern_confidence = 0.0
        
        if pattern_direction == 'bullish':
            if wave_phase == 'impulse':
                if wave_step >= 4:
                    if pattern_confidence > 0.7:
                        pattern_class = 'bullish_impulse_late_strong'
                    else:
                        pattern_class = 'bullish_impulse_late'
                elif wave_step >= 2:
                    pattern_class = 'bullish_impulse_mid'
                else:
                    pattern_class = 'bullish_impulse_early'
            elif wave_phase == 'correction':
                pattern_class = 'bullish_correction'
            else:
                pattern_class = 'bullish_consolidation'
        elif pattern_direction == 'bearish':
            if wave_phase == 'impulse':
                if wave_step >= 4:
                    if pattern_confidence > 0.7:
                        pattern_class = 'bearish_impulse_late_strong'
                    else:
                        pattern_class = 'bearish_impulse_late'
                elif wave_step >= 2:
                    pattern_class = 'bearish_impulse_mid'
                else:
                    pattern_class = 'bearish_impulse_early'
            elif wave_phase == 'correction':
                pattern_class = 'bearish_correction'
            else:
                pattern_class = 'bearish_consolidation'
        else:
            if wave_phase == 'consolidation':
                pattern_class = 'sideways_consolidation'
            else:
                if pattern_confidence < 0.3:
                    pattern_class = 'sideways_unknown_low_confidence'
                else:
                    pattern_class = 'sideways_unknown'
        pattern_classes.append(pattern_class)
    return pd.Series(pattern_classes, index=df.index)

def calculate_volatility_level(volatility):
    if pd.isna(volatility):
        return 'unknown'
    if volatility > 0.05:
        return 'high'
    elif volatility > 0.02:
        return 'medium'
    else:
        return 'low'

def calculate_risk_level(risk_score):
    if pd.isna(risk_score):
        return 'unknown'
    if risk_score > 0.7:
        return 'high'
    elif risk_score > 0.4:
        return 'medium'
    else:
        return 'low'

def calculate_flow_level_meta_simple(df):
    """ê°„ë‹¨í•œ Flow Level ë©”íƒ€ë°ì´í„° ê³„ì‚°"""
    if len(df) < 5:
        return 'Neutral'
    
    # RSI ê¸°ë°˜ ì¶”ì„¸ íŒë‹¨
    rsi = df['rsi'].iloc[-1] if 'rsi' in df.columns and not pd.isna(df['rsi'].iloc[-1]) else 50
    rsi_ema = df['rsi_ema'].iloc[-1] if 'rsi_ema' in df.columns and not pd.isna(df['rsi_ema'].iloc[-1]) else rsi
    
    # MACD ê¸°ë°˜ ëª¨ë©˜í…€ í™•ì¸
    macd = df['macd'].iloc[-1] if 'macd' in df.columns and not pd.isna(df['macd'].iloc[-1]) else 0
    macd_signal = df['macd_signal'].iloc[-1] if 'macd_signal' in df.columns and not pd.isna(df['macd_signal'].iloc[-1]) else 0
    
    # ê±°ë˜ëŸ‰ ê¸°ë°˜ ê°•ë„ íŒë‹¨
    volume_ratio = df['volume_ratio'].iloc[-1] if 'volume_ratio' in df.columns and not pd.isna(df['volume_ratio'].iloc[-1]) else 1.0
    
    # ì¶”ì„¸ íŒë‹¨
    if rsi > 70 and rsi_ema > 65:
        trend = "strong_up"
    elif rsi > 60 and rsi_ema > 55:
        trend = "up"
    elif rsi < 30 and rsi_ema < 35:
        trend = "strong_down"
    elif rsi < 40 and rsi_ema < 45:
        trend = "down"
    else:
        trend = "sideways"
    
    # ëª¨ë©˜í…€ íŒë‹¨
    if macd > macd_signal and macd > 0:
        momentum = "bullish"
    elif macd < macd_signal and macd < 0:
        momentum = "bearish"
    else:
        momentum = "neutral"
    
    # ê±°ë˜ëŸ‰ ê°•ë„ íŒë‹¨
    if volume_ratio > 2.0:
        volume_strength = "high"
    elif volume_ratio > 1.5:
        volume_strength = "medium"
    else:
        volume_strength = "low"
    
    # í†µí•© íŒë‹¨
    if trend in ["strong_up", "up"] and momentum == "bullish":
        if volume_strength == "high":
            return "Momentum Bull"
        else:
            return "Pullback Bull"
    elif trend in ["strong_down", "down"] and momentum == "bearish":
        return "Exhaustion Bear"
    else:
        return "Neutral"

# ğŸš€ ë ˆì§ ê³„ì‚° í•¨ìˆ˜ë“¤
def calculate_composite_regime_score(df: pd.DataFrame, interval: str) -> pd.Series:
    """ì¸í„°ë²Œë³„ ë³µí•© ì§€í‘œ ê¸°ë°˜ ë ˆì§ ì ìˆ˜ ê³„ì‚°"""
    criteria = REGIME_CRITERIA.get(interval, REGIME_CRITERIA['30m'])
    
    # RSI ì ìˆ˜ (0-1 ì •ê·œí™”)
    rsi_data = df.get('rsi', pd.Series(50.0, index=df.index)).fillna(50.0)
    rsi_score = (rsi_data - 20) / 60  # 20-80ì„ 0-1ë¡œ ë³€í™˜
    rsi_score = rsi_score.clip(0, 1)
    
    # MACD ëª¨ë©˜í…€ ì ìˆ˜
    macd_data = df.get('macd', pd.Series(0.0, index=df.index)).fillna(0.0)
    macd_signal_data = df.get('macd_signal', pd.Series(0.0, index=df.index)).fillna(0.0)
    macd_momentum = (macd_data - macd_signal_data).abs()
    macd_score = macd_momentum / (macd_momentum.rolling(20).max() + 1e-9)
    
    # Volume ê°•ë„ ì ìˆ˜
    volume_data = df.get('volume_ratio', pd.Series(1.0, index=df.index)).fillna(1.0)
    volume_score = volume_data.clip(0, 3) / 3
    
    # Volatility ì ìˆ˜ (ì•ˆì •ì„± ì¸¡ë©´)
    atr_data = df.get('atr', pd.Series(0.02, index=df.index)).fillna(0.02)
    close_data = df.get('close', pd.Series(100.0, index=df.index)).fillna(100.0)
    volatility_score = 1 - (atr_data / close_data).clip(0, 0.1) / 0.1
    
    # ê°€ì¤‘ í‰ê· 
    composite_score = (
        criteria['rsi_weight'] * rsi_score +
        criteria['macd_weight'] * macd_score +
        criteria['volume_weight'] * volume_score +
        0.1 * volatility_score  # ì•ˆì •ì„± ë³´ë„ˆìŠ¤
    )
    
    return composite_score.clip(0, 1)

def classify_regime_stage(composite_score: pd.Series, interval: str) -> pd.Series:
    """ë³µí•© ì ìˆ˜ë¥¼ 7ë‹¨ê³„ ë ˆì§ìœ¼ë¡œ ë¶„ë¥˜"""
    
    # NaN ê°’ ì²˜ë¦¬
    composite_score = composite_score.fillna(0.5)  # ê¸°ë³¸ê°’: neutral
    
    # ì¸í„°ë²Œë³„ ì„ê³„ê°’ ì¡°ì • (ì¤‘ë³µ ì œê±°)
    thresholds = {
        '15m': [0.2, 0.35, 0.5, 0.6, 0.75, 0.9],
        '30m': [0.25, 0.4, 0.55, 0.65, 0.8, 0.95],
        '240m': [0.3, 0.45, 0.6, 0.7, 0.85, 0.99],
        '1d': [0.3, 0.45, 0.6, 0.7, 0.85, 0.99]
    }
    
    thresh = thresholds.get(interval, [0.25, 0.4, 0.55, 0.65, 0.8, 0.95])
    
    # ì¤‘ë³µ ì œê±°ëœ bins ìƒì„±
    bins = [0] + thresh + [1.0]
    bins = sorted(list(set(bins)))  # ì¤‘ë³µ ì œê±° ë° ì •ë ¬
    
    regime_stage = pd.cut(composite_score, 
                         bins=bins,
                         labels=list(range(1, len(bins))),
                         include_lowest=True,
                         duplicates='drop')  # ì¤‘ë³µ ì œê±° ì˜µì…˜
    
    # NaN ê°’ ì²˜ë¦¬ ë° ì •ìˆ˜ ë³€í™˜
    regime_stage = regime_stage.fillna(4)  # ê¸°ë³¸ê°’: neutral
    regime_stage = regime_stage.astype(int)
    
    return regime_stage

def calculate_regime_confidence(df: pd.DataFrame, interval: str) -> pd.Series:
    """ë ˆì§ ì‹ ë¢°ë„ ê³„ì‚°"""
    criteria = REGIME_CRITERIA.get(interval, REGIME_CRITERIA['30m'])
    lookback = criteria['lookback_period']
    
    if len(df) < lookback:
        return pd.Series(0.5, index=df.index)
    
    # RSI ì¼ê´€ì„±
    rsi_data = df.get('rsi', pd.Series(50.0, index=df.index)).fillna(50.0)
    rsi_std = rsi_data.rolling(lookback).std()
    rsi_consistency = (1 - rsi_std / 20).clip(0, 1)
    
    # MACD ì‹ í˜¸ ê°•ë„ (ê³¼í¬í™” ë°©ì§€ - ë¡¤ë§ ìµœëŒ€ì¹˜ ê¸°ì¤€ ì •ê·œí™”)
    macd_data = df.get('macd', pd.Series(0.0, index=df.index)).fillna(0.0)
    macd_signal_data = df.get('macd_signal', pd.Series(0.0, index=df.index)).fillna(0.0)
    macd_diff = (macd_data - macd_signal_data).abs()
    macd_strength = macd_diff.rolling(lookback).mean()
    macd_strength = (macd_strength / (macd_strength.rolling(lookback).max() + 1e-9)).clip(0, 1)
    
    # Volume ì¼ê´€ì„±
    volume_data = df.get('volume_ratio', pd.Series(1.0, index=df.index)).fillna(1.0)
    volume_std = volume_data.rolling(lookback).std()
    volume_consistency = (1 - volume_std / 2.0).clip(0, 1)
    
    # ì¢…í•© ì‹ ë¢°ë„
    confidence = (
        criteria['rsi_weight'] * rsi_consistency +
        criteria['macd_weight'] * macd_strength +
        criteria['volume_weight'] * volume_consistency
    )
    
    return confidence.clip(0, 1)

def calculate_regime_transition_probability(df: pd.DataFrame, interval: str) -> pd.Series:
    """ë ˆì§ ì „í™˜ í™•ë¥  ê³„ì‚° (1ë‹¨ê³„ ì „í™˜ í¬í•¨, ì°¨ë“± ê°€ì¤‘ì¹˜ ì ìš©)"""
    criteria = REGIME_CRITERIA.get(interval, REGIME_CRITERIA['30m'])
    lookback = criteria['lookback_period']
    
    if len(df) < lookback * 2:
        return pd.Series(0.05, index=df.index)  # ê¸°ë³¸ê°’ 0.1â†’0.05ë¡œ ë‚®ì¶¤
    
    # ì ìˆ˜ ê³„ì‚° ë° í‰í™œí™”
    composite_score = calculate_composite_regime_score(df, interval)
    smooth_score = composite_score.ewm(span=lookback, min_periods=1).mean()
    regime_stage = classify_regime_stage(smooth_score, interval)
    
    # ì‹ ë¢°ë„ ê³„ì‚°
    confidence = calculate_regime_confidence(df, interval)
    
    # ğŸš€ 1ë‹¨ê³„ì™€ 2ë‹¨ê³„ ì´ìƒ ì „í™˜ì— ì°¨ë“± ê°€ì¤‘ì¹˜ ì ìš©
    changes = regime_stage.diff().abs()
    
    # 1ë‹¨ê³„ ì „í™˜: ì‹ ë¢°ë„ ê¸°ë°˜ ê°€ì¤‘ì¹˜ (0.3 ~ 0.6)
    minor_weight = 0.3 + (confidence * 0.3)  # ì‹ ë¢°ë„ ë†’ì„ìˆ˜ë¡ ê°€ì¤‘ì¹˜ ì¦ê°€
    minor_changes = (changes == 1).astype(float) * minor_weight
    
    # 2ë‹¨ê³„ ì´ìƒ ì „í™˜: ì „ì²´ ê°€ì¤‘ì¹˜ (1.0)
    major_changes = (changes >= 2).astype(float) * 1.0
    
    # í†µí•© ì „í™˜ ì‹ í˜¸
    all_changes = minor_changes + major_changes
    
    # ë¡¤ë§ í‰ê· ìœ¼ë¡œ ì „í™˜ ë¹ˆë„ ê³„ì‚°
    change_frequency = all_changes.rolling(lookback, min_periods=1).mean()
    
    # ìƒí•œì„  ì¡°ì • (0.5 â†’ 0.4, ë” ë„“ì€ ë²”ìœ„ í™œìš©)
    transition_prob = change_frequency.clip(0, 0.4)
    
    return transition_prob.fillna(0.05)

# ğŸš€ ml_candles_calculate.pyì™€ ë™ì¼í•œ ì €ì¥ í•¨ìˆ˜ ì¶”ê°€
def save_integrated_indicators_immediate(df: pd.DataFrame, coin: str, interval: str) -> bool:
    """ğŸš€ í†µí•© ë¶„ì„ ì™„ë£Œ ì¦‰ì‹œ ì €ì¥ - ml_candles_calculate.pyì˜ ì„±ê³µì ì¸ ë°©ì‹ ì ìš©"""
    try:
        if df.empty:
            return False
        
        # ğŸ“Œ ì†Œìˆ«ì  ìë¦¬ìˆ˜ í†µì¼ ì ìš© (integrated ì»¬ëŸ¼) - 4ìë¦¬ë¡œ í†µì¼
        rounding_map = {
            'atr': 4,
            'risk_score': 4,
            'integrated_strength': 4,
            'sentiment': 4,  # ğŸš€ ì‹¬ë¦¬ë„ ì ìˆ˜ ì¶”ê°€
            'regime_confidence': 4,  # ğŸš€ ë ˆì§ ì‹ ë¢°ë„ ì¶”ê°€
            'regime_transition_prob': 4  # ğŸš€ ë ˆì§ ì „í™˜ í™•ë¥  ì¶”ê°€
        }

        # ë°˜ì˜¬ë¦¼ ì ìš© (ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ)
        for col, digits in rounding_map.items():
            if col in df.columns:
                df[col] = df[col].round(digits)
        
        # ì‹¤ì œë¡œ í…Œì´ë¸”ì— ì¡´ì¬í•˜ëŠ” integrated ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸
        integrated_columns = [
            'volatility_level', 'risk_level', 'integrated_direction',
            'sentiment', 'sentiment_label',  # ğŸš€ ì‹¬ë¦¬ë„ ì»¬ëŸ¼ ì¶”ê°€
            'regime_stage', 'regime_label', 'regime_confidence', 'regime_transition_prob'  # ğŸš€ ë ˆì§ ì»¬ëŸ¼ ì¶”ê°€
        ]

        # ì»¤ë„¥ì…˜ ì—´ê³ 
        with sqlite3.connect(DB_PATH) as conn:
            # ğŸš€ SQLite ì„±ëŠ¥ ìµœì í™” ì„¤ì •
            conn.execute("PRAGMA journal_mode=WAL")  # Write-Ahead Logging
            conn.execute("PRAGMA synchronous=NORMAL")  # ë™ê¸°í™” ë ˆë²¨ ì¡°ì •
            conn.execute("PRAGMA cache_size=10000")  # ìºì‹œ í¬ê¸° ì¦ê°€
            conn.execute("PRAGMA temp_store=MEMORY")
            conn.execute("PRAGMA mmap_size=268435456")  # 256MB ë©”ëª¨ë¦¬ ë§¤í•‘
            
            # ì‹¤ì œë¡œ í…Œì´ë¸”ì— ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ í™•ì¸
            cursor = conn.cursor()
            cursor.execute("PRAGMA table_info(candles);")
            table_columns = [col[1] for col in cursor.fetchall()]
            
            # DataFrameì— ì¡´ì¬í•˜ê³  í…Œì´ë¸”ì—ë„ ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ ì„ íƒ
            existing_columns = [c for c in integrated_columns if c in df.columns and c in table_columns]
            
            # ğŸ”§ ëˆ„ë½ëœ ì»¬ëŸ¼ ìë™ ì¶”ê°€
            missing_columns = []
            for col in integrated_columns:
                if col in df.columns and col not in table_columns:
                    missing_columns.append(col)
            
            if missing_columns:
                for col in missing_columns:
                    # ì»¬ëŸ¼ íƒ€ì… ê²°ì •
                    if col in ['volatility_level', 'risk_level', 'integrated_direction', 'sentiment_label', 'regime_label']:
                        col_type = 'TEXT'
                    elif col in ['regime_stage']:
                        col_type = 'INTEGER'
                    else:
                        col_type = 'REAL'
                    
                    try:
                        cursor.execute(f'ALTER TABLE candles ADD COLUMN "{col}" {col_type}')
                    except Exception as e:
                        continue
                
                # ì»¬ëŸ¼ ì¶”ê°€ í›„ ë‹¤ì‹œ í™•ì¸
                cursor.execute("PRAGMA table_info(candles);")
                table_columns = [col[1] for col in cursor.fetchall()]
                existing_columns = [c for c in integrated_columns if c in df.columns and c in table_columns]
            
            if not existing_columns:
                return False
            
            # ğŸš€ ëŒ€ëŸ‰ UPDATE ìµœì í™” (ê¸°ì¡´ ë°ì´í„° ë³´ì¡´í•˜ë©´ì„œ integrated ì»¬ëŸ¼ë§Œ ì—…ë°ì´íŠ¸)
            total_rows = len(df)
            total_updated = 0
            
            # ğŸš€ ë°ì´í„° ì „ì²˜ë¦¬ (í•œ ë²ˆì— ì²˜ë¦¬)
            update_data = []
            for _, row in df.iterrows():
                row_data = []
                for col in existing_columns:
                    if pd.notna(row[col]):
                        value = row[col]
                        # ğŸš€ ì»¬ëŸ¼ íƒ€ì…ë³„ ì•ˆì „í•œ ë³€í™˜
                        if col in ['volatility_level', 'risk_level', 'integrated_direction', 'sentiment_label', 'regime_label']:
                            # TEXT íƒ€ì… ì»¬ëŸ¼ - ë¬¸ìì—´ë¡œ ë³€í™˜
                            value = str(value) if value is not None else 'unknown'
                        elif col in ['regime_stage']:
                            # INTEGER íƒ€ì… ì»¬ëŸ¼ - ì •ìˆ˜ë¡œ ë³€í™˜
                            try:
                                value = int(value) if value is not None else 4
                            except (ValueError, TypeError):
                                value = 4  # ê¸°ë³¸ê°’: neutral
                        else:
                            # REAL íƒ€ì… ì»¬ëŸ¼ - ìˆ«ìë¡œ ë³€í™˜
                            try:
                                value = float(value)
                            except (ValueError, TypeError):
                                value = 0.0
                        row_data.append(value)
                    else:
                        row_data.append(None)
                
                # í‚¤ ê°’ë“¤ ì¶”ê°€ (ê¸°ì¡´ ë°ì´í„° ì‹ë³„ìš©)
                row_data.extend([row['coin'], row['interval'], row['timestamp']])
                update_data.append(row_data)
            
            if update_data:
                # ğŸš€ executemanyë¡œ ë°°ì¹˜ ì—…ë°ì´íŠ¸ (ê¸°ì¡´ ë°ì´í„°ëŠ” ë³´ì¡´)
                set_clauses = [f'"{col}" = ?' for col in existing_columns]
                
                sql = f"""
                    UPDATE candles 
                    SET {', '.join(set_clauses)}
                    WHERE coin = ? AND interval = ? AND timestamp = ?
                """
                
                cursor.executemany(sql, update_data)
                total_updated = len(update_data)
                conn.commit()
        
        # ğŸ” ì €ì¥ ê²°ê³¼ ê²€ì¦
        if total_updated > 0:
            return True
        else:
            return False
        
    except Exception as e:
        print(f"    âŒ í†µí•© ë¶„ì„ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {coin}/{interval} - {str(e)}")
        import traceback
        traceback.print_exc()
        return False

def perform_integrated_analysis(coin: str, interval: str):
    """ğŸš€ 9ê°œ í†µí•© ì»¬ëŸ¼ ê³„ì‚° - volatility_level, risk_level, integrated_direction, sentiment, sentiment_label, regime_stage, regime_label, regime_confidence, regime_transition_prob"""
    try:
        with sqlite3.connect(DB_PATH, timeout=30) as conn:
            conn.execute("PRAGMA synchronous = NORMAL")
            conn.execute("PRAGMA journal_mode = WAL")
            conn.execute("PRAGMA temp_store = MEMORY")
            conn.execute("PRAGMA busy_timeout = 30000")
            
            df = pd.read_sql(
                "SELECT * FROM candles WHERE coin=? AND interval=? ORDER BY timestamp",
                conn, params=(coin, interval)
            )
            if df.empty or len(df) < 20:
                return
        
        # ğŸš€ 3ê°œ í†µí•© ì»¬ëŸ¼ë§Œ ê³„ì‚°
        # volatility_level ê³„ì‚°
        if 'atr' in df.columns:
            df['volatility_level'] = df['atr'].apply(calculate_volatility_level)
        else:
            df['volatility_level'] = 'unknown'
        
        # risk_level ê³„ì‚°
        if 'risk_score' in df.columns:
            df['risk_level'] = df['risk_score'].apply(calculate_risk_level)
        else:
            df['risk_level'] = 'unknown'
        
        # integrated_direction ê³„ì‚° (íŒŒë™ ë° íŒ¨í„´ ì •ë³´ í†µí•© ë°˜ì˜)
        if 'rsi' in df.columns and 'macd' in df.columns and 'macd_signal' in df.columns:
            rsi_data = df['rsi'].fillna(50)
            macd_data = df['macd'].fillna(0)
            macd_signal_data = df['macd_signal'].fillna(0)
            
            # ğŸš€ ê¸°ì¡´ RSI/MACD ê¸°ë°˜ ë°©í–¥ì„± ê³„ì‚°
            rsi_direction = np.where(rsi_data > 70, 'bearish', 
                                   np.where(rsi_data < 30, 'bullish', 'neutral'))
            macd_direction = np.where(macd_data > macd_signal_data, 'bullish', 'bearish')
            
            # ğŸš€ íŒŒë™ ë° íŒ¨í„´ ì •ë³´ í†µí•© ë°˜ì˜ (ê°œì„ ì‚¬í•­)
            wave_phase_data = df.get('wave_phase', pd.Series(['unknown'] * len(df)))
            pattern_type_data = df.get('pattern_type', pd.Series(['none'] * len(df)))
            pattern_confidence_data = df.get('pattern_confidence', pd.Series([0.0] * len(df)))
            structure_score_data = df.get('structure_score', pd.Series([0.5] * len(df)))
            
            # ğŸš€ êµ¬ì¡°ì  ì‹ í˜¸ ì¡°ê±´ë“¤
            strong_bullish_condition = (
                (wave_phase_data == 'impulse') & 
                (pattern_type_data.isin(['uptrend', 'strong_uptrend']))
            )
            
            strong_bearish_condition = (
                (wave_phase_data == 'correction') & 
                (pattern_type_data.isin(['downtrend', 'strong_downtrend']))
            )
            
            structural_signal_condition = (
                (pattern_confidence_data > 0.7) & 
                (structure_score_data > 0.7)
            )
            
            # ğŸš€ í†µí•© ë°©í–¥ì„± ê²°ì • (ìš°ì„ ìˆœìœ„: êµ¬ì¡°ì  ì‹ í˜¸ > ê¸°ì¡´ ëª¨ë©˜í…€)
            df['integrated_direction'] = np.select([
                strong_bullish_condition,
                strong_bearish_condition,
                structural_signal_condition,
                (rsi_direction == 'bullish') & (macd_direction == 'bullish'),
                (rsi_direction == 'bearish') & (macd_direction == 'bearish'),
                rsi_direction == 'neutral'
            ], [
                'strong_bullish',
                'strong_bearish', 
                'structural_signal',
                'bullish',
                'bearish',
                'neutral'
            ], default='mixed')
        else:
            df['integrated_direction'] = 'neutral'
        
        # ğŸš€ ì‹¬ë¦¬ë„ ê³„ì‚° ë° ì €ì¥ (ê°œì„ ì‚¬í•­)
        sent_series = _compute_sentiment_series(df)
        df['sentiment'] = sent_series.round(4)
        df['sentiment_label'] = [_label_sentiment(x) for x in sent_series]
        
        # ğŸš€ ë ˆì§ ê³„ì‚° ë° ì €ì¥ (í‰í™œí™” + ì•ˆì •í™” ì ìš©)
        composite_score = calculate_composite_regime_score(df, interval)
        # 1) ì ìˆ˜ í‰í™œí™”(EWM)
        lookback = REGIME_CRITERIA.get(interval, REGIME_CRITERIA['30m'])['lookback_period']
        smooth_score = composite_score.ewm(span=lookback, min_periods=1).mean()
        raw_stage = classify_regime_stage(smooth_score, interval)

        # 2) ìµœì†Œ ì²´ë¥˜ì‹œê°„ + ì‹ ë¢°ë„ ê²Œì´íŠ¸
        conf_series = calculate_regime_confidence(df, interval).round(4)
        stable_stage = raw_stage.copy()
        if len(stable_stage) > 0:
            last = int(stable_stage.iloc[0])
            stay = 1
            for i in range(1, len(stable_stage)):
                cand = int(stable_stage.iloc[i])
                conf = float(conf_series.iloc[i]) if not pd.isna(conf_series.iloc[i]) else 0.5
                if cand != last and (stay < REGIME_MIN_STAY or conf < REGIME_CONF_GATE):
                    stable_stage.iloc[i] = last
                    stay += 1
                else:
                    stable_stage.iloc[i] = cand
                    if cand == last:
                        stay += 1
                    else:
                        last = cand
                        stay = 1

        df['regime_stage'] = stable_stage.astype(int)
        df['regime_label'] = [REGIME_STAGES.get(int(stage), 'neutral') for stage in df['regime_stage']]
        df['regime_confidence'] = conf_series
        df['regime_transition_prob'] = calculate_regime_transition_probability(df, interval).round(4)
        
        # ğŸš€ NULL ê°’ ë°©ì§€: ì‹œê³„ì—´ ì—°ì†ì„±ì„ ê³ ë ¤í•œ ë³´ê°„ë²• ì ìš©
        # ìˆ«ìí˜• ì»¬ëŸ¼: ì„ í˜• ë³´ê°„ë²• ì‚¬ìš©
        numeric_columns = ['sentiment', 'regime_confidence', 'regime_transition_prob']
        for col in numeric_columns:
            if col in df.columns:
                # ì„ í˜• ë³´ê°„ë²•ìœ¼ë¡œ ì‹œê°„ì  ì—°ì†ì„± ìœ ì§€ (ì‹œì‘ ë¶€ë¶„ì€ NULL ìœ ì§€)
                # ë°ì´í„° íƒ€ì…ì„ ëª…ì‹œì ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ FutureWarning ë°©ì§€
                df[col] = pd.to_numeric(df[col], errors='coerce').interpolate(method='linear', limit_direction='forward')
                # ë ë¶€ë¶„ë§Œ Forward Fillë¡œ ì²˜ë¦¬ (ì‹œì‘ ë¶€ë¶„ì€ NULL ê·¸ëŒ€ë¡œ ìœ ì§€)
                df[col] = df[col].ffill()
        
        # ì •ìˆ˜í˜• ì»¬ëŸ¼: ì´ì „ ê°’ìœ¼ë¡œ ì±„ìš°ê¸°
        integer_columns = ['regime_stage']
        for col in integer_columns:
            if col in df.columns:
                # Forward Fillë¡œ ì‹œê°„ì  ì—°ì†ì„± ìœ ì§€ (ì‹œì‘ ë¶€ë¶„ì€ NULL ìœ ì§€)
                df[col] = df[col].ffill()
        
        # í…ìŠ¤íŠ¸í˜• ì»¬ëŸ¼: ì´ì „ ê°’ìœ¼ë¡œ ì±„ìš°ê¸°
        text_columns = ['volatility_level', 'risk_level', 'integrated_direction', 'sentiment_label', 'regime_label']
        for col in text_columns:
            if col in df.columns:
                # Forward Fillë¡œ ì‹œê°„ì  ì—°ì†ì„± ìœ ì§€ (ì‹œì‘ ë¶€ë¶„ì€ NULL ìœ ì§€)
                df[col] = df[col].ffill()
        
        # ğŸš€ 9ê°œ í†µí•© ì»¬ëŸ¼ ì €ì¥ (volatility_level, risk_level, integrated_direction, sentiment, sentiment_label, regime_stage, regime_label, regime_confidence, regime_transition_prob)
        save_success = save_integrated_indicators_immediate(df, coin, interval)
        
        if save_success:
            print(f"âœ… í†µí•© ë¶„ì„ ì™„ë£Œ: {coin}/{interval} â†’ {len(df)}ê°œ ì—…ë°ì´íŠ¸")
        else:
            print(f"âŒ í†µí•© ë¶„ì„ ì €ì¥ ì‹¤íŒ¨: {coin}/{interval}")
        
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {coin}/{interval} - {str(e)}")
        import traceback
        traceback.print_exc()
        return

def run_full_integrated_analysis():
    """ğŸš€ 9ê°œ í†µí•© ì»¬ëŸ¼ë§Œ ê³„ì‚°í•˜ëŠ” ìµœì í™”ëœ í†µí•©ë¶„ì„"""
    print(f"ğŸš€ í†µí•© ë¶„ì„ ì‹œì‘ (9ê°œ ì»¬ëŸ¼: volatility_level, risk_level, integrated_direction, sentiment, sentiment_label, regime_stage, regime_label, regime_confidence, regime_transition_prob)")
    
    with sqlite3.connect(DB_PATH) as conn:
        conn.execute("PRAGMA synchronous = NORMAL")
        conn.execute("PRAGMA journal_mode = WAL")
        conn.execute("PRAGMA temp_store = MEMORY")
        
        cursor = conn.cursor()
        cursor.execute("""
            SELECT DISTINCT coin, interval, COUNT(*) as count
            FROM candles 
            WHERE interval IN ('15m', '30m', '240m', '1d')
            AND rsi IS NOT NULL AND macd IS NOT NULL
            GROUP BY coin, interval
            ORDER BY coin, interval
        """)
        coin_intervals = cursor.fetchall()
    
    if not coin_intervals:
        print("âš ï¸ ì²˜ë¦¬í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
        return
    
    total_groups = len(coin_intervals)
    print(f"ğŸ“Š ì²˜ë¦¬ ëŒ€ìƒ: {total_groups}ê°œ ì½”ì¸/ì¸í„°ë²Œ ê·¸ë£¹")
    
    success_count = 0
    error_count = 0
    batch_size = 10
    
    for i in range(0, len(coin_intervals), batch_size):
        batch = coin_intervals[i:i + batch_size]
        batch_num = i//batch_size + 1
        total_batches = (len(coin_intervals) + batch_size - 1)//batch_size
        
        print(f"ğŸ”„ ë°°ì¹˜ {batch_num}/{total_batches}: {len(batch)}ê°œ ê·¸ë£¹ ì²˜ë¦¬ ì¤‘...")
        
        for coin, interval, count in batch:
            try:
                perform_integrated_analysis(coin, interval)
                success_count += 1
                print(f"âœ… í†µí•© ë¶„ì„ ì„±ê³µ: {coin}/{interval}")
            except Exception as e:
                error_count += 1
                print(f"âŒ í†µí•© ë¶„ì„ ì˜¤ë¥˜: {coin}/{interval} - {str(e)}")
                continue
        
        import gc
        gc.collect()
        import time
        time.sleep(0.5)
    
    print(f"ğŸ‰ í†µí•© ë¶„ì„ ì™„ë£Œ: ì„±ê³µ {success_count}ê°œ, ì‹¤íŒ¨ {error_count}ê°œ")


# ---------------- ì‹¤í–‰ë¶€ ----------------
if __name__ == '__main__':
    # print('ğŸš€ í†µí•©ë¶„ì„(íŒŒë™+íŒ¨í„´+í”„ë™íƒˆ+í†µí•©ë©”íƒ€) ì‹œì‘!')  # ì œê±°ë¨
    run_full_integrated_analysis()
    # print('âœ… í†µí•©ë¶„ì„(íŒŒë™+íŒ¨í„´+í”„ë™íƒˆ+í†µí•©ë©”íƒ€) ì™„ë£Œ!')  # ì œê±°ë¨ 