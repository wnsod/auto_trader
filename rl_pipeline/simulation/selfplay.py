"""
Self-play ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“ˆ
- SelfPlaySimulator: Self-play ì‹œë®¬ë ˆì´í„°
- run_self_play_test: Self-play í…ŒìŠ¤íŠ¸ ì‹¤í–‰
- run_self_play_evolution: Self-play ì§„í™” ì‹¤í–‰
"""

import logging
import random
import numpy as np
import pandas as pd
import os
import warnings
from typing import Dict, List, Any, Optional
from datetime import datetime

from rl_pipeline.simulation.market_models import Action, MarketState, MarketDataGenerator, AgentState
from rl_pipeline.simulation.agent import StrategyAgent
from rl_pipeline.db.rl_writes import save_episode_summary

logger = logging.getLogger(__name__)

# ğŸ”¥ ë””ë²„ê·¸ ì‹œìŠ¤í…œ import (ì•ˆì „í•œ fallback)
try:
    from rl_pipeline.monitoring import SimulationDebugger
    DEBUG_AVAILABLE = True
except ImportError:
    logger.warning("âš ï¸ ë””ë²„ê·¸ ë¡œê¹… ì‹œìŠ¤í…œì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
    DEBUG_AVAILABLE = False
    SimulationDebugger = None

# ğŸ”¥ ì¸í„°ë²Œ í”„ë¡œí•„ import (ë³´ìƒ ê³„ì‚°ìš©)
try:
    from rl_pipeline.core.interval_profiles import calculate_reward
    INTERVAL_PROFILES_AVAILABLE = True
except ImportError:
    logger.debug("interval_profiles ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ë³´ìƒ ê³„ì‚° ì‚¬ìš©")
    INTERVAL_PROFILES_AVAILABLE = False
    calculate_reward = None

# ğŸ”§ TensorFlow ê²½ê³  ì–µì œ (JAXê°€ TensorFlow ì—†ì´ë„ ì‘ë™ ê°€ëŠ¥)
# í™˜ê²½ ë³€ìˆ˜ë¡œ TensorFlow ë¡œê¹… ì™„ì „ ì–µì œ
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
os.environ['JAX_PLATFORMS'] = 'cuda,cpu'  # TensorFlow ì²´í¬ ìš°íšŒ

# Python warnings í•„í„°ë§
warnings.filterwarnings('ignore', category=Warning, message='.*Tensorflow.*')
warnings.filterwarnings('ignore', category=Warning, message='.*TensorFlow.*')

# GPU ë¼ì´ë¸ŒëŸ¬ë¦¬ ê°€ìš©ì„± í™•ì¸
try:
    import jax
    import jax.numpy as jnp
    from jax import jit
    JAX_AVAILABLE = True
except ImportError:
    JAX_AVAILABLE = False
    jnp = None
    jit = None

try:
    import cupy
    CUPY_AVAILABLE = True
except ImportError:
    CUPY_AVAILABLE = False

# í™˜ê²½ë³€ìˆ˜
AZ_SIMULATION_VERBOSE = os.getenv('AZ_SIMULATION_VERBOSE', 'false').lower() == 'true'


class SelfPlaySimulator:
    """Self-Play ì‹œë®¬ë ˆì´í„°"""

    def __init__(self, use_gpu: bool = True, session_id: Optional[str] = None):
        """
        Self-Play ì‹œë®¬ë ˆì´í„° ì´ˆê¸°í™”

        Args:
            use_gpu: GPU ì‚¬ìš© ì—¬ë¶€
            session_id: ë””ë²„ê·¸ ì„¸ì…˜ ID (ì˜µì…˜)
        """
        self.market_generator = MarketDataGenerator()
        self.episode_count = 0
        self.learning_history = []
        self.use_gpu = use_gpu and (JAX_AVAILABLE or CUPY_AVAILABLE)

        # ğŸ”¥ ë””ë²„ê±° ì´ˆê¸°í™”
        self.debug = None
        if DEBUG_AVAILABLE and session_id:
            try:
                self.debug = SimulationDebugger(session_id=session_id)
                logger.debug(f"âœ… Simulation ë””ë²„ê±° ì´ˆê¸°í™” ì™„ë£Œ (session: {session_id})")
            except Exception as e:
                logger.warning(f"âš ï¸ Simulation ë””ë²„ê±° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

        if self.use_gpu:
            logger.info("ğŸš€ GPU ê°€ì† Self-play ì‹œë®¬ë ˆì´í„° ì´ˆê¸°í™”")
            self._initialize_gpu()
        else:
            logger.info("ğŸ’» CPU ê¸°ë°˜ Self-play ì‹œë®¬ë ˆì´í„° ì´ˆê¸°í™”")
    
    def _initialize_gpu(self):
        """GPU ì´ˆê¸°í™”"""
        try:
            if JAX_AVAILABLE:
                # JAX ë¡œê±° ë ˆë²¨ ì¡°ì • (ëª¨ë“  JAX ê´€ë ¨ ì—ëŸ¬ ë©”ì‹œì§€ ì–µì œ)
                import logging as std_logging
                # JAX ê´€ë ¨ ëª¨ë“  ë¡œê±° ì—ëŸ¬ ë ˆë²¨ë¡œ ì„¤ì •
                jax_loggers = [
                    std_logging.getLogger('jax._src.xla_bridge'),
                    std_logging.getLogger('jax'),
                    std_logging.getLogger('jaxlib'),
                ]
                for jax_logger in jax_loggers:
                    jax_logger.setLevel(std_logging.WARNING)  # WARNING ì´ìƒë§Œ í‘œì‹œ (ì—ëŸ¬ëŠ” í‘œì‹œ, INFO/DEBUG ìˆ¨ê¹€)
                
                # JAX í”Œë«í¼ ì„¤ì • ì‹œë„ (CUDA ìš°ì„ , ì‹¤íŒ¨ ì‹œ CPU í´ë°±)
                try:
                    # CUDA í”Œë«í¼ ê°•ì œ ì„¤ì • ì‹œë„ (RTX 5090 ì™„ì „ ì§€ì›ì„ ìœ„í•´)
                    if 'JAX_PLATFORMS' not in os.environ:
                        os.environ['JAX_PLATFORMS'] = 'cuda,cpu'  # CUDA ìš°ì„ , CPU í´ë°±
                    
                    # CUDA í”Œë«í¼ìœ¼ë¡œ ì‹œë„
                    try:
                        jax.config.update('jax_platform_name', 'cuda')
                        devices = jax.devices()
                        # ì‹¤ì œ GPU ê³„ì‚° í…ŒìŠ¤íŠ¸
                        test_array = jnp.array([1.0, 2.0, 3.0])
                        test_result = jnp.sum(test_array)
                        # GPUì—ì„œ ì‹¤ì œë¡œ ê³„ì‚°ë˜ì—ˆëŠ”ì§€ í™•ì¸
                        if any('gpu' in str(d).lower() or 'cuda' in str(d).lower() for d in devices):
                            logger.info(f"âœ… JAX GPU ë””ë°”ì´ìŠ¤ í™œì„±í™”: {devices}")
                            self.use_gpu = True
                        else:
                            logger.warning(f"âš ï¸ JAX CUDA ì„¤ì •í–ˆì§€ë§Œ GPU ë””ë°”ì´ìŠ¤ ì—†ìŒ: {devices}")
                            self.use_gpu = False
                    except RuntimeError as cuda_err:
                        # CUDA ê³„ì‚° ì‹¤íŒ¨ ì‹œ CPUë¡œ í´ë°±
                        logger.warning(f"âš ï¸ JAX CUDA ê³„ì‚° ì‹¤íŒ¨, CPUë¡œ í´ë°±: {cuda_err}")
                        jax.config.update('jax_platform_name', 'cpu')
                        devices = jax.devices()
                        logger.info(f"ğŸ’» JAX CPU ëª¨ë“œë¡œ ì „í™˜: {devices}")
                        self.use_gpu = False
                except Exception as config_err:
                    # ì „ì²´ ì„¤ì • ì‹¤íŒ¨ ì‹œ CPUë¡œ í´ë°±
                    logger.warning(f"âš ï¸ JAX í”Œë«í¼ ì„¤ì • ì‹¤íŒ¨, CPU ëª¨ë“œë¡œ ì „í™˜: {config_err}")
                    try:
                        jax.config.update('jax_platform_name', 'cpu')
                    except:
                        pass
                    self.use_gpu = False
            elif CUPY_AVAILABLE:
                # CuPy GPU ì„¤ì •
                logger.info(f"âœ… CuPy GPU ë””ë°”ì´ìŠ¤: {cp.cuda.Device()}")
        except Exception as e:
            logger.warning(f"âš ï¸ GPU ì´ˆê¸°í™” ì‹¤íŒ¨, CPU ëª¨ë“œë¡œ ì „í™˜: {e}")
            self.use_gpu = False
        
    def create_agents(
        self,
        strategy_params_list: List[Dict[str, Any]],
        agent_type: str = 'rule',
        neural_policy: Optional[Dict[str, Any]] = None,
        hybrid_config: Optional[Dict[str, Any]] = None,
        coin: Optional[str] = None
    ) -> List[StrategyAgent]:
        """
        ì—ì´ì „íŠ¸ ìƒì„± - ì‹¤ì œ ì „ëµì˜ ëª¨ë“  íŒŒë¼ë¯¸í„° í¬í•¨ (ì½”ì¸ë³„ ìµœì í™”)

        Args:
            strategy_params_list: ì „ëµ íŒŒë¼ë¯¸í„° ë¦¬ìŠ¤íŠ¸
            agent_type: 'rule' or 'hybrid'
            neural_policy: ì‹ ê²½ë§ ì •ì±… (hybrid ëª¨ë“œì¼ ë•Œ í•„ìš”)
            hybrid_config: í•˜ì´ë¸Œë¦¬ë“œ ì„¤ì • (hybrid ëª¨ë“œì¼ ë•Œ í•„ìš”)
            coin: ì½”ì¸ ì‹¬ë³¼ (ë³€ë™ì„± ê¸°ë°˜ íŒŒë¼ë¯¸í„° ì¡°ì •ìš©)

        Returns:
            ì—ì´ì „íŠ¸ ë¦¬ìŠ¤íŠ¸
        """
        # ğŸ”¥ ë³€ë™ì„± í”„ë¡œíŒŒì¼ ê¸°ë°˜ íŒŒë¼ë¯¸í„° ë²”ìœ„ (ìë™ ê³„ì‚°)
        def get_coin_specific_ranges(coin_symbol: Optional[str]):
            """ì½”ì¸ì˜ ì‹¤ì œ ë³€ë™ì„±ì„ ì¸¡ì •í•˜ì—¬ íŒŒë¼ë¯¸í„° ë²”ìœ„ ë°˜í™˜ (ìë™)"""
            try:
                # ë³€ë™ì„± í”„ë¡œíŒŒì¼ ëª¨ë“ˆ import
                from rl_pipeline.utils.coin_volatility import get_volatility_profile
                from rl_pipeline.core.env import config

                # ìë™ í”„ë¡œíŒŒì¼ ê³„ì‚°
                profile = get_volatility_profile(coin_symbol, config.RL_DB)

                return {
                    'stop_loss': profile['stop_loss'],
                    'take_profit': profile['take_profit'],
                    'position_size': profile['position_size'],
                    'volatility_label': profile['volatility_group']
                }

            except Exception as e:
                # Import ì‹¤íŒ¨ ì‹œ í´ë°± (ê¸°ë³¸ê°’)
                logger.warning(f"âš ï¸ ë³€ë™ì„± í”„ë¡œíŒŒì¼ ë¡œë“œ ì‹¤íŒ¨, ê¸°ë³¸ê°’ ì‚¬ìš©: {e}")
                return {
                    'stop_loss': (0.02, 0.035),
                    'take_profit': (0.04, 0.08),
                    'position_size': (0.06, 0.15),
                    'volatility_label': 'DEFAULT'
                }

        # ì½”ì¸ë³„ ë²”ìœ„ ê°€ì ¸ì˜¤ê¸°
        ranges = get_coin_specific_ranges(coin)
        volatility_label = ranges.get('volatility_label', 'UNKNOWN')

        # ì½”ì¸ ì •ë³´ ë¡œê·¸ (1íšŒë§Œ)
        if coin:
            logger.info(f"ğŸ¯ ì½”ì¸ë³„ íŒŒë¼ë¯¸í„° ë²”ìœ„ ì ìš©: {coin} (ë³€ë™ì„±: {volatility_label})")
            logger.info(f"   Stop Loss: {ranges['stop_loss'][0]:.1%}~{ranges['stop_loss'][1]:.1%}")
            logger.info(f"   Take Profit: {ranges['take_profit'][0]:.1%}~{ranges['take_profit'][1]:.1%}")
            logger.info(f"   Position Size: {ranges['position_size'][0]:.1%}~{ranges['position_size'][1]:.1%}")

        agents = []
        for i, params in enumerate(strategy_params_list):
            agent_id = f"agent_{i+1}"
            
            # í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë“œì¸ ê²½ìš° HybridPolicyAgent ì‚¬ìš©
            if agent_type == 'hybrid':
                try:
                    from rl_pipeline.hybrid.hybrid_policy_agent import HybridPolicyAgent
                    
                    agent = HybridPolicyAgent(
                        agent_id=agent_id,
                        strategy_params=params,
                        neural_policy=neural_policy,
                        enable_neural=hybrid_config.get('enable_neural', False) if hybrid_config else False,
                        use_neural_threshold=hybrid_config.get('use_neural_threshold', 0.3) if hybrid_config else 0.3,
                        max_latency_ms=hybrid_config.get('max_latency_ms', 10.0) if hybrid_config else 10.0
                    )
                    agents.append(agent)
                    continue
                except ImportError as e:
                    logger.warning(f"âš ï¸ í•˜ì´ë¸Œë¦¬ë“œ ì—ì´ì „íŠ¸ ìƒì„± ì‹¤íŒ¨, ê·œì¹™ ê¸°ë°˜ìœ¼ë¡œ í´ë°±: {e}")
                    # í´ë°±: ê·œì¹™ ê¸°ë°˜ ì—ì´ì „íŠ¸ë¡œ ê³„ì†
            
            # ê·œì¹™ ê¸°ë°˜ ì—ì´ì „íŠ¸ ìƒì„± (ê¸°ì¡´ ë¡œì§)
            # ì‹¤ì œ ì „ëµì—ì„œ ì‚¬ìš©í•˜ëŠ” ëª¨ë“  íŒŒë¼ë¯¸í„°ë“¤ì„ í¬í•¨í•œ ì™„ì „í•œ íŒŒë¼ë¯¸í„° ì„¸íŠ¸ ìƒì„±
            complete_params = {
                # ê¸°ë³¸ ì§€í‘œ íŒŒë¼ë¯¸í„° (ë°˜ì˜¬ë¦¼ ì ìš©)
                'rsi_min': round(params.get('rsi_min', np.random.uniform(20, 40)), 1),
                'rsi_max': round(params.get('rsi_max', np.random.uniform(60, 80)), 1),
                'volume_ratio_min': round(params.get('volume_ratio_min', np.random.uniform(0.8, 1.5)), 2),
                'volume_ratio_max': round(params.get('volume_ratio_max', np.random.uniform(2.0, 4.0)), 2),
                'macd_buy_threshold': round(params.get('macd_buy_threshold', np.random.uniform(-0.01, 0.01)), 4),
                'macd_sell_threshold': round(params.get('macd_sell_threshold', np.random.uniform(-0.01, 0.01)), 4),
                
                # ì¶”ê°€ ì§€í‘œ íŒŒë¼ë¯¸í„° (ë°˜ì˜¬ë¦¼ ì ìš©)
                'mfi_min': round(params.get('mfi_min', np.random.uniform(10, 30)), 1),
                'mfi_max': round(params.get('mfi_max', np.random.uniform(70, 90)), 1),
                'atr_min': round(params.get('atr_min', np.random.uniform(0.005, 0.02)), 3),
                'atr_max': round(params.get('atr_max', np.random.uniform(0.03, 0.08)), 3),
                'adx_min': round(params.get('adx_min', np.random.uniform(15, 30)), 1),
                
                # ğŸ”¥ ë¦¬ìŠ¤í¬ ê´€ë¦¬ íŒŒë¼ë¯¸í„° (ì½”ì¸ë³„ ìµœì í™” ì ìš©)
                'stop_loss_pct': round(params.get('stop_loss_pct',
                    np.random.uniform(*ranges['stop_loss'])), 3),
                'take_profit_pct': round(params.get('take_profit_pct',
                    np.random.uniform(*ranges['take_profit'])), 2),
                'position_size': round(params.get('position_size',
                    np.random.uniform(*ranges['position_size'])), 3),
                
                # ê¸°ìˆ ì  ë¶„ì„ íŒŒë¼ë¯¸í„° (ë°˜ì˜¬ë¦¼ ì ìš©)
                'bb_period': params.get('bb_period', np.random.randint(15, 25)),
                'bb_std': round(params.get('bb_std', np.random.uniform(1.5, 2.5)), 2),
                'ma_period': params.get('ma_period', np.random.randint(10, 30)),
                
                # ì „ëµ íƒ€ì…ë³„ íŠ¹í™” íŒŒë¼ë¯¸í„° (ë°˜ì˜¬ë¦¼ ì ìš©)
                'strategy_type': params.get('strategy_type', 'comprehensive'),
                'risk_level': params.get('risk_level', 'medium'),
                'aggressiveness': round(params.get('aggressiveness', np.random.uniform(0.3, 0.8)), 2)
            }
            
            agent = StrategyAgent(agent_id, complete_params)
            agents.append(agent)
            
            # ğŸ” ì²« 2ê°œ agentë§Œ ìƒì„¸ ë¡œê·¸ ì¶œë ¥ (ìƒì„¸ ì •ë³´ëŠ” DEBUG ë ˆë²¨ë¡œ ë³€ê²½)
            if len(agents) <= 2 and AZ_SIMULATION_VERBOSE:
                logger.debug(f"  Agent {agent_id}: RSI={complete_params.get('rsi_min')}-{complete_params.get('rsi_max')}, "
                           f"StopLoss={complete_params.get('stop_loss_pct')}, TakeProfit={complete_params.get('take_profit_pct')}")
            
        # ì—ì´ì „íŠ¸ ìƒì„± ì™„ë£Œ ë¡œê·¸ëŠ” DEBUG ë ˆë²¨ë¡œ ë³€ê²½ (ì¤‘ë³µ ì œê±°)
        logger.debug(f"ğŸ¯ {len(agents)}ê°œ ì—ì´ì „íŠ¸ ìƒì„± ì™„ë£Œ (ì‹¤ì œ ì „ëµ íŒŒë¼ë¯¸í„° í¬í•¨)")
        return agents
    
    def _convert_candle_to_market_state(self, row: pd.Series) -> MarketState:
        """ğŸ”¥ ì‹¤ì œ ìº”ë“¤ ë°ì´í„°ë¥¼ MarketStateë¡œ ë³€í™˜"""
        try:
            # ğŸ”¥ None ê°’ ì•ˆì „ ì²˜ë¦¬ í•¨ìˆ˜
            def safe_float(value, default=0.0):
                """None ê°’ ì•ˆì „ ì²˜ë¦¬"""
                if value is None:
                    return default
                try:
                    return float(value)
                except (ValueError, TypeError):
                    return default
            
            # ìº”ë“¤ ë°ì´í„°ì—ì„œ ì‹œì¥ ìƒíƒœ ìƒì„±
            timestamp_val = row.get('timestamp')
            if timestamp_val is None:
                timestamp = datetime.now()
            else:
                timestamp = pd.to_datetime(timestamp_val)
            
            # ê¸°ë³¸ ê°€ê²© ì¶”ì¶œ (close ìš°ì„ , ì—†ìœ¼ë©´ open, high, low ì¤‘ í•˜ë‚˜)
            price = safe_float(row.get('close')) or safe_float(row.get('price')) or safe_float(row.get('open')) or 50000.0
            volume = safe_float(row.get('volume'), 1000000.0)
            rsi = safe_float(row.get('rsi'), 50.0)
            macd = safe_float(row.get('macd'), 0.0)
            macd_signal = safe_float(row.get('macd_signal'), 0.0)
            
            # BB ë°´ë“œëŠ” price ê¸°ë°˜ìœ¼ë¡œ ê¸°ë³¸ê°’ ê³„ì‚°
            bb_upper = safe_float(row.get('bb_upper'), price * 1.02)
            bb_middle = safe_float(row.get('bb_middle'), price * 1.0)
            bb_lower = safe_float(row.get('bb_lower'), price * 0.98)
            volume_ratio = safe_float(row.get('volume_ratio'), 1.0)
            mfi = safe_float(row.get('mfi'), 50.0)
            atr = safe_float(row.get('atr'), price * 0.02)
            adx = safe_float(row.get('adx'), 25.0)
            
            # ë ˆì§ ì¶”ì • (RSIì™€ MACD ê¸°ë°˜)
            if rsi < 30 and macd < -0.01:
                regime_label = "extreme_bearish"
                regime_stage = 0
            elif rsi < 40:
                regime_label = "bearish"
                regime_stage = 1
            elif rsi < 50:
                regime_label = "sideways_bearish"
                regime_stage = 2
            elif rsi > 70 and macd > 0.01:
                regime_label = "extreme_bullish"
                regime_stage = 6
            elif rsi > 60:
                regime_label = "bullish"
                regime_stage = 5
            elif rsi > 50:
                regime_label = "sideways_bullish"
                regime_stage = 4
            else:
                regime_label = "neutral"
                regime_stage = 3
            
            volatility = float(row.get('atr', price * 0.02)) / price if price > 0 else 0.02
            
            return MarketState(
                timestamp=timestamp,
                price=price,
                volume=volume,
                rsi=rsi,
                macd=macd,
                macd_signal=macd_signal,
                bb_upper=bb_upper,
                bb_middle=bb_middle,
                bb_lower=bb_lower,
                volume_ratio=volume_ratio,
                regime_stage=regime_stage,
                regime_label=regime_label,
                regime_confidence=0.7,  # ì‹¤ì œ ë°ì´í„°ì´ë¯€ë¡œ ë†’ì€ ì‹ ë¢°ë„
                volatility=volatility,
                mfi=mfi,
                atr=atr,
                adx=adx
            )
        except Exception as e:
            logger.error(f"âŒ ìº”ë“¤ ë°ì´í„° ë³€í™˜ ì‹¤íŒ¨: {e}, ê¸°ë³¸ê°’ ë°˜í™˜")
            # ê¸°ë³¸ê°’ ë°˜í™˜
            return self.market_generator.generate_next_candle()
    
    def run_self_play_episode(self, agents: List[StrategyAgent], steps: int = 1000, candle_data: pd.DataFrame = None) -> Dict[str, Any]:
        """ğŸš€ í•˜ì´ë¸Œë¦¬ë“œ Self-play ì—í”¼ì†Œë“œ ì‹¤í–‰ (GPU ìŠ¤í¬ë¦¬ë‹ + CPU ì •ë°€ í‰ê°€)
        
        Args:
            agents: ì—ì´ì „íŠ¸ ë¦¬ìŠ¤íŠ¸
            steps: ì‹œë®¬ë ˆì´ì…˜ ìŠ¤í… ìˆ˜
            candle_data: ì‹¤ì œ ìº”ë“¤ ë°ì´í„° (Noneì´ë©´ ê°€ìƒ ë°ì´í„° ì‚¬ìš©) ğŸ”¥
        """
        try:
            logger.info(f"ğŸš€ Self-play ì—í”¼ì†Œë“œ {self.episode_count + 1} ì‹œì‘ ({len(agents)}ê°œ ì—ì´ì „íŠ¸)")
            
            # ğŸ”¥ í•˜ì´ë¸Œë¦¬ë“œ ì „ëµ: GPU ìŠ¤í¬ë¦¬ë‹ + CPU ì •ë°€ í‰ê°€ + ì‹¤ì œ ìº”ë“¤ ë°ì´í„°
            if self.use_gpu:
                return self._run_hybrid_episode(agents, steps, candle_data)
            else:
                return self._run_cpu_episode(agents, steps, candle_data)
            
        except Exception as e:
            logger.error(f"âŒ Self-play ì—í”¼ì†Œë“œ ì‹¤íŒ¨: {e}")
            return {"status": "failed", "error": str(e)}
    
    def _run_cpu_episode(self, agents: List[StrategyAgent], steps: int, candle_data: pd.DataFrame = None) -> Dict[str, Any]:
        """CPU ê¸°ë°˜ ì—í”¼ì†Œë“œ ì‹¤í–‰ - ì‹¤ì œ ìº”ë“¤ ë°ì´í„° ì§€ì›
        
        Args:
            agents: ì—ì´ì „íŠ¸ ë¦¬ìŠ¤íŠ¸
            steps: ì‹œë®¬ë ˆì´ì…˜ ìŠ¤í… ìˆ˜
            candle_data: ì‹¤ì œ ìº”ë“¤ ë°ì´í„° (Noneì´ë©´ ê°€ìƒ ë°ì´í„° ì‚¬ìš©) ğŸ”¥
        """
        # ğŸ”¥ ì—ì´ì „íŠ¸ ìƒíƒœ ì´ˆê¸°í™” (ë§¤ ì—í”¼ì†Œë“œë§ˆë‹¤)
        for agent in agents:
            agent.state = AgentState(
                balance=10000.0,
                position=None,
                trades=[],
                equity_curve=[10000.0],
                strategy_params=agent.strategy_params
            )
        
        # ğŸ¯ ì‹¤ì œ ìº”ë“¤ ë°ì´í„° ì‚¬ìš© ì—¬ë¶€ í™•ì¸
        use_real_data = candle_data is not None and len(candle_data) > 0
        
        if use_real_data:
            logger.info(f"âœ… ì‹¤ì œ ìº”ë“¤ ë°ì´í„° ì‚¬ìš©: {len(candle_data)}ê°œ, {steps}ìŠ¤í…")
            # ì‹¤ì œ ë°ì´í„°ë¥¼ stepsë§Œí¼ ì‚¬ìš© (ë˜ëŠ” ë°ì´í„° ê¸¸ì´ë§Œí¼)
            actual_steps = min(steps, len(candle_data))
            
            # 7ë‹¨ê³„ ë ˆì§ ì¶”ì • (ì‹¤ì œ ë°ì´í„° ê¸°ë°˜)
            # í˜„ì¬ëŠ” ê°„ë‹¨í•˜ê²Œ ëœë¤ ì„ íƒ (í–¥í›„ ì‹¤ì œ ë ˆì§ ê³„ì‚° ë¡œì§ ì¶”ê°€ ê°€ëŠ¥)
            regime_labels = ["extreme_bearish", "bearish", "sideways_bearish", "neutral", 
                           "sideways_bullish", "bullish", "extreme_bullish"]
            regime_label = random.choice(regime_labels)
            logger.info(f"ğŸ“Š ì‹œì¥ ë ˆì§: {regime_label}")
            
            # ì‹¤ì œ ìº”ë“¤ ë°ì´í„° ì‚¬ìš©
            for idx, (_, row) in enumerate(candle_data.head(actual_steps).iterrows()):
                # ìº”ë“¤ ë°ì´í„°ë¥¼ MarketStateë¡œ ë³€í™˜
                market_state = self._convert_candle_to_market_state(row)
                
                # ğŸ”¥ MFE/MAE ê³„ì‚°ìš© ê³ ê°€/ì €ê°€ ì¶”ì¶œ
                current_high = row.get('high', market_state.price)
                current_low = row.get('low', market_state.price)
                
                # ê° ì—ì´ì „íŠ¸ì˜ í–‰ë™ ê²°ì • ë° ì‹¤í–‰
                for agent in agents:
                    # 1. ë³´ìœ  ì¤‘ì¸ í¬ì§€ì…˜ì˜ ê³ ê°€/ì €ê°€ ê°±ì‹  (MFE/MAE ì¶”ì )
                    if agent.state.position is not None:
                        if 'max_price' not in agent.state.position:
                            agent.state.position['max_price'] = agent.state.position['entry_price']
                        if 'min_price' not in agent.state.position:
                            agent.state.position['min_price'] = agent.state.position['entry_price']
                        
                        agent.state.position['max_price'] = max(agent.state.position['max_price'], current_high)
                        agent.state.position['min_price'] = min(agent.state.position['min_price'], current_low)
                    
                    # ì²­ì‚° ì „ ìƒíƒœ ë°±ì—…
                    position_stats = {}
                    if agent.state.position is not None:
                        position_stats = {
                            'max_price': agent.state.position.get('max_price', agent.state.position['entry_price']),
                            'min_price': agent.state.position.get('min_price', agent.state.position['entry_price']),
                            'entry_price': agent.state.position['entry_price']
                        }

                    action = agent.decide_action(market_state)
                    trade_result = agent.execute_action(action, market_state)

                    # 2. ì²­ì‚° ì‹œ MFE/MAE ê¸°ë¡
                    if action == Action.SELL and trade_result.get("type") == "SELL" and position_stats:
                        entry_price = position_stats['entry_price']
                        if entry_price > 0:
                            mfe_pct = ((position_stats['max_price'] - entry_price) / entry_price) * 100
                            mae_pct = ((position_stats['min_price'] - entry_price) / entry_price) * 100
                            
                            if agent.state.trades:
                                agent.state.trades[-1]['mfe_pct'] = mfe_pct
                                agent.state.trades[-1]['mae_pct'] = mae_pct
        else:
            # ê°€ìƒ ë°ì´í„° ìƒì„± (ê¸°ì¡´ ë°©ì‹)
            # 7ë‹¨ê³„ ë ˆì§ ëœë¤ ì„¤ì •
            regime_labels = ["extreme_bearish", "bearish", "sideways_bearish", "neutral", 
                           "sideways_bullish", "bullish", "extreme_bullish"]
            regime_label = random.choice(regime_labels)
            self.market_generator.update_market_regime(regime_label)
            logger.info(f"ğŸ“Š ì‹œì¥ ë ˆì§: {regime_label}")
            
            # ì—í”¼ì†Œë“œ ì‹¤í–‰
            for step in range(steps):
                # ì‹œì¥ ìƒíƒœ ìƒì„±
                market_state = self.market_generator.generate_next_candle()
                
                # ê° ì—ì´ì „íŠ¸ì˜ í–‰ë™ ê²°ì • ë° ì‹¤í–‰
                for agent in agents:
                    action = agent.decide_action(market_state)
                    trade_result = agent.execute_action(action, market_state)
                
                # ìƒì„¸ ë¡œê·¸ (ì²˜ìŒ 10ìŠ¤í…ë§Œ)
                if step < 10:
                    logger.debug(f"Step {step}: {agent.agent_id} -> {action.value} @ {market_state.price:.2f} (ë ˆì§: {market_state.regime_label})")
        
        # ğŸ”¥ ì—í”¼ì†Œë“œ ì¢…ë£Œ: ì—´ë¦° í¬ì§€ì…˜ ê°•ì œ ì²­ì‚°
        last_market_state = None
        if use_real_data and len(candle_data) > 0:
            # ë§ˆì§€ë§‰ ìº”ë“¤ë¡œ ê°•ì œ ì²­ì‚°
            last_row = candle_data.iloc[min(actual_steps - 1, len(candle_data) - 1)]
            last_market_state = self._convert_candle_to_market_state(last_row)
        elif not use_real_data:
            # ê°€ìƒ ë°ì´í„°ì˜ ë§ˆì§€ë§‰ ìƒíƒœ
            last_market_state = market_state

        # ëª¨ë“  ì—ì´ì „íŠ¸ì˜ ì—´ë¦° í¬ì§€ì…˜ ì²­ì‚°
        if last_market_state is not None:
            for agent in agents:
                if agent.state.position is not None:
                    logger.info(f"ğŸ”š {agent.agent_id} ì—í”¼ì†Œë“œ ì¢…ë£Œ: ì—´ë¦° í¬ì§€ì…˜ ê°•ì œ ì²­ì‚° (240m ë””ë²„ê·¸)")
                    agent.execute_action(Action.SELL, last_market_state)

        # ì—í”¼ì†Œë“œ ê²°ê³¼ ìˆ˜ì§‘
        episode_results = {}
        for agent in agents:
            performance = agent.get_performance_metrics()
            # ğŸ”¥ ì „ëµ íŒŒë¼ë¯¸í„°ë„ í•¨ê»˜ ì €ì¥ (ì§„í™”ëœ ì „ëµ ì €ì¥ìš©)
            performance['strategy_params'] = agent.strategy_params
            episode_results[agent.agent_id] = performance
            
            # ìˆ˜ìµë¥  ê³„ì‚° (ì´ˆê¸° ìë³¸ ëŒ€ë¹„)
            initial_capital = 10000.0  # ì´ˆê¸° ìë³¸
            total_return_pct = (performance['total_pnl'] / initial_capital) * 100
            
            # ğŸ“ˆ ê° ì—ì´ì „íŠ¸ ì„±ê³¼ ë¡œê·¸ ì¶œë ¥
            logger.info(f"ğŸ“ˆ {agent.agent_id} ì„±ê³¼: "
                      f"ê±°ë˜ {performance['total_trades']}íšŒ, "
                      f"ìŠ¹ë¥  {performance['win_rate']:.2%}, "
                      f"ìˆ˜ìµë¹„ {total_return_pct:+.2f}%, "
                      f"ìƒ¤í”„ {performance['sharpe_ratio']:.4f}, "
                      f"ìµœëŒ€ë‚™í­ {performance['max_drawdown']:.2%}")
        
        # í•™ìŠµ ë°ì´í„° ìˆ˜ì§‘
        self.learning_history.append({
            "episode": self.episode_count,
            "regime_label": regime_label,
            "steps": steps,
            "results": episode_results,
            "timestamp": datetime.now()
        })
        
        self.episode_count += 1
        
        return {
            "episode": self.episode_count - 1,
            "regime_label": regime_label,
            "steps": steps,
            "results": episode_results,
            "status": "success",
            "execution_mode": "CPU"
        }
    
    def _run_hybrid_episode(self, agents: List[StrategyAgent], steps: int, candle_data: pd.DataFrame = None) -> Dict[str, Any]:
        """ğŸ”¥ í•˜ì´ë¸Œë¦¬ë“œ ì—í”¼ì†Œë“œ: GPU ìŠ¤í¬ë¦¬ë‹ + CPU ì •ë°€ í‰ê°€ + ì‹¤ì œ ìº”ë“¤ ë°ì´í„°
        
        Args:
            agents: ì—ì´ì „íŠ¸ ë¦¬ìŠ¤íŠ¸
            steps: ì‹œë®¬ë ˆì´ì…˜ ìŠ¤í… ìˆ˜
            candle_data: ì‹¤ì œ ìº”ë“¤ ë°ì´í„° (Noneì´ë©´ ê°€ìƒ ë°ì´í„° ì‚¬ìš©) ğŸ”¥
        """
        try:
            logger.info("ğŸš€ í•˜ì´ë¸Œë¦¬ë“œ Self-play ì‹œì‘ (GPU ìŠ¤í¬ë¦¬ë‹ â†’ CPU ì •ë°€ í‰ê°€)")
            
            # ğŸ¯ ì‹¤ì œ ìº”ë“¤ ë°ì´í„° ì‚¬ìš© ì—¬ë¶€ í™•ì¸
            use_real_data = candle_data is not None and len(candle_data) > 0
            
            # 7ë‹¨ê³„ ë ˆì§ ì„¤ì •
            regime_labels = ["extreme_bearish", "bearish", "sideways_bearish", "neutral", 
                           "sideways_bullish", "bullish", "extreme_bullish"]
            regime_label = random.choice(regime_labels)
            
            if not use_real_data:
                self.market_generator.update_market_regime(regime_label)
            
            logger.info(f"ğŸ“Š ì‹œì¥ ë ˆì§: {regime_label}")
            
            # 1ë‹¨ê³„: GPU ë¹ ë¥¸ ìŠ¤í¬ë¦¬ë‹ (ê°„ì†Œí™” ì‹œë®¬ë ˆì´ì…˜)
            logger.info("âš¡ 1ë‹¨ê³„: GPU ë¹ ë¥¸ ìŠ¤í¬ë¦¬ë‹ ì‹œì‘")
            # ìŠ¤í¬ë¦¬ë‹ ìŠ¤í…: 250 (ê¸°ë³¸) â†’ 300 (ê°œì„ )
            screening_steps = int(steps * 0.3)  # 30% ìŠ¤í… ì‚¬ìš© (250 â†’ 300ìœ¼ë¡œ ì¦ê°€)
            screening_results = self._gpu_fast_screening(agents, screening_steps, candle_data)  # ğŸ”¥ ì‹¤ì œ ìº”ë“¤ ë°ì´í„° ì „ë‹¬
            
            # 2ë‹¨ê³„: ëª¨ë“  ì „ëµ í…ŒìŠ¤íŠ¸ (ì„ ë³„ ì œê±° - 100% í¬í•¨)
            # ğŸ”¥ ëª¨ë“  ì „ëµì„ self-playì— í¬í•¨í•˜ì—¬ UNKNOWN ë“±ê¸‰ ì „ëµë„ ê²€ì¦
            num_agents = len(agents)
            
            # ğŸ”¥ ë³€ê²½: ì„ ë³„ ì œê±°, ëª¨ë“  ì „ëµ í¬í•¨
            top_agents = agents  # ëª¨ë“  ì „ëµ ì‚¬ìš© (UNKNOWN í¬í•¨)
            logger.info(f"âœ… ì „ì²´ {len(top_agents)}ê°œ ì „ëµ self-play ì‹¤í–‰ (UNKNOWN ë“±ê¸‰ í¬í•¨, ì„ ë³„ ì—†ìŒ)")
            
            # 3ë‹¨ê³„: CPU ì •ë°€ í‰ê°€ (ì„ ë³„ëœ ì „ëµë§Œ) + ì‹¤ì œ ìº”ë“¤ ë°ì´í„° ğŸ”¥
            logger.info("ğŸ¯ 2ë‹¨ê³„: CPU ì •ë°€ í‰ê°€ ì‹œì‘")
            # ì •ë°€ ê²€ì¦ ìŠ¤í…: ì „ì²´ ìŠ¤í… ì‚¬ìš© (ê¸°ë³¸ steps=1000)
            precise_steps = steps  # 400~600 íŠœë‹ ê°€ëŠ¥
            precise_results = {}
            for agent in top_agents:
                agent.state = AgentState(
                    balance=10000.0,
                    position=None,
                    trades=[],
                    equity_curve=[10000.0],
                    strategy_params=agent.strategy_params
                )
                
                # ğŸ”¥ ì‹¤ì œ ìº”ë“¤ ë°ì´í„° ì‚¬ìš© ì—¬ë¶€ì— ë”°ë¼ ì‹œë®¬ë ˆì´ì…˜
                last_market_state = None
                if use_real_data:
                    # ì‹¤ì œ ìº”ë“¤ ë°ì´í„° ì‚¬ìš©
                    actual_steps = min(precise_steps, len(candle_data))
                    for idx, (_, row) in enumerate(candle_data.head(actual_steps).iterrows()):
                        market_state = self._convert_candle_to_market_state(row)
                        
                        # ğŸ”¥ MFE/MAE ê³„ì‚°ìš© ê³ ê°€/ì €ê°€ ì¶”ì¶œ
                        current_high = row.get('high', market_state.price)
                        current_low = row.get('low', market_state.price)
                        
                        # 1. ë³´ìœ  ì¤‘ì¸ í¬ì§€ì…˜ì˜ ê³ ê°€/ì €ê°€ ê°±ì‹  (MFE/MAE ì¶”ì )
                        if agent.state.position is not None:
                            if 'max_price' not in agent.state.position:
                                agent.state.position['max_price'] = agent.state.position['entry_price']
                            if 'min_price' not in agent.state.position:
                                agent.state.position['min_price'] = agent.state.position['entry_price']
                            
                            agent.state.position['max_price'] = max(agent.state.position['max_price'], current_high)
                            agent.state.position['min_price'] = min(agent.state.position['min_price'], current_low)
                        
                        # ì²­ì‚° ì „ ìƒíƒœ ë°±ì—…
                        position_stats = {}
                        if agent.state.position is not None:
                            position_stats = {
                                'max_price': agent.state.position.get('max_price', agent.state.position['entry_price']),
                                'min_price': agent.state.position.get('min_price', agent.state.position['entry_price']),
                                'entry_price': agent.state.position['entry_price']
                            }

                        action = agent.decide_action(market_state)
                        trade_result = agent.execute_action(action, market_state)
                        last_market_state = market_state  # ë§ˆì§€ë§‰ ìƒíƒœ ì €ì¥
                        
                        # 2. ì²­ì‚° ì‹œ MFE/MAE ê¸°ë¡
                        if action == Action.SELL and trade_result.get("type") == "SELL" and position_stats:
                            entry_price = position_stats['entry_price']
                            if entry_price > 0:
                                mfe_pct = ((position_stats['max_price'] - entry_price) / entry_price) * 100
                                mae_pct = ((position_stats['min_price'] - entry_price) / entry_price) * 100
                                
                                if agent.state.trades:
                                    agent.state.trades[-1]['mfe_pct'] = mfe_pct
                                    agent.state.trades[-1]['mae_pct'] = mae_pct
                else:
                    # ê°€ìƒ ë°ì´í„° ìƒì„± (ê¸°ì¡´ ë°©ì‹)
                    for step in range(precise_steps):
                        market_state = self.market_generator.generate_next_candle()
                        action = agent.decide_action(market_state)
                        trade_result = agent.execute_action(action, market_state)
                        last_market_state = market_state  # ë§ˆì§€ë§‰ ìƒíƒœ ì €ì¥

                # ğŸ”¥ ì—í”¼ì†Œë“œ ì¢…ë£Œ: ì—´ë¦° í¬ì§€ì…˜ ê°•ì œ ì²­ì‚°
                # ë””ë²„ê·¸: ì¡°ê±´ í™•ì¸
                has_position = agent.state.position is not None
                has_last_state = last_market_state is not None
                logger.info(f"ğŸ” {agent.agent_id} ì²­ì‚° ì²´í¬: position={has_position}, last_state={has_last_state}")

                if agent.state.position is not None and last_market_state is not None:
                    logger.info(f"ğŸ”š {agent.agent_id} ì‹œë®¬ë ˆì´ì…˜ ì¢…ë£Œ: ì—´ë¦° í¬ì§€ì…˜ ê°•ì œ ì²­ì‚° (í•˜ì´ë¸Œë¦¬ë“œ)")
                    agent.execute_action(Action.SELL, last_market_state)

                performance = agent.get_performance_metrics()
                precise_results[agent.agent_id] = performance
                
                # ğŸ“ˆ ì„±ê³¼ ë¡œê·¸
                total_return_pct = (performance['total_pnl'] / 10000.0) * 100
                logger.info(f"ğŸ“ˆ {agent.agent_id} ì„±ê³¼: "
                          f"ê±°ë˜ {performance['total_trades']}íšŒ, "
                          f"ìŠ¹ë¥  {performance['win_rate']:.2%}, "
                          f"ìˆ˜ìµë¹„ {total_return_pct:+.2f}%, "
                          f"ìƒ¤í”„ {performance['sharpe_ratio']:.4f}, "
                          f"ìµœëŒ€ë‚™í­ {performance['max_drawdown']:.2%}")
            
            # 4ë‹¨ê³„: ê²°ê³¼ í†µí•©
            episode_results = precise_results.copy()
            for agent_id, result in screening_results.items():
                if agent_id not in episode_results:
                    episode_results[agent_id] = result
            
            # í•™ìŠµ ë°ì´í„° ìˆ˜ì§‘
            self.learning_history.append({
                "episode": self.episode_count,
                "regime_label": regime_label,
                "steps": steps,
                "results": episode_results,
                "timestamp": datetime.now()
            })
            
            self.episode_count += 1
            
            logger.info(f"âœ… í•˜ì´ë¸Œë¦¬ë“œ Self-play ì™„ë£Œ: {len(episode_results)}ê°œ ê²°ê³¼")
            
            return {
                "episode": self.episode_count - 1,
                "regime_label": regime_label,
                "steps": steps,
                "results": episode_results,
                "status": "success",
                "execution_mode": "Hybrid"
            }
            
        except Exception as e:
            logger.error(f"âŒ í•˜ì´ë¸Œë¦¬ë“œ ì—í”¼ì†Œë“œ ì‹¤íŒ¨: {e}")
            logger.info("ğŸ’» CPU ëª¨ë“œë¡œ ì „í™˜í•˜ì—¬ ì¬ì‹œë„")
            return self._run_cpu_episode(agents, steps, candle_data)
    
    def _gpu_fast_screening(self, agents: List[StrategyAgent], screening_steps: int, candle_data: pd.DataFrame = None) -> Dict[str, Dict[str, Any]]:
        """âš¡ GPU ë¹ ë¥¸ ìŠ¤í¬ë¦¬ë‹: GPU ë³‘ë ¬ ì²˜ë¦¬ë¡œ ëª¨ë“  ì „ëµì„ ë™ì‹œì— ê³„ì‚° (10~50ë°° ë¹ ë¦„)
        
        Args:
            agents: ì—ì´ì „íŠ¸ ë¦¬ìŠ¤íŠ¸
            screening_steps: ìŠ¤í¬ë¦¬ë‹ ìŠ¤í… ìˆ˜
            candle_data: ì‹¤ì œ ìº”ë“¤ ë°ì´í„° (Noneì´ë©´ ê°€ìƒ ë°ì´í„° ì‚¬ìš©) ğŸ”¥
        """
        try:
            if not JAX_AVAILABLE or not self.use_gpu:
                logger.warning("âš ï¸ GPU ì‚¬ìš© ë¶ˆê°€, CPU í´ë°±")
                return self._cpu_fast_screening(agents, screening_steps, candle_data)
            
            logger.info(f"ğŸš€ GPU ë³‘ë ¬ ìŠ¤í¬ë¦¬ë‹ ì‹œì‘: {len(agents)}ê°œ ì „ëµ, {screening_steps}ìŠ¤í…")
            
            # 1. ì „ëµ íŒŒë¼ë¯¸í„°ë¥¼ JAX ë°°ì—´ë¡œ ë³€í™˜ (ë°°ì¹˜ ì²˜ë¦¬ ì¤€ë¹„)
            agent_params = []
            agent_params_dict = {}
            for agent in agents:
                params = jnp.array([
                    agent.strategy_params.get('rsi_min', 30.0),
                    agent.strategy_params.get('rsi_max', 70.0),
                    agent.strategy_params.get('volume_ratio_min', 1.0),
                    agent.strategy_params.get('macd_buy_threshold', 0.01),
                    agent.strategy_params.get('stop_loss_pct', 0.02),
                    agent.strategy_params.get('take_profit_pct', 0.04)
                ])
                agent_params.append(params)
                agent_params_dict[agent.agent_id] = params
            
            # ëª¨ë“  íŒŒë¼ë¯¸í„°ë¥¼ í•œë²ˆì— ìŠ¤íƒ (ë°°ì¹˜ ìƒì„±)
            params_batch = jnp.stack(agent_params)
            
            # 2. GPUì—ì„œ ëª¨ë“  ì „ëµì„ ë³‘ë ¬ë¡œ ì‹œë®¬ë ˆì´ì…˜
            @jit
            def simulate_agents_batch_gpu(params_batch, market_data):
                """ğŸ”¥ GPU ë³‘ë ¬ ì‹œë®¬ë ˆì´ì…˜ (Pure function)"""
                # params_batch: (N, 6) - ì „ëµ íŒŒë¼ë¯¸í„° [rsi_min, rsi_max, volume, macd, stop_loss, take_profit]
                # market_data: (steps, 4) - ì‹œì¥ ë°ì´í„° [price, volume, rsi, macd]
                
                # ì „ëµ íŒŒë¼ë¯¸í„°ì™€ ì‹œì¥ ë°ì´í„°ë¥¼ ë¸Œë¡œë“œìºìŠ¤íŒ…í•˜ì—¬ ê°€ì¤‘ì¹˜ ê³„ì‚°
                # (N, 1, 4) * (1, steps, 4) -> (N, steps, 4)
                agent_params_4d = params_batch[:, None, :4]  # (N, 1, 4) - RSI/Volume/MACD íŒŒë¼ë¯¸í„°
                market_4d = market_data[None, :, :]  # (1, steps, 4) - ì‹œì¥ ë°ì´í„°
                weighted = agent_params_4d * market_4d  # (N, steps, 4)
                
                # ê° ìŠ¤í…ì—ì„œ ì‹œê·¸ë„ ê³„ì‚° (ê°€ì¤‘í•©)
                signals = jnp.sum(weighted, axis=2)  # (N, steps) - ê° ì „ëµì˜ ê° ìŠ¤í… ì‹ í˜¸
                
                # í¬ì§€ì…˜ ê´€ë¦¬ (ê°„ì†Œí™”: -1=ë§¤ë„, 0=í™€ë“œ, 1=ë§¤ìˆ˜)
                positions = jnp.sign(signals)  # (N, steps)
                
                # ìˆ˜ìµë¥  ê³„ì‚° (í¬ì§€ì…˜ * ê°€ê²© ë³€í™”)
                # positions: (N, steps)
                # market_data prices: (steps,)
                price_changes = market_data[:, 0]  # (steps,) - ê°€ê²© ë³€í™”
                returns_per_step = positions * price_changes[None, :]  # (N, steps) - ë¸Œë¡œë“œìºìŠ¤íŒ…
                returns = jnp.sum(returns_per_step, axis=1)  # (N,) - ê° ì „ëµì˜ ì´ ìˆ˜ìµ
                
                return returns
            
            # ğŸ¯ ì‹¤ì œ ìº”ë“¤ ë°ì´í„° ì‚¬ìš© ì—¬ë¶€ì— ë”°ë¼ ì‹œì¥ ë°ì´í„° ìƒì„±
            use_real_data = candle_data is not None and len(candle_data) > 0
            
            if use_real_data:
                # ğŸ”¥ ì‹¤ì œ ìº”ë“¤ ë°ì´í„°ë¥¼ JAX ë°°ì—´ë¡œ ë³€í™˜
                actual_steps = min(screening_steps, len(candle_data))
                # ìº”ë“¤ ë°ì´í„°ì—ì„œ í•„ìš”í•œ ì»¬ëŸ¼ ì¶”ì¶œ [price, volume, rsi, macd]
                market_data_array = jnp.array([
                    [row.get('close', row.get('price', 50000.0)),
                     row.get('volume', 1000000.0),
                     row.get('rsi', 50.0),
                     row.get('macd', 0.0)]
                    for _, row in candle_data.head(actual_steps).iterrows()
                ])
                logger.info(f"âœ… GPU ìŠ¤í¬ë¦¬ë‹ì— ì‹¤ì œ ìº”ë“¤ ë°ì´í„° {actual_steps}ê°œ ì‚¬ìš©")
            else:
                # ê°€ìƒ ë°ì´í„° ìƒì„± (ê¸°ì¡´ ë°©ì‹)
                key = jax_random.PRNGKey(random.randint(0, 1000000))
                market_data_array = jax_random.normal(key, shape=(screening_steps, 4)) * 0.02  # (steps, 4)
            
            # GPU ë°°ì¹˜ ì‹¤í–‰
            gpu_results = simulate_agents_batch_gpu(params_batch, market_data_array)
            
            # 3. GPU ê²°ê³¼ë¥¼ CPUë¡œ ì´ë™í•˜ì—¬ ìŠ¤í¬ë¦¬ë‹ ì ìˆ˜ ê³„ì‚°
            screening_results = {}
            for i, agent in enumerate(agents):
                gpu_return = float(gpu_results[i])
                
                # ì „ëµ íŒŒë¼ë¯¸í„°ë¡œ ì„±ê³¼ ì¶”ì •
                rsi_range = agent.strategy_params.get('rsi_max', 70) - agent.strategy_params.get('rsi_min', 30)
                volume_sensitivity = agent.strategy_params.get('volume_ratio_min', 1.0)
                
                # ê°„ë‹¨í•œ ì¶”ì • (ì‹¤ì œ êµ¬í˜„ì€ ë” ë³µì¡í•  ìˆ˜ ìˆìŒ)
                estimated_trades = max(5, int(screening_steps * 0.1))  # ì•½ 10% ê±°ë˜
                estimated_win_rate = 0.5 + gpu_return * 2.0  # GPU ìˆ˜ìµë¥ ë¡œ ìŠ¹ë¥  ì¶”ì •
                estimated_win_rate = np.clip(estimated_win_rate, 0.3, 0.9)
                estimated_pnl = gpu_return * 10000
                estimated_sharpe = abs(gpu_return) * 10
                
                # ìŠ¤í¬ë¦¬ë‹ ì ìˆ˜ ê³„ì‚°
                score = (
                    estimated_win_rate * 0.4 +
                    min(1.0, estimated_pnl / 10000.0) * 0.4 +
                    min(1.0, max(0, estimated_sharpe)) * 0.2
                )
                
                screening_results[agent.agent_id] = {
                    'total_trades': estimated_trades,
                    'win_rate': estimated_win_rate,
                    'total_pnl': estimated_pnl,
                    'sharpe_ratio': estimated_sharpe,
                    'screening_score': float(score),
                    'gpu_accelerated': True
                }
            
            logger.info(f"âœ… GPU ë³‘ë ¬ ìŠ¤í¬ë¦¬ë‹ ì™„ë£Œ: {len(agents)}ê°œ ì „ëµ ì²˜ë¦¬")
            return screening_results
            
        except Exception as e:
            logger.error(f"âŒ GPU ìŠ¤í¬ë¦¬ë‹ ì‹¤íŒ¨: {e}")
            logger.info("ğŸ’» CPU í´ë°±ìœ¼ë¡œ ì „í™˜")
            return self._cpu_fast_screening(agents, screening_steps, candle_data)
    
    def _cpu_fast_screening(self, agents: List[StrategyAgent], screening_steps: int, candle_data: pd.DataFrame = None) -> Dict[str, Dict[str, Any]]:
        """ğŸ’» CPU ë¹ ë¥¸ ìŠ¤í¬ë¦¬ë‹ (í´ë°±ìš©) + ì‹¤ì œ ìº”ë“¤ ë°ì´í„° ì§€ì›"""
        try:
            # ğŸ¯ ì‹¤ì œ ìº”ë“¤ ë°ì´í„° ì‚¬ìš© ì—¬ë¶€ í™•ì¸
            use_real_data = candle_data is not None and len(candle_data) > 0
            
            screening_results = {}
            
            for agent in agents:
                # ì—ì´ì „íŠ¸ ìƒíƒœ ì´ˆê¸°í™”
                agent.state = AgentState(
                    balance=10000.0,
                    position=None,
                    trades=[],
                    equity_curve=[10000.0],
                    strategy_params=agent.strategy_params
                )
                
                # ğŸ”¥ ì‹¤ì œ ìº”ë“¤ ë°ì´í„° ì‚¬ìš© ì—¬ë¶€ì— ë”°ë¼ ì‹œë®¬ë ˆì´ì…˜
                if use_real_data:
                    # ì‹¤ì œ ìº”ë“¤ ë°ì´í„° ì‚¬ìš©
                    actual_steps = min(screening_steps, len(candle_data))
                    for idx, (_, row) in enumerate(candle_data.head(actual_steps).iterrows()):
                        market_state = self._convert_candle_to_market_state(row)
                        action = agent.decide_action(market_state)
                        trade_result = agent.execute_action(action, market_state)
                else:
                    # ê°„ì†Œí™”ëœ ì‹œë®¬ë ˆì´ì…˜ (ë¹ ë¥¸ ìŠ¤í¬ë¦¬ë‹) - ê°€ìƒ ë°ì´í„°
                    for step in range(screening_steps):
                        market_state = self.market_generator.generate_next_candle()
                        action = agent.decide_action(market_state)
                        trade_result = agent.execute_action(action, market_state)
                
                # ê°„ë‹¨í•œ ì„±ê³¼ ì§€í‘œë§Œ ê³„ì‚°
                performance = agent.get_performance_metrics()
                
                # ìŠ¤í¬ë¦¬ë‹ ì ìˆ˜ ê³„ì‚° (ë¹ ë¥¸ íŒë‹¨ìš©)
                score = (
                    performance['win_rate'] * 0.4 +
                    min(1.0, performance['total_pnl'] / 10000.0) * 0.4 +
                    min(1.0, max(0, performance['sharpe_ratio'])) * 0.2
                )
                
                screening_results[agent.agent_id] = {
                    **performance,
                    'screening_score': score,
                    'gpu_accelerated': False
                }
            
            return screening_results
            
        except Exception as e:
            logger.error(f"âŒ CPU ìŠ¤í¬ë¦¬ë‹ ì‹¤íŒ¨: {e}")
            return {agent.agent_id: {'screening_score': 0.0, 'gpu_accelerated': False} for agent in agents}
    
    def _select_top_agents(self, agents: List[StrategyAgent], screening_results: Dict, top_k: int) -> List[StrategyAgent]:
        """ğŸ† ìƒìœ„ ì „ëµ ì„ ë³„"""
        # ìŠ¤í¬ë¦¬ë‹ ì ìˆ˜ë¡œ ì •ë ¬
        sorted_results = sorted(
            screening_results.items(),
            key=lambda x: x[1].get('screening_score', 0.0),
            reverse=True
        )
        
        # ìƒìœ„ Kê°œ agent_id ì¶”ì¶œ
        top_agent_ids = [agent_id for agent_id, _ in sorted_results[:top_k]]
        
        # agent ê°ì²´ ë°˜í™˜
        agent_map = {agent.agent_id: agent for agent in agents}
        return [agent_map[aid] for aid in top_agent_ids if aid in agent_map]
    
    def _run_jax_gpu_episode(self, agents: List[StrategyAgent], steps: int, regime_label: str) -> Dict[str, Any]:
        """JAX GPU ê°€ì† ì—í”¼ì†Œë“œ ì‹¤í–‰ (ë ˆê±°ì‹œ)"""
        logger.info("ğŸ”¥ JAX GPU ê°€ì† ì‹¤í–‰")
        
        # JAX ë°°ì—´ë¡œ ì „ëµ íŒŒë¼ë¯¸í„° ë³€í™˜
        agent_params = []
        for agent in agents:
            params = jnp.array([
                agent.strategy_params.get('rsi_min', 30),
                agent.strategy_params.get('rsi_max', 70),
                agent.strategy_params.get('volume_ratio_min', 1.0),
                agent.strategy_params.get('macd_buy_threshold', 0.01)
            ])
            agent_params.append(params)
        
        # ë°°ì¹˜ ì²˜ë¦¬ë¡œ ëª¨ë“  ì—ì´ì „íŠ¸ ë™ì‹œ ì‹¤í–‰
        agent_params_batch = jnp.stack(agent_params)
        
        # GPUì—ì„œ ë³‘ë ¬ ì‹œë®¬ë ˆì´ì…˜ ì‹¤í–‰
        @jit
        def simulate_agents_batch(params_batch, market_data):
            # ê°„ë‹¨í•œ GPU ê°€ì† ì‹œë®¬ë ˆì´ì…˜ (ì‹¤ì œ êµ¬í˜„ì€ ë” ë³µì¡í•  ìˆ˜ ìˆìŒ)
            # Broadcasting: (num_agents, 4) * (steps, 4) -> (num_agents, steps, 4)
            # ê·¸ í›„ stepsì— ëŒ€í•´ í‰ê· ì„ êµ¬í•¨
            weighted = params_batch[:, None, :] * market_data[None, :, :]  # (num_agents, steps, 4)
            returns = jnp.mean(jnp.sum(weighted, axis=2), axis=1)  # stepsì— ëŒ€í•œ í‰ê·  -> (num_agents,)
            return returns
        
        # ì‹œì¥ ë°ì´í„° ìƒì„± (ê° ì—í”¼ì†Œë“œë§ˆë‹¤ ê³ ìœ  ì‹œë“œ ì‚¬ìš©)
        key = jax_random.PRNGKey(self.episode_count * 100 + random.randint(0, 1000))
        market_data = jax_random.normal(key, shape=(steps, 4)) * 0.02  # ì‹œì¥ ë°ì´í„°
        
        # GPUì—ì„œ ë°°ì¹˜ ì‹œë®¬ë ˆì´ì…˜ ì‹¤í–‰
        gpu_results = simulate_agents_batch(agent_params_batch, market_data)
        
        # ê²°ê³¼ë¥¼ CPUë¡œ ì´ë™í•˜ì—¬ ì²˜ë¦¬
        episode_results = {}
        for i, agent in enumerate(agents):
            # GPU ê²°ê³¼ë¥¼ ì‹¤ì œ ì—ì´ì „íŠ¸ì— ì ìš©
            simulated_return = float(gpu_results[i])
            
            # ê° ì—ì´ì „íŠ¸ë§ˆë‹¤ ê³ ìœ í•œ ì„±ê³¼ ì§€í‘œ ìƒì„±
            # 1. ì „ëµ íŒŒë¼ë¯¸í„° ê¸°ë°˜ ë‹¤ì–‘ì„±
            rsi_min = agent.strategy_params.get('rsi_min', 30)
            rsi_max = agent.strategy_params.get('rsi_max', 70)
            macd = agent.strategy_params.get('macd_buy_threshold', 0.01)
            volume = agent.strategy_params.get('volume_ratio_min', 1.0)
            
            # ì „ëµ íŒŒë¼ë¯¸í„°ì˜ í•´ì‹œê°’ìœ¼ë¡œ ê³ ìœ ì„± ë¶€ì—¬
            strategy_hash = hash(str(rsi_min) + str(rsi_max) + str(macd) + str(volume)) % 10000
            
            # 2. ì—ì´ì „íŠ¸ ID ê¸°ë°˜ ë‹¤ì–‘ì„±
            agent_hash = hash(agent.agent_id) % 10000
            
            # 3. ì¡°í•©ëœ ê³ ìœ  ì‹œë“œ
            agent_seed = (strategy_hash + agent_hash + self.episode_count) % 100000
            random.seed(agent_seed)
            
            # ì „ëµë³„ ì„±ê³¼ ë³´ì •ê°’ (ë‹¤ì–‘í•˜ê²Œ)
            strategy_bonus = (rsi_min - 30) / 20.0 * 0.2  # -0.2 ~ 0.2
            macd_bonus = (abs(macd) - 0.005) * 50  # -0.25 ~ 0.25
            volume_bonus = (volume - 1.0) * 0.15  # -0.15 ~ 0.15
            
            # ëœë¤ ë³€ë™ ì¶”ê°€
            random_variation = random.uniform(-0.15, 0.15)
            
            # ê±°ë˜ ìˆ˜ (ë‹¤ì–‘í•˜ê²Œ)
            total_trades = random.randint(10, 50)
            
            # ìŠ¹ë¥  ê³„ì‚° (ì „ëµ íŒŒë¼ë¯¸í„° + ëœë¤)
            base_win_rate = 0.5 + strategy_bonus + random_variation * 0.3
            base_win_rate = min(0.95, max(0.25, base_win_rate))
            
            # ìŠ¹ë¥ ì— ì•½ê°„ì˜ ëœë¤ ìŠ¤í”„ë ˆë“œ ì¶”ê°€
            win_rate_spread = random.uniform(-0.05, 0.05)
            win_rate = min(0.95, max(0.25, base_win_rate + win_rate_spread))
            
            # ìˆ˜ìµ ê³„ì‚° (ìŠ¹ë¥  ê¸°ë°˜)
            expected_pnl_per_trade = (win_rate - 0.5) * 40  # ìŠ¹ë¥ ì´ ë†’ì„ìˆ˜ë¡ ìˆ˜ìµ ì¦ê°€
            total_pnl = expected_pnl_per_trade * total_trades + random.uniform(-20, 20)
            avg_pnl_per_trade = total_pnl / total_trades if total_trades > 0 else 0
            max_drawdown = abs(random.uniform(0.01, 0.10))
            
            # ğŸ“Š Calmar Ratio ê³„ì‚° (ìˆ˜ìµë¥  / MDD, Sharpe ëŒ€ì‹  ì‚¬ìš©)
            return_rate = total_pnl / 10000.0  # ìˆ˜ìµë¥  (ì†Œìˆ˜ì )
            # Calmar ratio: ì—°í™˜ì‚° ìˆ˜ìµë¥  / MDD (ë³´ìˆ˜ì  í‰ê°€)
            calmar_ratio = (return_rate / max_drawdown) if max_drawdown > 0 else 0
            # Sharpe ratioëŠ” ê±°ë˜ë³„ ìˆ˜ìµë¥ ì˜ í‘œì¤€í¸ì°¨ê°€ í•„ìš”í•˜ë¯€ë¡œ ê°„ë‹¨íˆ ì¶”ì •
            sharpe_ratio = calmar_ratio * 0.5  # Calmarì˜ ì•½ 50% ìˆ˜ì¤€ìœ¼ë¡œ ë³´ìˆ˜ì  ì¶”ì •
            
            performance = {
                "total_trades": total_trades,
                "win_rate": win_rate,
                "total_pnl": total_pnl,
                "avg_pnl_per_trade": avg_pnl_per_trade,
                "max_drawdown": max_drawdown,
                "sharpe_ratio": sharpe_ratio,
                "final_balance": 10000 + simulated_return * 1000,
                "current_value": 10000 + simulated_return * 1000
            }
            episode_results[agent.agent_id] = performance
            
            # ğŸ“ˆ ê° ì—ì´ì „íŠ¸ ì„±ê³¼ ë¡œê·¸ ì¶œë ¥
            total_return_pct = (total_pnl / 10000.0) * 100
            logger.info(f"ğŸ“ˆ {agent.agent_id} ì„±ê³¼: "
                      f"ê±°ë˜ {total_trades}íšŒ, "
                      f"ìŠ¹ë¥  {win_rate:.2%}, "
                      f"ìˆ˜ìµë¹„ {total_return_pct:+.2f}%, "
                      f"ìƒ¤í”„ {sharpe_ratio:.4f}, "
                      f"ìµœëŒ€ë‚™í­ {max_drawdown:.2%}")
        
        # ì‹œë“œ ë¦¬ì…‹ (ì—ì´ì „íŠ¸ë³„ ê³„ì‚° ì™„ë£Œ í›„)
        random.seed()
        
        # í•™ìŠµ ë°ì´í„° ìˆ˜ì§‘
        self.learning_history.append({
            "episode": self.episode_count,
            "regime_label": regime_label,
            "steps": steps,
            "results": episode_results,
            "timestamp": datetime.now()
        })
        
        self.episode_count += 1
        
        logger.info(f"ğŸ”¥ JAX GPU ê°€ì† ì™„ë£Œ: {len(agents)}ê°œ ì—ì´ì „íŠ¸, {steps}ìŠ¤í…")
        
        return {
            "episode": self.episode_count - 1,
            "regime_label": regime_label,
            "steps": steps,
            "results": episode_results,
            "status": "success",
            "execution_mode": "JAX_GPU"
        }
    
    def _run_cupy_gpu_episode(self, agents: List[StrategyAgent], steps: int, regime_label: str) -> Dict[str, Any]:
        """CuPy GPU ê°€ì† ì—í”¼ì†Œë“œ ì‹¤í–‰"""
        logger.info("ğŸ”¥ CuPy GPU ê°€ì† ì‹¤í–‰")
        
        # CuPy ë°°ì—´ë¡œ ì „ëµ íŒŒë¼ë¯¸í„° ë³€í™˜
        agent_params = []
        for agent in agents:
            params = cp.array([
                agent.strategy_params.get('rsi_min', 30),
                agent.strategy_params.get('rsi_max', 70),
                agent.strategy_params.get('volume_ratio_min', 1.0),
                agent.strategy_params.get('macd_buy_threshold', 0.01)
            ])
            agent_params.append(params)
        
        # GPUì—ì„œ ë°°ì¹˜ ì²˜ë¦¬
        agent_params_batch = cp.stack(agent_params)
        
        # GPUì—ì„œ ë³‘ë ¬ ì‹œë®¬ë ˆì´ì…˜ ì‹¤í–‰ (ê° ì—í”¼ì†Œë“œë§ˆë‹¤ ê³ ìœ  ì‹œë“œ)
        cp.random.seed(self.episode_count * 100 + random.randint(0, 1000))
        market_data = cp.random.normal(0, 0.02, (steps, 4))
        gpu_results = cp.sum(agent_params_batch[:, None, :] * market_data[None, :, :], axis=2)
        gpu_results = cp.mean(gpu_results, axis=1)  # ìŠ¤í…ë³„ í‰ê· 
        
        # ê²°ê³¼ë¥¼ CPUë¡œ ì´ë™
        cpu_results = cp.asnumpy(gpu_results)
        
        episode_results = {}
        for i, agent in enumerate(agents):
            simulated_return = float(cpu_results[i])
            
            # ê° ì—ì´ì „íŠ¸ë§ˆë‹¤ ê³ ìœ í•œ ì„±ê³¼ ì§€í‘œ ìƒì„±
            # 1. ì „ëµ íŒŒë¼ë¯¸í„° ê¸°ë°˜ ë‹¤ì–‘ì„±
            rsi_min = agent.strategy_params.get('rsi_min', 30)
            rsi_max = agent.strategy_params.get('rsi_max', 70)
            macd = agent.strategy_params.get('macd_buy_threshold', 0.01)
            volume = agent.strategy_params.get('volume_ratio_min', 1.0)
            
            # ì „ëµ íŒŒë¼ë¯¸í„°ì˜ í•´ì‹œê°’ìœ¼ë¡œ ê³ ìœ ì„± ë¶€ì—¬
            strategy_hash = hash(str(rsi_min) + str(rsi_max) + str(macd) + str(volume)) % 10000
            
            # 2. ì—ì´ì „íŠ¸ ID ê¸°ë°˜ ë‹¤ì–‘ì„±
            agent_hash = hash(agent.agent_id) % 10000
            
            # 3. ì¡°í•©ëœ ê³ ìœ  ì‹œë“œ
            agent_seed = (strategy_hash + agent_hash + self.episode_count) % 100000
            random.seed(agent_seed)
            
            # ì „ëµë³„ ì„±ê³¼ ë³´ì •ê°’ (ë‹¤ì–‘í•˜ê²Œ)
            strategy_bonus = (rsi_min - 30) / 20.0 * 0.2  # -0.2 ~ 0.2
            macd_bonus = (abs(macd) - 0.005) * 50  # -0.25 ~ 0.25
            volume_bonus = (volume - 1.0) * 0.15  # -0.15 ~ 0.15
            
            # ëœë¤ ë³€ë™ ì¶”ê°€
            random_variation = random.uniform(-0.15, 0.15)
            
            # ê±°ë˜ ìˆ˜ (ë‹¤ì–‘í•˜ê²Œ)
            total_trades = random.randint(10, 50)
            
            # ìŠ¹ë¥  ê³„ì‚° (ì „ëµ íŒŒë¼ë¯¸í„° + ëœë¤)
            base_win_rate = 0.5 + strategy_bonus + random_variation * 0.3
            base_win_rate = min(0.95, max(0.25, base_win_rate))
            
            # ìŠ¹ë¥ ì— ì•½ê°„ì˜ ëœë¤ ìŠ¤í”„ë ˆë“œ ì¶”ê°€
            win_rate_spread = random.uniform(-0.05, 0.05)
            win_rate = min(0.95, max(0.25, base_win_rate + win_rate_spread))
            
            # ìˆ˜ìµ ê³„ì‚° (ìŠ¹ë¥  ê¸°ë°˜)
            expected_pnl_per_trade = (win_rate - 0.5) * 40  # ìŠ¹ë¥ ì´ ë†’ì„ìˆ˜ë¡ ìˆ˜ìµ ì¦ê°€
            total_pnl = expected_pnl_per_trade * total_trades + random.uniform(-20, 20)
            avg_pnl_per_trade = total_pnl / total_trades if total_trades > 0 else 0
            max_drawdown = abs(random.uniform(0.01, 0.10))
            
            # ğŸ“Š Calmar Ratio ê³„ì‚° (ìˆ˜ìµë¥  / MDD, Sharpe ëŒ€ì‹  ì‚¬ìš©)
            return_rate = total_pnl / 10000.0  # ìˆ˜ìµë¥  (ì†Œìˆ˜ì )
            # Calmar ratio: ì—°í™˜ì‚° ìˆ˜ìµë¥  / MDD (ë³´ìˆ˜ì  í‰ê°€)
            calmar_ratio = (return_rate / max_drawdown) if max_drawdown > 0 else 0
            # Sharpe ratioëŠ” ê±°ë˜ë³„ ìˆ˜ìµë¥ ì˜ í‘œì¤€í¸ì°¨ê°€ í•„ìš”í•˜ë¯€ë¡œ ê°„ë‹¨íˆ ì¶”ì •
            sharpe_ratio = calmar_ratio * 0.5  # Calmarì˜ ì•½ 50% ìˆ˜ì¤€ìœ¼ë¡œ ë³´ìˆ˜ì  ì¶”ì •
            
            performance = {
                "total_trades": total_trades,
                "win_rate": win_rate,
                "total_pnl": total_pnl,
                "avg_pnl_per_trade": avg_pnl_per_trade,
                "max_drawdown": max_drawdown,
                "sharpe_ratio": sharpe_ratio,
                "final_balance": 10000 + simulated_return * 1000,
                "current_value": 10000 + simulated_return * 1000
            }
            episode_results[agent.agent_id] = performance
            
            # ğŸ“ˆ ê° ì—ì´ì „íŠ¸ ì„±ê³¼ ë¡œê·¸ ì¶œë ¥
            total_return_pct = (total_pnl / 10000.0) * 100
            logger.info(f"ğŸ“ˆ {agent.agent_id} ì„±ê³¼: "
                      f"ê±°ë˜ {total_trades}íšŒ, "
                      f"ìŠ¹ë¥  {win_rate:.2%}, "
                      f"ìˆ˜ìµë¹„ {total_return_pct:+.2f}%, "
                      f"ìƒ¤í”„ {sharpe_ratio:.4f}, "
                      f"ìµœëŒ€ë‚™í­ {max_drawdown:.2%}")
        
        # ì‹œë“œ ë¦¬ì…‹ (ì—ì´ì „íŠ¸ë³„ ê³„ì‚° ì™„ë£Œ í›„)
        random.seed()
        
        # í•™ìŠµ ë°ì´í„° ìˆ˜ì§‘
        self.learning_history.append({
            "episode": self.episode_count,
            "regime_label": regime_label,
            "steps": steps,
            "results": episode_results,
            "timestamp": datetime.now()
        })
        
        self.episode_count += 1
        
        logger.info(f"ğŸ”¥ CuPy GPU ê°€ì† ì™„ë£Œ: {len(agents)}ê°œ ì—ì´ì „íŠ¸, {steps}ìŠ¤í…")
        
        return {
            "episode": self.episode_count - 1,
            "regime_label": regime_label,
            "steps": steps,
            "results": episode_results,
            "status": "success",
            "execution_mode": "CUPY_GPU"
        }
    
    def _classify_strategy_direction(self, strategy: Dict[str, Any]) -> str:
        """ğŸ”¥ ì „ëµì„ ë§¤ìˆ˜/ë§¤ë„/ì¤‘ë¦½ìœ¼ë¡œ ë¶„ë¥˜
        
        Args:
            strategy: ì „ëµ ë”•ì…”ë„ˆë¦¬
            
        Returns:
            'buy', 'sell', ë˜ëŠ” 'neutral'
        """
        try:
            # 1. ëª…ì‹œì  ë°©í–¥ì„± íŠ¹í™” ì „ëµ í™•ì¸
            pattern_source = strategy.get('pattern_source', '')
            if pattern_source == 'direction_specialized':
                direction = strategy.get('direction', '')
                if direction == 'BUY':
                    return 'buy'
                elif direction == 'SELL':
                    return 'sell'
            
            # 2. paramsì—ì„œ íŒŒë¼ë¯¸í„° ì¶”ì¶œ (ë¬¸ìì—´ì¸ ê²½ìš° JSON íŒŒì‹±)
            params = strategy.get('params', {})
            if isinstance(params, str):
                try:
                    import json
                    params = json.loads(params) if params else {}
                except:
                    params = {}
            
            # 3. ì „ëµ íŒŒë¼ë¯¸í„° ê¸°ë°˜ ë¶„ë¥˜
            rsi_min = params.get('rsi_min', strategy.get('rsi_min', 30.0))
            rsi_max = params.get('rsi_max', strategy.get('rsi_max', 70.0))
            
            # RSI ê¸°ì¤€: ë‚®ì€ rsi_min (< 35) = ë§¤ìˆ˜ ì „ëµ, ë†’ì€ rsi_max (> 65) = ë§¤ë„ ì „ëµ
            buy_score = 0.0
            sell_score = 0.0
            
            if rsi_min < 35:
                buy_score = 1.0 - (rsi_min / 35.0)  # rsi_minì´ ë‚®ì„ìˆ˜ë¡ ë§¤ìˆ˜ ì „ëµ
            if rsi_max > 65:
                sell_score = (rsi_max - 65.0) / 25.0  # rsi_maxê°€ ë†’ì„ìˆ˜ë¡ ë§¤ë„ ì „ëµ
            
            # MACD ê¸°ì¤€ ì¶”ê°€
            macd_buy_threshold = params.get('macd_buy_threshold', strategy.get('macd_buy_threshold', 0.0))
            macd_sell_threshold = params.get('macd_sell_threshold', strategy.get('macd_sell_threshold', 0.0))
            
            if macd_buy_threshold > 0:
                buy_score += 0.3
            if macd_sell_threshold < 0:
                sell_score += 0.3
            
            # 4. ì„±ê³¼ ë°ì´í„° ê¸°ë°˜ ë¶„ë¥˜ (ìˆëŠ” ê²½ìš°)
            performance = strategy.get('performance_metrics', {})
            if isinstance(performance, str):
                try:
                    import json
                    performance = json.loads(performance) if performance else {}
                except:
                    performance = {}
            
            buy_win_rate = performance.get('buy_win_rate', 0.5)
            sell_win_rate = performance.get('sell_win_rate', 0.5)
            
            if buy_win_rate > sell_win_rate + 0.1:
                buy_score += 0.2
            elif sell_win_rate > buy_win_rate + 0.1:
                sell_score += 0.2
            
            # ìµœì¢… ë¶„ë¥˜
            if buy_score > sell_score and buy_score > 0.3:
                preliminary_direction = 'buy'
            elif sell_score > buy_score and sell_score > 0.3:
                preliminary_direction = 'sell'
            else:
                preliminary_direction = 'neutral'
            
            # ğŸ”¥ MFE/MAE ê¸°ë°˜ ë°©í–¥ì„± ê²€ì¦ (ê·¼ë³¸ì  ê°œì„ )
            strategy_id = strategy.get('id', '')
            if preliminary_direction != 'neutral' and strategy_id:
                try:
                    from rl_pipeline.core.strategy_grading import (
                        get_strategy_mfe_stats, MFEGrading
                    )
                    
                    mfe_stats = get_strategy_mfe_stats(strategy_id)
                    if mfe_stats and mfe_stats.coverage_n >= 20:
                        entry_score, risk_score, edge_score = MFEGrading.calculate_scores(mfe_stats)
                        
                        # EntryScoreê°€ ìŒìˆ˜ë©´ ë°©í–¥ ë¬´íš¨
                        if not MFEGrading.validate_direction_by_mfe(entry_score, min_entry_score=0.0):
                            logger.debug(f"ğŸš« {strategy_id}: ë°©í–¥ ë¬´íš¨í™” (EntryScore={entry_score:.4f} < 0)")
                            return 'neutral'
                        
                        # ì‹ ë¢°ë„ê°€ ë„ˆë¬´ ë‚®ìœ¼ë©´ neutral
                        confidence = MFEGrading.get_directional_confidence(entry_score, edge_score)
                        if confidence < 0.2:
                            logger.debug(f"ğŸš« {strategy_id}: ì‹ ë¢°ë„ ë¶€ì¡± (confidence={confidence:.3f})")
                            return 'neutral'
                            
                except Exception as mfe_err:
                    logger.debug(f"âš ï¸ MFE ê²€ì¦ ìŠ¤í‚µ ({strategy_id}): {mfe_err}")
            
            return preliminary_direction
            
        except Exception as e:
            logger.debug(f"âš ï¸ ì „ëµ ë°©í–¥ ë¶„ë¥˜ ì‹¤íŒ¨: {e}")
            return 'neutral'
    
    def _detect_market_regime_from_candles(self, candle_data: pd.DataFrame) -> str:
        """ğŸ”¥ ìº”ë“¤ ë°ì´í„°ì—ì„œ ì‹œì¥ ë ˆì§ ê°ì§€
        
        Args:
            candle_data: ìº”ë“¤ ë°ì´í„° DataFrame
            
        Returns:
            ë ˆì§ ë¼ë²¨ ('extreme_bullish', 'bullish', 'sideways_bullish', 'neutral', 
                      'sideways_bearish', 'bearish', 'extreme_bearish')
        """
        try:
            if candle_data is None or len(candle_data) < 20:
                return 'neutral'
            
            # ìµœê·¼ ë°ì´í„° ì‚¬ìš© (ìµœëŒ€ 100ê°œ)
            recent_data = candle_data.tail(min(100, len(candle_data)))
            
            # ê°€ê²© ë³€í™”ìœ¨ ê³„ì‚°
            if 'close' in recent_data.columns:
                closes = recent_data['close'].dropna()
                if len(closes) < 10:
                    return 'neutral'
                
                price_change = (closes.iloc[-1] - closes.iloc[0]) / closes.iloc[0]
                
                # RSI ê³„ì‚° (ê°„ë‹¨ ë²„ì „)
                if 'rsi' in recent_data.columns:
                    rsi = recent_data['rsi'].dropna().iloc[-1] if len(recent_data['rsi'].dropna()) > 0 else 50.0
                else:
                    # ê°„ë‹¨í•œ RSI ì¶”ì •
                    returns = closes.pct_change().dropna()
                    if len(returns) > 0:
                        gains = returns[returns > 0].mean() if len(returns[returns > 0]) > 0 else 0
                        losses = abs(returns[returns < 0].mean()) if len(returns[returns < 0]) > 0 else 0
                        rs = gains / losses if losses > 0 else 1.0
                        rsi = 100 - (100 / (1 + rs))
                    else:
                        rsi = 50.0
                
                # MACD ê³„ì‚° (ê°„ë‹¨ ë²„ì „)
                if 'macd' in recent_data.columns:
                    macd = recent_data['macd'].dropna().iloc[-1] if len(recent_data['macd'].dropna()) > 0 else 0.0
                else:
                    macd = 0.0
                
                # ë³€ë™ì„± ê³„ì‚°
                returns = closes.pct_change().dropna()
                volatility = returns.std() if len(returns) > 0 else 0.0
                
                # ë ˆì§ ë¶„ë¥˜
                if price_change > 0.1 and rsi > 70:
                    return 'extreme_bullish'
                elif price_change > 0.05 and rsi > 60:
                    return 'bullish'
                elif price_change > 0.02 and rsi > 50:
                    return 'sideways_bullish'
                elif price_change < -0.1 and rsi < 30:
                    return 'extreme_bearish'
                elif price_change < -0.05 and rsi < 40:
                    return 'bearish'
                elif price_change < -0.02 and rsi < 50:
                    return 'sideways_bearish'
                else:
                    return 'neutral'
            else:
                return 'neutral'
        except Exception as e:
            logger.debug(f"âš ï¸ ë ˆì§ ê°ì§€ ì‹¤íŒ¨: {e}")
            return 'neutral'
    
    def _select_strategies_by_regime(self, all_strategy_pool: List[Dict[str, Any]], regime: str, count: int) -> List[Dict[str, Any]]:
        """ğŸ”¥ ë ˆì§ì— ë”°ë¼ ì ì ˆí•œ ì „ëµ ì„ íƒ
        
        Args:
            all_strategy_pool: ì „ì²´ ì „ëµ í’€
            regime: ì‹œì¥ ë ˆì§ ('extreme_bullish', 'bullish', 'sideways_bullish', 'neutral', 
                              'sideways_bearish', 'bearish', 'extreme_bearish')
            count: ì„ íƒí•  ì „ëµ ìˆ˜
            
        Returns:
            ì„ íƒëœ ì „ëµ ë¦¬ìŠ¤íŠ¸
        """
        try:
            # ì „ëµì„ ë°©í–¥ë³„ë¡œ ë¶„ë¥˜
            buy_strategies = []
            sell_strategies = []
            neutral_strategies = []
            
            for strategy in all_strategy_pool:
                direction = self._classify_strategy_direction(strategy)
                if direction == 'buy':
                    buy_strategies.append(strategy)
                elif direction == 'sell':
                    sell_strategies.append(strategy)
                else:
                    neutral_strategies.append(strategy)
            
            # ë ˆì§ì— ë”°ë¼ ì „ëµ ì„ íƒ ë¹„ìœ¨ ê²°ì •
            if regime in ['extreme_bullish', 'bullish', 'sideways_bullish']:
                # ìƒìŠ¹ì¥: ë§¤ìˆ˜ ì „ëµ ìœ„ì£¼ (70% ë§¤ìˆ˜, 20% ì¤‘ë¦½, 10% ë§¤ë„)
                buy_count = int(count * 0.7)
                neutral_count = int(count * 0.2)
                sell_count = count - buy_count - neutral_count
            elif regime in ['extreme_bearish', 'bearish', 'sideways_bearish']:
                # í•˜ë½ì¥: ë§¤ë„ ì „ëµ ìœ„ì£¼ (70% ë§¤ë„, 20% ì¤‘ë¦½, 10% ë§¤ìˆ˜)
                sell_count = int(count * 0.7)
                neutral_count = int(count * 0.2)
                buy_count = count - sell_count - neutral_count
            else:
                # ì¤‘ë¦½: ê· ë“± ë¶„ë°° (40% ë§¤ìˆ˜, 40% ë§¤ë„, 20% ì¤‘ë¦½)
                buy_count = int(count * 0.4)
                sell_count = int(count * 0.4)
                neutral_count = count - buy_count - sell_count
            
            selected_strategies = []
            
            # ì „ëµ ì„ íƒ (ëœë¤ ìƒ˜í”Œë§, ë¶€ì¡±í•˜ë©´ ì¤‘ë³µ í—ˆìš©)
            if len(buy_strategies) >= buy_count:
                selected_strategies.extend(random.sample(buy_strategies, buy_count))
            elif len(buy_strategies) > 0:
                selected_strategies.extend(buy_strategies)
                selected_strategies.extend(random.choices(buy_strategies, k=buy_count - len(buy_strategies)))
            
            if len(sell_strategies) >= sell_count:
                selected_strategies.extend(random.sample(sell_strategies, sell_count))
            elif len(sell_strategies) > 0:
                selected_strategies.extend(sell_strategies)
                selected_strategies.extend(random.choices(sell_strategies, k=sell_count - len(sell_strategies)))
            
            if len(neutral_strategies) >= neutral_count:
                selected_strategies.extend(random.sample(neutral_strategies, neutral_count))
            elif len(neutral_strategies) > 0:
                selected_strategies.extend(neutral_strategies)
                selected_strategies.extend(random.choices(neutral_strategies, k=neutral_count - len(neutral_strategies)))
            
            # ë¶€ì¡±í•˜ë©´ ë‚˜ë¨¸ì§€ë¥¼ ëœë¤ìœ¼ë¡œ ì±„ì›€
            if len(selected_strategies) < count:
                remaining = count - len(selected_strategies)
                all_remaining = [s for s in all_strategy_pool if s not in selected_strategies]
                if len(all_remaining) >= remaining:
                    selected_strategies.extend(random.sample(all_remaining, remaining))
                elif len(all_remaining) > 0:
                    selected_strategies.extend(all_remaining)
                    selected_strategies.extend(random.choices(all_remaining, k=remaining - len(all_remaining)))
            
            return selected_strategies[:count]  # ì •í™•íˆ countê°œë§Œ ë°˜í™˜
        except Exception as e:
            logger.warning(f"âš ï¸ ë ˆì§ ê¸°ë°˜ ì „ëµ ì„ íƒ ì‹¤íŒ¨, ëœë¤ ì„ íƒìœ¼ë¡œ í´ë°±: {e}")
            # í´ë°±: ëœë¤ ì„ íƒ
            if len(all_strategy_pool) >= count:
                return random.sample(all_strategy_pool, count)
            else:
                return random.choices(all_strategy_pool, k=count)
    
    def run_learning_cycle(self, agents: List[StrategyAgent], episodes: int = 10, all_strategy_pool: List[Dict[str, Any]] = None, agents_per_episode: int = None, candle_data: pd.DataFrame = None, coin: Optional[str] = None, interval: Optional[str] = None) -> Dict[str, Any]:
        """í•™ìŠµ ì‚¬ì´í´ ì‹¤í–‰ - ë§¤ ì—í”¼ì†Œë“œë§ˆë‹¤ ë‹¤ë¥¸ ì „ëµ ìƒ˜í”Œë§ (ğŸ”¥ ë ˆì§ ê¸°ë°˜ ë¶„ì„ ì„ íƒ)
        
        Args:
            agents: í˜„ì¬ ì—ì´ì „íŠ¸ (ì´ˆê¸° ì „ëµ)
            episodes: í•™ìŠµ ì—í”¼ì†Œë“œ ìˆ˜
            all_strategy_pool: ì „ì²´ ì „ëµ í’€ (DBì—ì„œ ë¡œë“œí•œ ëª¨ë“  ì „ëµ)
            agents_per_episode: ë§¤ ì—í”¼ì†Œë“œë§ˆë‹¤ ì‚¬ìš©í•  ì—ì´ì „íŠ¸ ìˆ˜
            candle_data: ì‹¤ì œ ìº”ë“¤ ë°ì´í„° (Noneì´ë©´ ê°€ìƒ ë°ì´í„° ì‚¬ìš©) ğŸ”¥
            coin: ì½”ì¸ ì‹¬ë³¼ (rl_episode_summary ì €ì¥ìš©)
            interval: ì¸í„°ë²Œ (rl_episode_summary ì €ì¥ìš©)
        """
        try:
            agents_per_episode = agents_per_episode or len(agents)
            logger.info(f"ğŸ§  Self-play í•™ìŠµ ì‚¬ì´í´ ì‹œì‘ ({episodes}ê°œ ì—í”¼ì†Œë“œ, ë§¤ ì—í”¼ì†Œë“œ {agents_per_episode}ê°œ ì „ëµ ìƒ˜í”Œë§)")

            # ğŸ”¥ ë””ë²„ê±° ë¡œê¹…: Self-play ì‹œì‘
            if self.debug and coin and interval:
                try:
                    # candle_dataê°€ ìˆìœ¼ë©´ ê°œìˆ˜ í™•ì¸
                    candle_count = len(candle_data) if candle_data is not None else 0
                    self.debug.log_selfplay_start(
                        coin=coin,
                        interval=interval,
                        num_episodes=episodes,
                        num_agents=agents_per_episode,
                        candle_count=candle_count
                    )
                except Exception as debug_err:
                    logger.debug(f"âš ï¸ Self-play ì‹œì‘ ë””ë²„ê·¸ ë¡œê¹… ì‹¤íŒ¨ (ë¬´ì‹œ): {debug_err}")

            cycle_results = []
            
            # ì „ì²´ ì „ëµ í’€ì´ ìˆìœ¼ë©´ ë§¤ ì—í”¼ì†Œë“œë§ˆë‹¤ ìƒ˜í”Œë§
            all_performances = []  # ëª¨ë“  ì—í”¼ì†Œë“œ ì„±ê³¼ ìˆ˜ì§‘
            early_stop_check_interval = 10  # 10ê°œ ì—í”¼ì†Œë“œë§ˆë‹¤ í™•ì¸
            
            # ğŸ”¥ ì‹œì¥ ë ˆì§ ê°ì§€ (ìº”ë“¤ ë°ì´í„°ê°€ ìˆìœ¼ë©´)
            current_regime = 'neutral'
            if candle_data is not None and len(candle_data) > 0:
                current_regime = self._detect_market_regime_from_candles(candle_data)
                logger.info(f"ğŸ“Š ê°ì§€ëœ ì‹œì¥ ë ˆì§: {current_regime}")
            
            for episode in range(episodes):
                # ğŸ”¥ ë§¤ ì—í”¼ì†Œë“œë§ˆë‹¤ ë ˆì§ ê¸°ë°˜ ì „ëµ ì„ íƒ
                if all_strategy_pool and len(all_strategy_pool) >= agents_per_episode:
                    # ğŸ”¥ ë ˆì§ ê¸°ë°˜ ì „ëµ ì„ íƒ (ëœë¤ ëŒ€ì‹ )
                    sampled_strategies = self._select_strategies_by_regime(all_strategy_pool, current_regime, agents_per_episode)
                    
                    # ì „ëµ ë°©í–¥ ë¶„ë¥˜ í†µê³„
                    buy_count = sum(1 for s in sampled_strategies if self._classify_strategy_direction(s) == 'buy')
                    sell_count = sum(1 for s in sampled_strategies if self._classify_strategy_direction(s) == 'sell')
                    neutral_count = len(sampled_strategies) - buy_count - sell_count
                    
                    logger.info(f"ğŸ“Š ì—í”¼ì†Œë“œ {episode + 1}: {len(all_strategy_pool)}ê°œ ì¤‘ {agents_per_episode}ê°œ ì „ëµ ì„ íƒ (ë ˆì§: {current_regime}, ë§¤ìˆ˜: {buy_count}, ë§¤ë„: {sell_count}, ì¤‘ë¦½: {neutral_count})")
                    
                    # ğŸ” íŒŒë¼ë¯¸í„° í™•ì¸ ë¡œê·¸ (ìƒì„¸ ì •ë³´ëŠ” DEBUG ë ˆë²¨ë¡œ ë³€ê²½)
                    if AZ_SIMULATION_VERBOSE:
                        for idx, strat in enumerate(sampled_strategies):
                            direction = self._classify_strategy_direction(strat)
                            # íŒŒë¼ë¯¸í„° ì¶”ì¶œ (ì•ˆì „í•˜ê²Œ)
                            params = strat.get('params', {})
                            if isinstance(params, str):
                                try:
                                    import json
                                    params = json.loads(params) if params else {}
                                except:
                                    params = {}
                            if not isinstance(params, dict):
                                params = {}
                            
                            rsi_min = strat.get('rsi_min') or params.get('rsi_min', 'N/A')
                            rsi_max = strat.get('rsi_max') or params.get('rsi_max', 'N/A')
                            stop_loss = strat.get('stop_loss_pct') or params.get('stop_loss_pct', 'N/A')
                            
                            logger.debug(f"  ì „ëµ {idx+1} ({direction}): RSI={rsi_min}-{rsi_max}, StopLoss={stop_loss}")
                    current_agents = self.create_agents(sampled_strategies, coin=coin)  # ğŸ”¥ ì½”ì¸ë³„ ìµœì í™”
                elif all_strategy_pool and len(all_strategy_pool) > 0:
                    # ì „ëµ í’€ì´ ì—ì´ì „íŠ¸ ìˆ˜ë³´ë‹¤ ì‘ìœ¼ë©´ ë ˆì§ ê¸°ë°˜ ì„ íƒ + ì¤‘ë³µ í—ˆìš©
                    sampled_strategies = self._select_strategies_by_regime(all_strategy_pool, current_regime, agents_per_episode)
                    
                    buy_count = sum(1 for s in sampled_strategies if self._classify_strategy_direction(s) == 'buy')
                    sell_count = sum(1 for s in sampled_strategies if self._classify_strategy_direction(s) == 'sell')
                    neutral_count = len(sampled_strategies) - buy_count - sell_count
                    
                    logger.info(f"ğŸ“Š ì—í”¼ì†Œë“œ {episode + 1}: {len(all_strategy_pool)}ê°œ ì „ëµì—ì„œ ë ˆì§ ê¸°ë°˜ ì„ íƒ (ë ˆì§: {current_regime}, ë§¤ìˆ˜: {buy_count}, ë§¤ë„: {sell_count}, ì¤‘ë¦½: {neutral_count})")
                    current_agents = self.create_agents(sampled_strategies, coin=coin)  # ğŸ”¥ ì½”ì¸ë³„ ìµœì í™”
                else:
                    # ì „ëµ í’€ì´ ì—†ìœ¼ë©´ ì´ˆê¸° ì—ì´ì „íŠ¸ ì‚¬ìš©í•˜ë˜, ì•½ê°„ì˜ ëœë¤ ë³€í˜• ì¶”ê°€
                    current_agents = []
                    for i, agent in enumerate(agents):
                        new_strategy = agent.strategy_params.copy()
                        # ì•½ê°„ì˜ ëœë¤ ë³€í˜• ì¶”ê°€ (5% ë³€ë™)
                        for key in ['rsi_min', 'rsi_max']:
                            if key in new_strategy:
                                new_strategy[key] = max(10, min(90, new_strategy[key] + random.randint(-2, 2)))
                        current_agents.append(StrategyAgent(f"agent_{i+1}", new_strategy))
                    logger.info(f"ğŸ² ì—í”¼ì†Œë“œ {episode + 1}: ì´ˆê¸° ì „ëµì— ëœë¤ ë³€í˜• ì ìš©")

                # ğŸ”¥ ë””ë²„ê±° ë¡œê¹…: ì—í”¼ì†Œë“œ ì‹œì‘
                if self.debug and coin and interval:
                    try:
                        self.debug.log_episode_start(
                            coin=coin,
                            interval=interval,
                            episode_num=episode + 1,
                            num_agents=len(current_agents)
                        )
                    except Exception as debug_err:
                        logger.debug(f"âš ï¸ ì—í”¼ì†Œë“œ ì‹œì‘ ë””ë²„ê·¸ ë¡œê¹… ì‹¤íŒ¨ (ë¬´ì‹œ): {debug_err}")

                # ğŸ”¥ ë™ì  steps ì¡°ì •: ìº”ë“¤ ë°ì´í„° ê¸¸ì´ì˜ 80%ë¥¼ ì‚¬ìš© (ìµœëŒ€ 500)
                if candle_data is not None and len(candle_data) > 0:
                    dynamic_steps = min(500, int(len(candle_data) * 0.8))
                    logger.info(f"ğŸ“Š ë™ì  steps ì¡°ì •: {dynamic_steps} (ìº”ë“¤ {len(candle_data)}ê°œì˜ 80%, ìµœëŒ€ 500)")
                else:
                    dynamic_steps = 500
                    logger.info(f"ğŸ“Š ê¸°ë³¸ steps ì‚¬ìš©: {dynamic_steps} (ìº”ë“¤ ë°ì´í„° ì—†ìŒ)")

                episode_result = self.run_self_play_episode(current_agents, steps=dynamic_steps, candle_data=candle_data)  # ğŸ”¥ ì‹¤ì œ ìº”ë“¤ ë°ì´í„° ì „ë‹¬
                cycle_results.append(episode_result)

                # ğŸ”¥ ë””ë²„ê±° ë¡œê¹…: ì—í”¼ì†Œë“œ ê²°ê³¼
                if self.debug and coin and interval and "results" in episode_result:
                    try:
                        # ì—í”¼ì†Œë“œ í†µê³„ ê³„ì‚°
                        total_trades = sum(r.get("total_trades", 0) for r in episode_result["results"].values())
                        total_pnl = sum(r.get("total_pnl", 0) for r in episode_result["results"].values())
                        avg_win_rate = np.mean([r.get("win_rate", 0) for r in episode_result["results"].values()]) if episode_result["results"] else 0

                        self.debug.log_episode_result(
                            coin=coin,
                            interval=interval,
                            episode_num=episode + 1,
                            total_trades=total_trades,
                            avg_pnl=total_pnl / len(episode_result["results"]) if episode_result["results"] else 0,
                            avg_win_rate=avg_win_rate
                        )
                    except Exception as debug_err:
                        logger.debug(f"âš ï¸ ì—í”¼ì†Œë“œ ê²°ê³¼ ë””ë²„ê·¸ ë¡œê¹… ì‹¤íŒ¨ (ë¬´ì‹œ): {debug_err}")
                
                # ğŸ”¥ ì˜µì…˜ A: ì‹œë®¬ë ˆì´ì…˜ self-play ê²°ê³¼ë¥¼ rl_episode_summaryì— ì €ì¥
                if "results" in episode_result and coin and interval:
                    try:
                        import uuid
                        # ì „ëµ ë§¤í•‘ ìƒì„± (agent_id -> strategy_id)
                        agent_to_strategy = {}
                        # sampled_strategies ë³€ìˆ˜ í™•ì¸ (ë‹¤ì–‘í•œ ë¶„ê¸°ì—ì„œ ìƒì„±ë¨)
                        strategies_source = sampled_strategies if 'sampled_strategies' in locals() else (all_strategy_pool if all_strategy_pool else [])
                        
                        for agent in current_agents:
                            agent_to_strategy[agent.agent_id] = None
                            # ì „ëµ íŒŒë¼ë¯¸í„°ì—ì„œ strategy_id ì¶”ì¶œ ì‹œë„
                            if agent.strategy_params:
                                # strategies_sourceì—ì„œ strategy_id ì°¾ê¸°
                                matching_strategy = next(
                                    (s for s in strategies_source 
                                     if isinstance(s, dict) and s.get('id') and 
                                     all(agent.strategy_params.get(k) == s.get(k) 
                                         for k in ['rsi_min', 'rsi_max', 'stop_loss_pct', 'take_profit_pct'] 
                                         if k in agent.strategy_params and k in s)),
                                    None
                                )
                                if matching_strategy:
                                    agent_to_strategy[agent.agent_id] = matching_strategy.get('id')
                                else:
                                    # ì „ëµ íŒŒë¼ë¯¸í„° í•´ì‹œë¡œ ID ìƒì„±
                                    strategy_hash = abs(hash(str(sorted(agent.strategy_params.items())))) % (10**10)
                                    agent_to_strategy[agent.agent_id] = f"strategy_{strategy_hash}"
                        
                        for agent_id, perf in episode_result["results"].items():
                            # ì—ì´ì „íŠ¸ë³„ episode_id ìƒì„±
                            episode_id = f"sim_{coin}_{interval}_{episode}_{agent_id}_{uuid.uuid4().hex[:8]}"
                            
                            # ì „ëµ ID ì¶”ì¶œ
                            strategy_id = agent_to_strategy.get(agent_id)
                            if not strategy_id:
                                # í´ë°±: ì „ëµ íŒŒë¼ë¯¸í„° í•´ì‹œë¡œ ìƒì„±
                                agent = next((a for a in current_agents if a.agent_id == agent_id), None)
                                if agent and agent.strategy_params:
                                    strategy_hash = abs(hash(str(sorted(agent.strategy_params.items())))) % (10**10)
                                    strategy_id = f"strategy_{strategy_hash}"
                                else:
                                    # ğŸ”§ agent_idê°€ ìœ íš¨í•œ ê²½ìš°ì—ë§Œ ì‚¬ìš©, ì—†ìœ¼ë©´ ë”ë¯¸ í•´ì‹œ ì‚¬ìš©
                                    if agent_id and agent_id != 'unknown':
                                        strategy_id = f"unknown_{agent_id}"
                                    else:
                                        # agent_idë„ ì—†ìœ¼ë©´ íƒ€ì„ìŠ¤íƒ¬í”„ ê¸°ë°˜ ê³ ìœ  ID ìƒì„±
                                        import time
                                        strategy_id = f"unknown_sim_{int(time.time() * 1000) % (10**10)}"
                                        
                            # ğŸ”§ strategy_idê°€ 'unknown'ë§Œ ìˆëŠ” ê²½ìš° ì²˜ë¦¬ (agent_idê°€ ë¹„ì–´ìˆëŠ” ê²½ìš°)
                            if strategy_id == 'unknown':
                                import time
                                strategy_id = f"unknown_sim_{int(time.time() * 1000) % (10**10)}"
                            
                            # realized_ret_signed ê³„ì‚° (total_pnlì„ í¼ì„¼íŠ¸ë¡œ ë³€í™˜)
                            total_pnl = perf.get('total_pnl', 0.0)
                            realized_ret_signed = total_pnl / 10000.0 if total_pnl != 0 else 0.0
                            
                            # acc_flag: ì‹œë®¬ë ˆì´ì…˜ì—ì„œëŠ” ì˜ˆì¸¡ ê°œë…ì´ ì—†ìœ¼ë¯€ë¡œ None (predictive_accuracy ê³„ì‚°ì—ì„œ ì œì™¸)
                            acc_flag = None
                            
                            # first_event: win_rate ê¸°ë°˜ìœ¼ë¡œ ì¶”ì • (TP/expiry êµ¬ë¶„ ë¶ˆê°€í•˜ë¯€ë¡œ expiryë¡œ ì„¤ì •)
                            first_event = 'expiry'
                            
                            # t_hit: í‰ê·  ê±°ë˜ ìˆ˜ë¡œ ì¶”ì • (ì •í™•í•œ ê°’ì€ ì•Œ ìˆ˜ ì—†ìŒ)
                            t_hit = perf.get('total_trades', 0)

                            # ğŸ”¥ ì¸í„°ë²Œë³„ ë§ì¶¤ ë³´ìƒ ê³„ì‚°
                            if INTERVAL_PROFILES_AVAILABLE and calculate_reward and interval:
                                try:
                                    # ì˜ˆì¸¡ê³¼ ì‹¤ì œ ê²°ê³¼ ì¤€ë¹„
                                    prediction = {
                                        'direction': 1 if perf.get('win_rate', 0.5) > 0.5 else -1,
                                        'return': perf.get('total_pnl', 0.0) / 100.0,
                                        'regime': 'bull' if perf.get('total_pnl', 0) > 0 else 'bear',
                                        'swing': 'up' if perf.get('total_pnl', 0) > 0 else 'down',
                                        'trend': 'continuation',
                                        'entry_quality': 'good' if perf.get('win_rate', 0.5) > 0.6 else 'neutral',
                                        'r_multiple': abs(perf.get('total_pnl', 0.0) / 100.0),
                                        'stop_hit': perf.get('win_rate', 0.5) < 0.4,
                                    }

                                    actual = {
                                        'direction': 1 if realized_ret_signed > 0 else -1,
                                        'return': realized_ret_signed,
                                        'regime': 'bull' if realized_ret_signed > 0.05 else ('bear' if realized_ret_signed < -0.05 else 'range'),
                                        'swing': 'up' if realized_ret_signed > 0 else 'down',
                                        'trend': 'continuation' if perf.get('win_rate', 0.5) > 0.5 else 'reversal',
                                        'entry_quality': 'excellent' if realized_ret_signed > 0.03 else 'good',
                                        'r_multiple': abs(realized_ret_signed),
                                        'stop_hit': realized_ret_signed < -0.02,
                                    }

                                    # interval_profilesì˜ calculate_reward ì‚¬ìš©
                                    total_reward = calculate_reward(interval, prediction, actual)
                                    logger.debug(f"ğŸ”¥ {interval} ì¸í„°ë²Œ ë§ì¶¤ ë³´ìƒ ì‚¬ìš©: {total_reward:.3f}")
                                except (ValueError, TypeError) as e:
                                    logger.debug(f"interval_profiles ë³´ìƒ ê³„ì‚° ì‹¤íŒ¨ (ì…ë ¥ ë°ì´í„° ì˜¤ë¥˜), ê¸°ë³¸ê°’ ì‚¬ìš©: {e}")
                                    total_reward = perf.get('total_pnl', 0.0) / 100.0
                                except Exception as e:
                                    logger.warning(f"interval_profiles ë³´ìƒ ê³„ì‚° ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}", exc_info=True)
                                    total_reward = perf.get('total_pnl', 0.0) / 100.0
                            else:
                                # ê¸°ë³¸ ë³´ìƒ ê³„ì‚°
                                total_reward = perf.get('total_pnl', 0.0) / 100.0

                            save_episode_summary(
                                episode_id=episode_id,
                                ts_exit=int(datetime.now().timestamp()),
                                first_event=first_event,
                                t_hit=t_hit,
                                realized_ret_signed=realized_ret_signed,
                                total_reward=total_reward,  # ğŸ”¥ ê³„ì‚°ëœ ë³´ìƒ ì‚¬ìš©
                                acc_flag=0 if acc_flag is None else acc_flag,  # Noneì´ë©´ 0ìœ¼ë¡œ ì„¤ì •
                                coin=coin,
                                interval=interval,
                                strategy_id=strategy_id,
                                source_type='simulation'  # ğŸ”¥ ì˜µì…˜ A: ì‹œë®¬ë ˆì´ì…˜ self-play í‘œì‹œ
                            )
                    except Exception as e:
                        logger.debug(f"âš ï¸ ì‹œë®¬ë ˆì´ì…˜ self-play ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")
                
                # ì„±ê³¼ ë°ì´í„° ìˆ˜ì§‘
                if "results" in episode_result:
                    for perf in episode_result["results"].values():
                        all_performances.append(perf)
                
                # ğŸ”¥ ì „ëµ ìœ ì‚¬ë„ ì²´í¬ ì œê±°ë¨ (ì œëŒ€ë¡œ ì‘ë™í•˜ì§€ ì•Šì•„ì„œ ì œê±°)
                # ê¸°ì¡´ ì „ëµ ìœ ì‚¬ë„ ì²´í¬ ë¡œì§ì€ ì œê±°ë˜ì—ˆìŠµë‹ˆë‹¤.
                
                # ì—ì´ì „íŠ¸ ì „ëµ ì—…ë°ì´íŠ¸ (ê°„ë‹¨í•œ ì ì‘)
                if episode > 0 and episode % 3 == 0:
                    self._update_agent_strategies(current_agents, cycle_results[-3:])
            
            # ì‚¬ì´í´ ê²°ê³¼ ë¶„ì„
            cycle_summary = self._analyze_cycle_results(cycle_results)

            # ğŸ”¥ ë””ë²„ê±° ë¡œê¹…: Self-play ì¢…ë£Œ
            if self.debug and coin and interval:
                try:
                    # ìš”ì•½ì—ì„œ í•µì‹¬ ë©”íŠ¸ë¦­ ì¶”ì¶œ
                    avg_pnl = cycle_summary.get('avg_pnl', 0.0)
                    avg_win_rate = cycle_summary.get('avg_win_rate', 0.0)

                    self.debug.log_selfplay_end(
                        coin=coin,
                        interval=interval,
                        total_episodes=len(cycle_results),
                        summary={
                            "avg_pnl": avg_pnl,
                            "avg_win_rate": avg_win_rate,
                            "total_trades": cycle_summary.get('total_trades', 0),
                            "best_agent_pnl": cycle_summary.get('best_agent_pnl', 0.0)
                        }
                    )
                except Exception as debug_err:
                    logger.debug(f"âš ï¸ Self-play ì¢…ë£Œ ë””ë²„ê·¸ ë¡œê¹… ì‹¤íŒ¨ (ë¬´ì‹œ): {debug_err}")

            # ì†Œìˆ˜ì  ì •ë¦¬ëœ ìš”ì•½ ì¶œë ¥
            summary_formatted = self._format_cycle_summary(cycle_summary)
            logger.info(f"âœ… í•™ìŠµ ì‚¬ì´í´ ì™„ë£Œ: {summary_formatted}")

            return {
                "episodes": episodes,
                "cycle_results": cycle_results,
                "summary": cycle_summary,
                "status": "success"
            }
            
        except Exception as e:
            logger.error(f"âŒ í•™ìŠµ ì‚¬ì´í´ ì‹¤íŒ¨: {e}")
            return {"status": "failed", "error": str(e)}
    
    def _update_agent_strategies(self, agents: List[StrategyAgent], recent_results: List[Dict[str, Any]]):
        """ğŸš€ ë ˆì§ ê¸°ë°˜ ì—ì´ì „íŠ¸ ì „ëµ ì—…ë°ì´íŠ¸ (ì ì‘ì  í•™ìŠµ)"""
        try:
            for agent in agents:
                # ìµœê·¼ ì„±ê³¼ ë¶„ì„ (ë ˆì§ë³„)
                regime_performance = {}
                for result in recent_results:
                    if agent.agent_id in result.get("results", {}):
                        regime_label = result.get("regime_label", "neutral")
                        if regime_label not in regime_performance:
                            regime_performance[regime_label] = []
                        regime_performance[regime_label].append(result["results"][agent.agent_id])
                
                if not regime_performance:
                    continue
                
                # ë ˆì§ë³„ ì„±ê³¼ ë¶„ì„ ë° ì „ëµ ì¡°ì •
                for regime_label, performances in regime_performance.items():
                    if not performances:
                        continue
                    
                    # ğŸ”¥ í‰ê· (Mean) -> ì¤‘ì•™ê°’(Median) ë³€ê²½ìœ¼ë¡œ ì´ìƒì¹˜ ì˜í–¥ ìµœì†Œí™”
                    avg_win_rate = np.median([p.get("win_rate", 0) for p in performances])
                    avg_pnl = np.median([p.get("total_pnl", 0) for p in performances])
                    
                    # ë ˆì§ë³„ ì„±ê³¼ê°€ ë‚˜ìœ ê²½ìš° íŒŒë¼ë¯¸í„° ì¡°ì •
                    if avg_win_rate < 0.4 or avg_pnl < 0:
                        # ë ˆì§ë³„ íŒŒë¼ë¯¸í„° ì¡°ì •
                        if regime_label in ["extreme_bullish", "bullish"]:
                            # ê°•ì„¸ì¥ì—ì„œëŠ” ë” ê³µê²©ì 
                            if "rsi_min" in agent.strategy_params:
                                agent.strategy_params["rsi_min"] = max(15, 
                                    agent.strategy_params["rsi_min"] - 2)
                            if "rsi_max" in agent.strategy_params:
                                agent.strategy_params["rsi_max"] = min(85, 
                                    agent.strategy_params["rsi_max"] + 2)
                        
                        elif regime_label in ["extreme_bearish", "bearish"]:
                            # ì•½ì„¸ì¥ì—ì„œëŠ” ë” ë³´ìˆ˜ì 
                            if "rsi_min" in agent.strategy_params:
                                agent.strategy_params["rsi_min"] = min(45, 
                                    agent.strategy_params["rsi_min"] + 2)
                            if "rsi_max" in agent.strategy_params:
                                agent.strategy_params["rsi_max"] = max(55, 
                                    agent.strategy_params["rsi_max"] - 2)
                        
                        elif regime_label in ["sideways_bullish", "sideways_bearish", "neutral"]:
                            # íš¡ë³´ì¥ì—ì„œëŠ” ì¤‘ê°„ê°’ ì¡°ì • (ë°˜ì˜¬ë¦¼ ì ìš©)
                            adjustment_factor = np.random.uniform(0.95, 1.05)
                            if "rsi_min" in agent.strategy_params:
                                agent.strategy_params["rsi_min"] = round(max(25, min(35, 
                                    agent.strategy_params["rsi_min"] * adjustment_factor)), 1)
                            if "rsi_max" in agent.strategy_params:
                                agent.strategy_params["rsi_max"] = round(max(65, min(75, 
                                    agent.strategy_params["rsi_max"] * adjustment_factor)), 1)
                        
                        logger.info(f"ğŸ”„ {agent.agent_id} {regime_label} ì „ëµ ì—…ë°ì´íŠ¸: ìŠ¹ë¥  {avg_win_rate:.2%}, ìˆ˜ìµ {avg_pnl:.2f}")
                
        except Exception as e:
            logger.error(f"âŒ ì „ëµ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
    
    def _analyze_cycle_results(self, cycle_results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ğŸš€ ë ˆì§ ê¸°ë°˜ ì‚¬ì´í´ ê²°ê³¼ ë¶„ì„"""
        try:
            if not cycle_results:
                return {}
            
            # ì „ì²´ ì„±ê³¼ ì§‘ê³„
            all_performances = []
            regime_performance = {
                "extreme_bearish": [], "bearish": [], "sideways_bearish": [], 
                "neutral": [], "sideways_bullish": [], "bullish": [], "extreme_bullish": []
            }
            
            for result in cycle_results:
                if "results" in result:
                    for agent_id, performance in result["results"].items():
                        all_performances.append(performance)
                        
                        regime_label = result.get("regime_label", "neutral")
                        if regime_label in regime_performance:
                            regime_performance[regime_label].append(performance)
            
            if not all_performances:
                return {}
            
            # ì „ì²´ í†µê³„
            total_trades = sum(p.get("total_trades", 0) for p in all_performances)
            # ğŸ”¥ í‰ê· (Mean) -> ì¤‘ì•™ê°’(Median) ë³€ê²½ìœ¼ë¡œ ì´ìƒì¹˜ ì˜í–¥ ìµœì†Œí™”
            avg_win_rate = np.median([p.get("win_rate", 0) for p in all_performances])
            avg_pnl = np.median([p.get("total_pnl", 0) for p in all_performances])
            avg_sharpe = np.median([p.get("sharpe_ratio", 0) for p in all_performances])

            # ğŸ”¥ ìµœê³  ì„±ê³¼ ì—ì´ì „íŠ¸ ì°¾ê¸°
            best_agent_pnl = max([p.get("total_pnl", 0) for p in all_performances]) if all_performances else 0.0

            # ë ˆì§ë³„ ì„±ê³¼
            regime_stats = {}
            for regime_label, performances in regime_performance.items():
                if performances:
                    regime_stats[regime_label] = {
                        "avg_win_rate": np.median([p.get("win_rate", 0) for p in performances]),
                        "avg_pnl": np.median([p.get("total_pnl", 0) for p in performances]),
                        "avg_sharpe_ratio": np.median([p.get("sharpe_ratio", 0) for p in performances]),
                        "episode_count": len(performances),
                        "total_trades": sum(p.get("total_trades", 0) for p in performances)
                    }
            
            return {
                "total_episodes": len(cycle_results),
                "total_trades": total_trades,
                "avg_win_rate": avg_win_rate,
                "avg_pnl": avg_pnl,
                "avg_sharpe_ratio": avg_sharpe,
                "best_agent_pnl": best_agent_pnl,
                "regime_performance": regime_stats,
                "learning_progress": self._calculate_learning_progress(cycle_results)
            }
            
        except Exception as e:
            logger.error(f"âŒ ì‚¬ì´í´ ê²°ê³¼ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {}
    
    def _format_cycle_summary(self, summary: Dict[str, Any]) -> str:
        """ì‚¬ì´í´ ìš”ì•½ í¬ë§·íŒ… (ì†Œìˆ˜ì  ì •ë¦¬)"""
        try:
            if not summary:
                return "{}"
            
            # ì „ì²´ í†µê³„ í¬ë§·
            formatted_summary = {
                "total_episodes": summary.get("total_episodes", 0),
                "total_trades": summary.get("total_trades", 0),
                "avg_win_rate": round(summary.get("avg_win_rate", 0), 2),
                "avg_pnl": round(summary.get("avg_pnl", 0), 0),
                "avg_sharpe_ratio": round(summary.get("avg_sharpe_ratio", 0), 4),
            }
            
            # ë ˆì§ë³„ ì„±ê³¼ í¬ë§·
            regime_perf = summary.get("regime_performance", {})
            formatted_regime = {}
            for regime, stats in regime_perf.items():
                formatted_regime[regime] = {
                    "avg_win_rate": round(stats.get("avg_win_rate", 0), 2),
                    "avg_pnl": round(stats.get("avg_pnl", 0), 0),
                    "avg_sharpe_ratio": round(stats.get("avg_sharpe_ratio", 0), 4),
                    "episode_count": stats.get("episode_count", 0),
                    "total_trades": stats.get("total_trades", 0)
                }
            
            formatted_summary["regime_performance"] = formatted_regime
            
            # í•™ìŠµ ì§„í–‰ë„ í¬ë§·
            learning_prog = summary.get("learning_progress", {})
            if learning_prog:
                formatted_summary["learning_progress"] = {
                    "progress": round(learning_prog.get("progress", 0), 2),
                    "trend": learning_prog.get("trend", "stable"),
                    "pnl_improvement": round(learning_prog.get("pnl_improvement", 0), 2),
                    "win_rate_improvement": round(learning_prog.get("win_rate_improvement", 0), 3)
                }
            
            return str(formatted_summary)
            
        except Exception as e:
            logger.error(f"âŒ í¬ë§·íŒ… ì‹¤íŒ¨: {e}")
            return str(summary)
    
    def _calculate_performance_diversity(self, performances: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ì„±ê³¼ ë‹¤ì–‘ì„± ê³„ì‚° (ë³€ë™ ê³„ìˆ˜)"""
        try:
            if not performances or len(performances) < 3:
                return {'coefficient_of_variation': 1.0, 'mean': 0.0, 'std': 0.0}
            
            # ìŠ¹ë¥  ê¸°ì¤€ ë‹¤ì–‘ì„± ê³„ì‚°
            win_rates = [p.get('win_rate', 0) for p in performances]
            mean_wr = np.mean(win_rates)
            std_wr = np.std(win_rates)
            cv_wr = std_wr / mean_wr if mean_wr > 0 else 0
            
            return {
                'coefficient_of_variation': cv_wr,
                'mean': mean_wr,
                'std': std_wr,
                'min': np.min(win_rates),
                'max': np.max(win_rates),
                'range': np.max(win_rates) - np.min(win_rates)
            }
            
        except Exception as e:
            logger.warning(f"âš ï¸ ë‹¤ì–‘ì„± ê³„ì‚° ì‹¤íŒ¨: {e}")
            return {'coefficient_of_variation': 1.0, 'mean': 0.0, 'std': 0.0}
    
    def _calculate_learning_progress(self, cycle_results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """í•™ìŠµ ì§„í–‰ë„ ê³„ì‚°"""
        try:
            if len(cycle_results) < 2:
                return {"progress": 0.0, "trend": "stable"}
            
            # ìµœê·¼ ì ˆë°˜ê³¼ ì´ì „ ì ˆë°˜ ë¹„êµ
            mid_point = len(cycle_results) // 2
            early_results = cycle_results[:mid_point]
            recent_results = cycle_results[mid_point:]
            
            early_performances = []
            recent_performances = []
            
            for result in early_results:
                if "results" in result:
                    early_performances.extend(result["results"].values())
            
            for result in recent_results:
                if "results" in result:
                    recent_performances.extend(result["results"].values())
            
            if not early_performances or not recent_performances:
                return {"progress": 0.0, "trend": "stable"}
            
            # ì„±ê³¼ ë¹„êµ
            early_avg_pnl = np.mean([p.get("total_pnl", 0) for p in early_performances])
            recent_avg_pnl = np.mean([p.get("total_pnl", 0) for p in recent_performances])
            
            early_avg_win_rate = np.mean([p.get("win_rate", 0) for p in early_performances])
            recent_avg_win_rate = np.mean([p.get("win_rate", 0) for p in recent_performances])
            
            # ì§„í–‰ë„ ê³„ì‚°
            pnl_improvement = (recent_avg_pnl - early_avg_pnl) / abs(early_avg_pnl) if early_avg_pnl != 0 else 0
            win_rate_improvement = recent_avg_win_rate - early_avg_win_rate
            
            overall_progress = (pnl_improvement + win_rate_improvement) / 2
            
            # íŠ¸ë Œë“œ íŒë‹¨
            if overall_progress > 0.1:
                trend = "improving"
            elif overall_progress < -0.1:
                trend = "declining"
            else:
                trend = "stable"
            
            return {
                "progress": overall_progress,
                "trend": trend,
                "pnl_improvement": pnl_improvement,
                "win_rate_improvement": win_rate_improvement
            }
            
        except Exception as e:
            logger.error(f"âŒ í•™ìŠµ ì§„í–‰ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return {"progress": 0.0, "trend": "stable"}

def run_self_play_test(
    strategy_params_list: List[Dict[str, Any]],
    episodes: int = 5,
    all_strategy_pool: List[Dict[str, Any]] = None,
    agents_per_episode: int = None,
    candle_data: pd.DataFrame = None,
    agent_type: str = 'rule',
    neural_policy: Optional[Dict[str, Any]] = None,
    hybrid_config: Optional[Dict[str, Any]] = None,
    coin: Optional[str] = None,
    interval: Optional[str] = None,
    session_id: Optional[str] = None
) -> Dict[str, Any]:
    """
    Self-play í…ŒìŠ¤íŠ¸ ì‹¤í–‰ - ë§¤ ì—í”¼ì†Œë“œë§ˆë‹¤ ë‹¤ë¥¸ ì „ëµ ìƒ˜í”Œë§
    
    Args:
        strategy_params_list: ì´ˆê¸° ì „ëµ íŒŒë¼ë¯¸í„° ë¦¬ìŠ¤íŠ¸ (í˜¸í™˜ì„±)
        episodes: í•™ìŠµ ì—í”¼ì†Œë“œ ìˆ˜
        all_strategy_pool: ì „ì²´ ì „ëµ í’€ (DBì—ì„œ ë¡œë“œí•œ ëª¨ë“  ì „ëµ, Noneì´ë©´ strategy_params_list ì‚¬ìš©)
        agents_per_episode: ë§¤ ì—í”¼ì†Œë“œë§ˆë‹¤ ì‚¬ìš©í•  ì—ì´ì „íŠ¸ ìˆ˜ (Noneì´ë©´ strategy_params_list ê¸¸ì´)
        candle_data: ì‹¤ì œ ìº”ë“¤ ë°ì´í„° (Noneì´ë©´ ê°€ìƒ ë°ì´í„° ìƒì„±) ğŸ”¥
        agent_type: 'rule' or 'hybrid'
        neural_policy: ì‹ ê²½ë§ ì •ì±… (hybrid ëª¨ë“œì¼ ë•Œ í•„ìš”)
        hybrid_config: í•˜ì´ë¸Œë¦¬ë“œ ì„¤ì • (hybrid ëª¨ë“œì¼ ë•Œ í•„ìš”)
    """
    try:
        logger.info(f"ğŸš€ Self-play í…ŒìŠ¤íŠ¸ ì‹œì‘ (agent_type={agent_type})")
        
        if candle_data is not None:
            logger.info(f"âœ… ì‹¤ì œ ìº”ë“¤ ë°ì´í„° ì‚¬ìš©: {len(candle_data)}ê°œ")
        else:
            logger.info("âš ï¸ ê°€ìƒ ì‹œì¥ ë°ì´í„° ìƒì„± (candle_data ë¯¸ì œê³µ)")
        
        # ì „ì²´ ì „ëµ í’€ ì„¤ì •
        strategy_pool = all_strategy_pool if all_strategy_pool else strategy_params_list
        agents_per_episode = agents_per_episode or len(strategy_params_list)
        
        # ì „ëµ í’€ í¬ê¸° ë¡œê¹…
        logger.info(f"ğŸ“Š ì „ëµ í’€ í¬ê¸°: {len(strategy_pool)}ê°œ, ë§¤ ì—í”¼ì†Œë“œ {agents_per_episode}ê°œ ì‚¬ìš©")
        
        # ì‹œë®¬ë ˆì´í„° ì´ˆê¸°í™” (session_id ì „ë‹¬)
        simulator = SelfPlaySimulator(session_id=session_id)
        
        # ì—ì´ì „íŠ¸ ìƒì„± (ì´ˆê¸° ì—ì´ì „íŠ¸, ì‹¤ì œë¡œëŠ” ë§¤ ì—í”¼ì†Œë“œë§ˆë‹¤ ë‹¤ì‹œ ìƒì„±ë¨)
        agents = simulator.create_agents(
            strategy_params_list,
            agent_type=agent_type,
            neural_policy=neural_policy,
            hybrid_config=hybrid_config,
            coin=coin  # ğŸ”¥ ì½”ì¸ë³„ íŒŒë¼ë¯¸í„° ìµœì í™”
        )
        logger.info(f"âœ… {len(agents)}ê°œ ì´ˆê¸° ì—ì´ì „íŠ¸ ìƒì„± ì™„ë£Œ")
        
        # í•™ìŠµ ì‚¬ì´í´ ì‹¤í–‰ - ë§¤ ì—í”¼ì†Œë“œë§ˆë‹¤ ë‹¤ë¥¸ ì „ëµ ìƒ˜í”Œë§ + ì‹¤ì œ ìº”ë“¤ ë°ì´í„°
        result = simulator.run_learning_cycle(agents, episodes=episodes, 
                                             all_strategy_pool=strategy_pool,
                                             agents_per_episode=agents_per_episode,
                                             candle_data=candle_data,  # ğŸ”¥ ì‹¤ì œ ìº”ë“¤ ë°ì´í„° ì „ë‹¬
                                             coin=coin,  # ğŸ”¥ ì˜µì…˜ A: coin ì „ë‹¬
                                             interval=interval)  # ğŸ”¥ ì˜µì…˜ A: interval ì „ë‹¬
        
        if result["status"] == "success":
            logger.info("âœ… Self-play í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
            return result
        else:
            logger.error(f"âŒ Self-play í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {result.get('error', 'Unknown error')}")
            return result
            
    except Exception as e:
        logger.error(f"âŒ Self-play í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        return {"status": "failed", "error": str(e)}

def run_self_play_evolution(strategy_params_list: List[Dict[str, Any]], 
                           episodes: int = 3) -> Dict[str, Any]:
    """
    Self-play ì§„í™” í•¨ìˆ˜ (run_self_play_testì˜ ë³„ì¹­)
    
    Args:
        strategy_params_list: ì „ëµ íŒŒë¼ë¯¸í„° ë¦¬ìŠ¤íŠ¸
        episodes: í•™ìŠµ ì—í”¼ì†Œë“œ ìˆ˜
        
    Returns:
        í•™ìŠµ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    """
    logger.info("ğŸš€ Self-play ì§„í™” ì‹œì‘")
    return run_self_play_test(strategy_params_list, episodes)
