"""
DQN Agent í•™ìŠµ ë£¨í”„
ì§„ì§œ ê°•í™”í•™ìŠµ íŠ¸ë ˆì´ë‹!
"""

import sys
import os
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', '..'))

import logging
import numpy as np
import pandas as pd
from typing import Dict, List

from rl_pipeline.agents.dqn_agent import DQNAgent, DQNConfig
from rl_pipeline.agents.replay_buffer import ReplayBuffer
from rl_pipeline.simulation.rl_environment import TradingEnvironment

logger = logging.getLogger(__name__)


def train_dqn_agent(
    candle_data: pd.DataFrame,
    num_episodes: int = 100,
    state_dim: int = 20,
    save_path: str = "models/dqn_agent.pkl",
    log_interval: int = 10
) -> Dict:
    """
    DQN Agent í•™ìŠµ

    Args:
        candle_data: ìº”ë“¤ ë°ì´í„° (RSI, MACD ë“± í¬í•¨)
        num_episodes: í•™ìŠµ ì—í”¼ì†Œë“œ ìˆ˜
        state_dim: ìƒíƒœ ì°¨ì›
        save_path: ëª¨ë¸ ì €ì¥ ê²½ë¡œ
        log_interval: ë¡œê·¸ ì¶œë ¥ ê°„ê²©

    Returns:
        í•™ìŠµ ê²°ê³¼ í†µê³„
    """
    # DQN Agent ìƒì„±
    config = DQNConfig(
        state_dim=state_dim,
        action_dim=3,  # HOLD, BUY, SELL
        hidden_dims=[128, 64, 32],
        learning_rate=0.0001,
        gamma=0.99,
        epsilon_start=1.0,
        epsilon_end=0.01,
        epsilon_decay=0.995,
        batch_size=64,
        buffer_size=10000
    )

    agent = DQNAgent(config)
    buffer = ReplayBuffer(capacity=config.buffer_size, state_dim=state_dim)

    logger.info("ğŸš€ DQN í•™ìŠµ ì‹œì‘")
    logger.info(f"   ì—í”¼ì†Œë“œ: {num_episodes}")
    logger.info(f"   ìº”ë“¤ ë°ì´í„°: {len(candle_data)}ê°œ")

    # í•™ìŠµ í†µê³„
    episode_rewards = []
    episode_returns = []
    episode_trades = []
    episode_win_rates = []
    losses = []

    # í•™ìŠµ ë£¨í”„
    for episode in range(num_episodes):
        env = TradingEnvironment(candle_data, state_dim=state_dim)
        state = env.reset()

        episode_reward = 0.0
        episode_loss = []
        done = False

        # í•œ ì—í”¼ì†Œë“œ ì‹¤í–‰
        while not done:
            # í–‰ë™ ì„ íƒ (Epsilon-greedy)
            action = agent.select_action(state, training=True)

            # í™˜ê²½ì—ì„œ ìŠ¤í… ì‹¤í–‰
            next_state, reward, done, info = env.step(action)

            # Experience Replay Bufferì— ì €ì¥
            buffer.add(state, action, reward, next_state, done)

            episode_reward += reward

            # í•™ìŠµ (ë²„í¼ê°€ ì¶©ë¶„íˆ ì°¨ë©´)
            if buffer.is_ready(config.batch_size):
                batch = buffer.sample(config.batch_size)
                loss = agent.train_step(batch)
                episode_loss.append(loss)

            state = next_state

        # ì—í”¼ì†Œë“œ í†µê³„
        stats = env.get_episode_stats()
        episode_rewards.append(episode_reward)
        episode_returns.append(stats['total_return'])
        episode_trades.append(stats['total_trades'])
        episode_win_rates.append(stats['win_rate'])

        if episode_loss:
            losses.append(np.mean(episode_loss))

        # ë¡œê¹…
        if (episode + 1) % log_interval == 0:
            avg_reward = np.mean(episode_rewards[-log_interval:])
            avg_return = np.mean(episode_returns[-log_interval:])
            avg_win_rate = np.mean(episode_win_rates[-log_interval:])
            avg_loss = np.mean(losses[-log_interval:]) if losses else 0

            logger.info(f"Episode {episode + 1}/{num_episodes}")
            logger.info(f"  Avg Reward: {avg_reward:.2f}")
            logger.info(f"  Avg Return: {avg_return:.2%}")
            logger.info(f"  Avg Win Rate: {avg_win_rate:.2%}")
            logger.info(f"  Avg Loss: {avg_loss:.4f}")
            logger.info(f"  Epsilon: {agent.epsilon:.4f}")
            logger.info(f"  Buffer Size: {len(buffer)}")

    # ëª¨ë¸ ì €ì¥
    os.makedirs(os.path.dirname(save_path), exist_ok=True)
    agent.save(save_path)

    logger.info("âœ… DQN í•™ìŠµ ì™„ë£Œ!")
    logger.info(f"   ìµœì¢… Epsilon: {agent.epsilon:.4f}")
    logger.info(f"   ì´ í•™ìŠµ ìŠ¤í…: {agent.train_steps_count}")
    logger.info(f"   ëª¨ë¸ ì €ì¥: {save_path}")

    # ê²°ê³¼ ìš”ì•½
    results = {
        'episode_rewards': episode_rewards,
        'episode_returns': episode_returns,
        'episode_trades': episode_trades,
        'episode_win_rates': episode_win_rates,
        'losses': losses,
        'final_epsilon': agent.epsilon,
        'total_train_steps': agent.train_steps_count
    }

    return results


def load_candle_data_from_db(coin: str, interval: str, limit: int = 5000) -> pd.DataFrame:
    """
    DBì—ì„œ ìº”ë“¤ ë°ì´í„° ë¡œë“œ

    Args:
        coin: ì½”ì¸ (ì˜ˆ: "ADA")
        interval: ì¸í„°ë²Œ (ì˜ˆ: "15m")
        limit: ìµœëŒ€ ìº”ë“¤ ìˆ˜

    Returns:
        ìº”ë“¤ ë°ì´í„° DataFrame
    """
    import sqlite3
    from rl_pipeline.core.env import config

    with sqlite3.connect(config.RL_DB) as conn:
        query = f"""
            SELECT
                timestamp, open, high, low, close, volume,
                rsi, macd, macd_signal, mfi, atr, adx,
                bb_upper, bb_middle, bb_lower,
                volume_ratio, regime_label, regime_stage
            FROM candles
            WHERE coin = ? AND interval = ?
            ORDER BY timestamp DESC
            LIMIT ?
        """

        df = pd.read_sql_query(query, conn, params=(coin, interval, limit))

    # ì—­ìˆœ ì •ë ¬ (ì˜¤ë˜ëœ ê²ƒë¶€í„°)
    df = df.iloc[::-1].reset_index(drop=True)

    # ê²°ì¸¡ì¹˜ ì²˜ë¦¬
    df['rsi'] = df['rsi'].fillna(50.0)
    df['macd'] = df['macd'].fillna(0.0)
    df['macd_signal'] = df['macd_signal'].fillna(0.0)
    df['mfi'] = df['mfi'].fillna(50.0)
    df['atr'] = df['atr'].fillna(0.02)
    df['adx'] = df['adx'].fillna(25.0)
    df['volume_ratio'] = df['volume_ratio'].fillna(1.0)
    df['regime_stage'] = df['regime_stage'].fillna(3)

    # BB ê³„ì‚° (ì—†ìœ¼ë©´)
    if df['bb_upper'].isna().any():
        df['bb_middle'] = df['close']
        df['bb_upper'] = df['close'] * 1.02
        df['bb_lower'] = df['close'] * 0.98

    logger.info(f"âœ… ìº”ë“¤ ë°ì´í„° ë¡œë“œ: {coin}-{interval} ({len(df)}ê°œ)")

    return df


if __name__ == "__main__":
    print("ì‚¬ìš©ë²•: python rl_pipeline/training/train_all_dqn_agents.py")
