"""
ë ˆì§ ë¼ìš°íŒ… ê²°ê³¼ ê²€ì¦
ì‹œì¥ ë ˆì§ íŒë‹¨, ì „ëµ ë¼ìš°íŒ…, ë°±í…ŒìŠ¤íŠ¸ ê²°ê³¼ ê²€ì¦
"""

import os
import sqlite3
import json
import numpy as np
from typing import Dict, Any, List, Optional
from datetime import datetime
import logging

from .base_validator import BaseValidator
from ..core.validation_context import ValidationContext
from ..core.validation_result import ValidationResult, ValidationIssue, ValidationStatus, ValidationSeverity

logger = logging.getLogger(__name__)

class RoutingValidator(BaseValidator):
    """ë ˆì§ ê¸°ë°˜ ë¼ìš°íŒ… ê²°ê³¼ ê²€ì¦"""

    def validate(self, data: Dict[str, Any], context: ValidationContext) -> ValidationResult:
        """ë¼ìš°íŒ… ê²°ê³¼ ê²€ì¦

        Args:
            data: {
                'routing_results': List[Dict],    # ë¼ìš°íŒ… ê²°ê³¼
                'regime': str,                     # ê°ì§€ëœ ë ˆì§
                'selected_strategies': List[Dict], # ì„ íƒëœ ì „ëµ
                'backtest_results': Dict,          # ë°±í…ŒìŠ¤íŠ¸ ê²°ê³¼
                'signal_scores': List[float],      # ì‹ í˜¸ ì ìˆ˜
                'coin': str,
                'interval': str
            }
        """
        result = self.create_result(context)
        start_time = datetime.now()

        try:
            # 1. ë°ì´í„° êµ¬ì¡° ê²€ì¦
            if context.validation_options.get("check_data_types", True):
                self._validate_data_structure(data, result, context)

            # 2. ë ˆì§ íŒë‹¨ ê²€ì¦
            if context.validation_options.get("check_consistency", True):
                self._validate_regime_detection(data, result, context)

            # 3. ì „ëµ ë¼ìš°íŒ… ê²€ì¦
            if context.validation_options.get("check_relationships", True):
                self._validate_strategy_routing(data, result, context)

            # 4. ë°±í…ŒìŠ¤íŠ¸ ê²°ê³¼ ê²€ì¦
            if context.validation_options.get("check_statistics", True):
                self._validate_backtest_results(data, result, context)

            # 5. ì‹ í˜¸ ì ìˆ˜ ê²€ì¦
            if context.validation_options.get("check_ranges", True):
                self._validate_signal_scores(data, result, context)

            # 6. ê³¼ê±° ë°ì´í„°ì™€ ë¹„êµ
            if context.should_compare_with_history():
                self._compare_with_history(data, result, context)

            # 7. ì‹œê°„ ì¼ê´€ì„± ê²€ì¦
            if context.validation_options.get("check_consistency", True):
                self._validate_temporal_consistency(data, result, context)

        except Exception as e:
            logger.error(f"Routing validation error: {e}")
            result.add_issue(ValidationIssue(
                check_name="routing_validation",
                status=ValidationStatus.FAILED,
                severity=ValidationSeverity.CRITICAL,
                message=f"Validation failed with error: {str(e)}",
                suggestion="Check validation logs for details"
            ))

        result.validation_duration_ms = (datetime.now() - start_time).total_seconds() * 1000
        return result

    def _validate_data_structure(self, data: Dict[str, Any], result: ValidationResult, context: ValidationContext):
        """ë°ì´í„° êµ¬ì¡° ê²€ì¦"""
        required_fields = ['routing_results', 'regime', 'coin', 'interval']

        for field in required_fields:
            issue = self.check_not_none(data.get(field), field)
            result.add_issue(issue)

        # ë¼ìš°íŒ… ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ ê²€ì¦
        if 'routing_results' in data:
            routing_results = data['routing_results']
            issue = self.check_list_not_empty(routing_results, "routing_results", min_size=1)
            result.add_issue(issue)

    def _validate_regime_detection(self, data: Dict[str, Any], result: ValidationResult, context: ValidationContext):
        """ë ˆì§ íŒë‹¨ ê²€ì¦"""
        regime = data.get('regime', '').lower()
        # ğŸ”¥ 7ê°œ ë ˆì§ ì²´ê³„ë¡œ ì—…ë°ì´íŠ¸
        valid_regimes = [
            'extreme_bearish', 'bearish', 'sideways_bearish',
            'neutral',
            'sideways_bullish', 'bullish', 'extreme_bullish'
        ]

        if regime not in valid_regimes:
            result.add_issue(ValidationIssue(
                check_name="regime_validity",
                status=ValidationStatus.FAILED,
                severity=ValidationSeverity.HIGH,
                message=f"Invalid regime detected: {regime}",
                expected=f"One of {valid_regimes}",
                actual=regime,
                suggestion="Check regime detection logic"
            ))
        else:
            result.add_issue(ValidationIssue(
                check_name="regime_validity",
                status=ValidationStatus.PASSED,
                severity=ValidationSeverity.INFO,
                message=f"Valid regime detected: {regime}"
            ))

        # ë¼ìš°íŒ… ê²°ê³¼ì™€ ë ˆì§ ì¼ê´€ì„± ì²´í¬
        routing_results = data.get('routing_results', [])
        if routing_results:
            # ê° ë¼ìš°íŒ… ê²°ê³¼ì˜ ë ˆì§ê³¼ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸
            inconsistent_regimes = []
            for i, rr in enumerate(routing_results[:10]):  # ìƒ˜í”Œ ì²´í¬
                if 'regime' in rr and rr['regime'].lower() != regime:
                    inconsistent_regimes.append((i, rr['regime']))

            if inconsistent_regimes:
                result.add_issue(ValidationIssue(
                    check_name="regime_consistency",
                    status=ValidationStatus.WARNING,
                    severity=ValidationSeverity.MEDIUM,
                    message=f"Found {len(inconsistent_regimes)} routing results with different regimes",
                    actual=str(inconsistent_regimes[:3]),
                    suggestion="Ensure regime is consistently applied"
                ))

    def _validate_strategy_routing(self, data: Dict[str, Any], result: ValidationResult, context: ValidationContext):
        """ì „ëµ ë¼ìš°íŒ… ê²€ì¦"""
        selected_strategies = data.get('selected_strategies', [])
        routing_results = data.get('routing_results', [])
        regime = data.get('regime', '').lower()

        # ì„ íƒëœ ì „ëµ ìˆ˜ ì²´í¬
        if selected_strategies:
            strategy_count = len(selected_strategies)

            # ë ˆì§ë³„ ì ì • ì „ëµ ìˆ˜ (ê²½í—˜ì  ê°’)
            expected_ranges = {
                'bullish': (5, 50),
                'bearish': (5, 50),
                'neutral': (10, 100),
                'volatile': (3, 30),
                'trending': (5, 40),
                'ranging': (10, 60)
            }

            if regime in expected_ranges:
                min_strategies, max_strategies = expected_ranges[regime]
                if not (min_strategies <= strategy_count <= max_strategies):
                    result.add_issue(ValidationIssue(
                        check_name="strategy_count_for_regime",
                        status=ValidationStatus.WARNING,
                        severity=ValidationSeverity.MEDIUM,
                        message=f"Unusual strategy count for {regime} regime: {strategy_count}",
                        expected=f"[{min_strategies}, {max_strategies}]",
                        actual=strategy_count,
                        suggestion=f"Review strategy selection criteria for {regime} market"
                    ))
                else:
                    result.add_issue(ValidationIssue(
                        check_name="strategy_count_for_regime",
                        status=ValidationStatus.PASSED,
                        severity=ValidationSeverity.INFO,
                        message=f"Appropriate strategy count for {regime}: {strategy_count}"
                    ))

        # ë¼ìš°íŒ… ê²°ê³¼ í’ˆì§ˆ ì²´í¬
        if routing_results:
            high_confidence_count = 0
            low_performance_count = 0

            for rr in routing_results[:50]:  # ìƒ˜í”Œ
                if 'confidence' in rr and rr['confidence'] > 0.7:
                    high_confidence_count += 1
                if 'performance_score' in rr and rr['performance_score'] < 0:
                    low_performance_count += 1

            confidence_rate = high_confidence_count / min(50, len(routing_results))
            poor_performance_rate = low_performance_count / min(50, len(routing_results))

            if confidence_rate < 0.3:
                result.add_issue(ValidationIssue(
                    check_name="routing_confidence",
                    status=ValidationStatus.WARNING,
                    severity=ValidationSeverity.MEDIUM,
                    message=f"Low confidence in routing: only {confidence_rate:.1%} have high confidence",
                    suggestion="Review routing criteria and strategy quality"
                ))

            if poor_performance_rate > 0.5:
                result.add_issue(ValidationIssue(
                    check_name="routing_performance",
                    status=ValidationStatus.WARNING,
                    severity=ValidationSeverity.HIGH,
                    message=f"Many poor performers: {poor_performance_rate:.1%} have negative scores",
                    suggestion="Filter out low-quality strategies before routing"
                ))

    def _validate_backtest_results(self, data: Dict[str, Any], result: ValidationResult, context: ValidationContext):
        """ë°±í…ŒìŠ¤íŠ¸ ê²°ê³¼ ê²€ì¦"""
        backtest = data.get('backtest_results', {})

        if not backtest:
            result.add_issue(ValidationIssue(
                check_name="backtest_presence",
                status=ValidationStatus.WARNING,
                severity=ValidationSeverity.HIGH,
                message="No backtest results found",
                suggestion="Ensure backtest is performed after routing"
            ))
            return

        # ì£¼ìš” ë©”íŠ¸ë¦­ ê²€ì¦
        metrics = ['total_return', 'sharpe_ratio', 'max_drawdown', 'win_rate', 'total_trades']

        for metric in metrics:
            if metric not in backtest:
                result.add_issue(ValidationIssue(
                    check_name=f"backtest_{metric}",
                    status=ValidationStatus.WARNING,
                    severity=ValidationSeverity.MEDIUM,
                    message=f"Missing backtest metric: {metric}"
                ))
                continue

            value = backtest[metric]

            # ë©”íŠ¸ë¦­ë³„ í•©ë¦¬ì  ë²”ìœ„ ì²´í¬
            if metric == 'total_return':
                issue = self.check_range(value, -0.5, 2.0, metric, ValidationSeverity.MEDIUM)
                result.add_issue(issue)

            elif metric == 'sharpe_ratio':
                issue = self.check_range(value, -2.0, 3.0, metric, ValidationSeverity.LOW)
                result.add_issue(issue)

            elif metric == 'max_drawdown':
                issue = self.check_range(value, -1.0, 0, metric, ValidationSeverity.MEDIUM)
                result.add_issue(issue)

            elif metric == 'win_rate':
                issue = self.check_range(value, 0.2, 0.8, metric, ValidationSeverity.MEDIUM)
                result.add_issue(issue)

            elif metric == 'total_trades':
                min_trades = 10
                if value < min_trades:
                    result.add_issue(ValidationIssue(
                        check_name=f"backtest_{metric}",
                        status=ValidationStatus.WARNING,
                        severity=ValidationSeverity.HIGH,
                        message=f"Too few trades in backtest: {value}",
                        expected=f">= {min_trades}",
                        actual=value,
                        suggestion="Increase backtest period or review signal generation"
                    ))

    def _validate_signal_scores(self, data: Dict[str, Any], result: ValidationResult, context: ValidationContext):
        """ì‹ í˜¸ ì ìˆ˜ ê²€ì¦"""
        signal_scores = data.get('signal_scores', [])

        if not signal_scores:
            return

        # ì ìˆ˜ ë²”ìœ„ ì²´í¬ (0-1)
        invalid_scores = [s for s in signal_scores if not (0 <= s <= 1)]

        if invalid_scores:
            result.add_issue(ValidationIssue(
                check_name="signal_score_range",
                status=ValidationStatus.FAILED,
                severity=ValidationSeverity.HIGH,
                message=f"Found {len(invalid_scores)} signal scores out of [0,1] range",
                actual=f"Invalid scores: {invalid_scores[:5]}",
                suggestion="Normalize signal scores to [0,1] range"
            ))
        else:
            result.add_issue(ValidationIssue(
                check_name="signal_score_range",
                status=ValidationStatus.PASSED,
                severity=ValidationSeverity.INFO,
                message="All signal scores within valid range"
            ))

        # ì ìˆ˜ ë¶„í¬ ì²´í¬
        if len(signal_scores) > 10:
            mean_score = np.mean(signal_scores)
            std_score = np.std(signal_scores)

            # ëª¨ë“  ì ìˆ˜ê°€ ë„ˆë¬´ ë¹„ìŠ·í•œ ê²½ìš° (ë¶„ì‚°ì´ ë„ˆë¬´ ì‘ìŒ)
            if std_score < 0.01:
                result.add_issue(ValidationIssue(
                    check_name="signal_score_diversity",
                    status=ValidationStatus.WARNING,
                    severity=ValidationSeverity.MEDIUM,
                    message=f"Signal scores lack diversity (std={std_score:.4f})",
                    suggestion="Review scoring logic - may not be discriminative enough"
                ))

            # í‰ê· ì´ ê·¹ë‹¨ì ì¸ ê²½ìš°
            if mean_score < 0.2 or mean_score > 0.8:
                result.add_issue(ValidationIssue(
                    check_name="signal_score_distribution",
                    status=ValidationStatus.WARNING,
                    severity=ValidationSeverity.MEDIUM,
                    message=f"Signal scores skewed: mean={mean_score:.2f}",
                    suggestion="Review scoring calibration"
                ))

    def _validate_temporal_consistency(self, data: Dict[str, Any], result: ValidationResult, context: ValidationContext):
        """ì‹œê°„ì  ì¼ê´€ì„± ê²€ì¦"""
        routing_results = data.get('routing_results', [])

        if not routing_results:
            return

        # íƒ€ì„ìŠ¤íƒ¬í”„ ìˆœì„œ ì²´í¬
        timestamps = []
        for rr in routing_results:
            if 'timestamp' in rr:
                try:
                    ts = datetime.fromisoformat(rr['timestamp'])
                    timestamps.append(ts)
                except:
                    pass

        if len(timestamps) > 1:
            # ì‹œê°„ ìˆœì„œê°€ ì˜¬ë°”ë¥¸ì§€ ì²´í¬
            is_sorted = all(timestamps[i] <= timestamps[i+1] for i in range(len(timestamps)-1))

            if not is_sorted:
                result.add_issue(ValidationIssue(
                    check_name="temporal_ordering",
                    status=ValidationStatus.WARNING,
                    severity=ValidationSeverity.HIGH,
                    message="Routing results not in chronological order",
                    suggestion="Sort results by timestamp"
                ))

            # ì‹œê°„ ê°­ ì²´í¬
            gaps = [(timestamps[i+1] - timestamps[i]).total_seconds()
                   for i in range(len(timestamps)-1)]

            if gaps:
                max_gap = max(gaps)
                if max_gap > 3600:  # 1ì‹œê°„ ì´ìƒ ê°­
                    result.add_issue(ValidationIssue(
                        check_name="temporal_gaps",
                        status=ValidationStatus.WARNING,
                        severity=ValidationSeverity.MEDIUM,
                        message=f"Large time gap detected: {max_gap/3600:.1f} hours",
                        suggestion="Check for missing data periods"
                    ))

    def _compare_with_history(self, data: Dict[str, Any], result: ValidationResult, context: ValidationContext):
        """ê³¼ê±° ë¼ìš°íŒ… ê²°ê³¼ì™€ ë¹„êµ"""
        if not self.db_connections or 'learning_results' not in self.db_connections:
            return

        try:
            coin = data.get('coin')
            interval = data.get('interval')
            regime = data.get('regime')
            current_count = len(data.get('routing_results', []))

            # ê°™ì€ ë ˆì§ì˜ ê³¼ê±° ë¼ìš°íŒ… ê²°ê³¼ ì¡°íšŒ
            historical_data = self.get_historical_data(
                self.db_connections['learning_results'],
                """
                SELECT routing_count, performance_score
                FROM regime_routing_results
                WHERE symbol = ? AND interval = ? AND regime = ?
                AND created_at > datetime('now', '-14 days')
                ORDER BY created_at DESC
                LIMIT 10
                """,
                (coin, interval, regime)
            )

            if historical_data:
                hist_counts = [d['routing_count'] for d in historical_data]
                if hist_counts:
                    count_issue = self.compare_with_history(
                        current_count, hist_counts, f"{regime}_routing_count"
                    )
                    if count_issue:
                        result.add_issue(count_issue)

        except Exception as e:
            logger.warning(f"Historical comparison failed: {e}")