"""
ë°ì´í„°ë² ì´ìŠ¤ ì“°ê¸° ì „ìš© ì»¤ë„¥ì…˜ ë° ë°°ì¹˜ ì²˜ë¦¬
ë‹¨ì¼ writer íë¡œ ë°°ì¹˜ insert/update, íŠ¸ëœì­ì…˜ ê´€ë¦¬

í•µì‹¬ ì„¤ê³„:
- coin â†’ symbol ë§¤í•‘
- market_type, market ì»¬ëŸ¼ ì¶”ê°€
- í…Œì´ë¸”ëª… ë²”ìš©í™” (strategies â†’ strategies)
"""

import sqlite3
import logging
import pandas as pd
from typing import Dict, List, Any, Optional, Tuple
from contextlib import contextmanager
from datetime import datetime
from rl_pipeline.db.connection_pool import get_strategy_db_pool, get_batch_loading_pool
from rl_pipeline.core.errors import DBWriteError
from rl_pipeline.core.utils import safe_json_dumps, _format_decimal_precision

logger = logging.getLogger(__name__)

# ============================================================================
# ìƒìˆ˜ ì •ì˜
# ============================================================================

DEFAULT_MARKET_TYPE = "COIN"
DEFAULT_MARKET = "BITHUMB"


# ============================================================================
# ë§¤í•‘ ìœ í‹¸ë¦¬í‹°
# ============================================================================

def _map_coin_to_symbol(row: Dict[str, Any]) -> Dict[str, Any]:
    """coin ì»¬ëŸ¼ì„ symbolë¡œ ë§¤í•‘í•˜ê³  market_type, market ì¶”ê°€"""
    result = row.copy()

    # coin â†’ symbol ë§¤í•‘
    if 'coin' in result and 'symbol' not in result:
        result['symbol'] = result.pop('coin')

    # market_type, market ê¸°ë³¸ê°’ ì¶”ê°€
    if 'market_type' not in result:
        result['market_type'] = DEFAULT_MARKET_TYPE
    if 'market' not in result:
        result['market'] = DEFAULT_MARKET

    return result


def _sanitize_strategy_row(row: Dict[str, Any]) -> Dict[str, Any]:
    """ì „ëµ íŒŒë¼ë¯¸í„° í˜„ì‹¤í™” (ê³¼ë„í•œ ì¡°ê±´ ìë™ ì™„í™”)"""
    sanitized = row.copy()
    try:
        stype = str(sanitized.get('strategy_type', '')).lower()
        is_sell_strategy = 'sell' in stype and 'buy' not in stype

        def _safe_float(value, default=None):
            try:
                return float(value)
            except (TypeError, ValueError):
                return default

        vol_min = _safe_float(sanitized.get('volume_ratio_min'))
        vol_max = _safe_float(sanitized.get('volume_ratio_max'))
        if vol_min is not None:
            upper = 2.0 if is_sell_strategy else 3.0
            clamped = min(max(0.4, vol_min), upper)
            if clamped != vol_min and logger.isEnabledFor(logging.DEBUG):
                logger.debug(f"ğŸ“‰ volume_ratio_min ì¡°ì •: {vol_min:.3f} â†’ {clamped:.3f} (strategy={stype})")
            sanitized['volume_ratio_min'] = clamped
            vol_min = clamped
        if vol_max is not None:
            upper_max = 3.2 if is_sell_strategy else 4.0
            clamped_max = min(max(vol_max, (vol_min + 0.2) if vol_min is not None else 0.6), upper_max)
            if clamped_max != vol_max and logger.isEnabledFor(logging.DEBUG):
                logger.debug(f"ğŸ“‰ volume_ratio_max ì¡°ì •: {vol_max:.3f} â†’ {clamped_max:.3f} (strategy={stype})")
            sanitized['volume_ratio_max'] = clamped_max
        elif vol_min is not None:
            sanitized['volume_ratio_max'] = vol_min + 0.2

        rsi_max = _safe_float(sanitized.get('rsi_max'))
        if rsi_max is not None:
            max_cap = 78.0 if is_sell_strategy else 85.0
            clamped_rsi_max = min(max(rsi_max, 55.0), max_cap)
            if clamped_rsi_max != rsi_max and logger.isEnabledFor(logging.DEBUG):
                logger.debug(f"ğŸ“‰ rsi_max ì¡°ì •: {rsi_max:.2f} â†’ {clamped_rsi_max:.2f} (strategy={stype})")
            sanitized['rsi_max'] = clamped_rsi_max

        rsi_min = _safe_float(sanitized.get('rsi_min'))
        if rsi_min is not None and rsi_min >= sanitized.get('rsi_max', rsi_min + 5):
            sanitized['rsi_min'] = sanitized['rsi_max'] - 5.0

        return sanitized
    except Exception as exc:
        logger.warning(f"âš ï¸ ì „ëµ íŒŒë¼ë¯¸í„° ì •ê·œí™” ì‹¤íŒ¨: {exc}")
        return row


def _map_rows_to_schema(rows: List[Dict[str, Any]], table: str) -> List[Dict[str, Any]]:
    """rows ë¦¬ìŠ¤íŠ¸ë¥¼ ìŠ¤í‚¤ë§ˆì— ë§ê²Œ ë§¤í•‘"""
    # ë§¤í•‘ í•„ìš”í•œ í…Œì´ë¸” ëª©ë¡
    mapped_tables = {
        'strategies', 'strategy_performance_rl', 'strategy_grades',
        'rl_episodes', 'rl_episode_summary', 'rl_state_ensemble',
        'global_strategies', 'analysis_ratios', 'symbol_global_weights',
        'runs', 'run_records', 'pipeline_execution_logs',
        'integrated_analysis_results', 'strategy_training_history'
    }

    # í˜¸í™˜ì„± ë·° (ì‹¤ì œ í…Œì´ë¸”ëª…ìœ¼ë¡œ ë³€í™˜)
    table_mapping = {
        'strategies': 'strategies',
        'rl_strategy_rollup': 'strategy_performance_rl',
        'coin_analysis_ratios': 'analysis_ratios',
        'coin_global_weights': 'symbol_global_weights'
    }

    # í…Œì´ë¸”ëª… ë³€í™˜
    actual_table = table_mapping.get(table, table)

    # ë§¤í•‘ì´ í•„ìš”í•œ í…Œì´ë¸”ì´ë©´ ì ìš©
    if actual_table in mapped_tables:
        mapped = [_map_coin_to_symbol(row) for row in rows]
        if actual_table == 'strategies':
            # strategy_id í‚¤ê°€ ì—†ê³  idê°€ ìˆìœ¼ë©´ strategy_idë¡œ ë³€í™˜ (DB ìŠ¤í‚¤ë§ˆê°€ strategy_idê°€ ì•„ë‹Œ idë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ ë§¤í•‘ ë¡œì§ ìˆ˜ì • í•„ìš”)
            # âš ï¸ ì‹¤ì œ strategies í…Œì´ë¸” ìŠ¤í‚¤ë§ˆëŠ” id TEXT PRIMARY KEYë¡œ ë˜ì–´ ìˆìŒ.
            # ë”°ë¼ì„œ strategy_id -> idë¡œ ë³€í™˜í•´ì•¼ í•¨.
            def _map_strategy_id(r):
                new_r = r.copy()
                # strategy_id í‚¤ê°€ ìˆìœ¼ë©´ ì²˜ë¦¬
                if 'strategy_id' in new_r:
                    # id í‚¤ê°€ ì—†ìœ¼ë©´ strategy_id ê°’ì„ idë¡œ ì´ë™
                    if 'id' not in new_r:
                        new_r['id'] = new_r['strategy_id']
                    # strategy_id í‚¤ëŠ” ë¬´ì¡°ê±´ ì œê±° (í…Œì´ë¸”ì— ì—†ëŠ” ì»¬ëŸ¼ì´ë¯€ë¡œ)
                    del new_r['strategy_id']
                return new_r
            
            mapped = [_map_strategy_id(row) for row in mapped]
            mapped = [_sanitize_strategy_row(row) for row in mapped]
        return mapped

    return rows

def execute_query(query: str, params: tuple = (), db_path: str = "strategies") -> bool:
    """ì¿¼ë¦¬ ì‹¤í–‰"""
    try:
        pool = get_strategy_db_pool() if db_path == "strategies" else get_batch_loading_pool()
        
        with pool.get_connection() as conn:
            cursor = conn.cursor()
            cursor.execute(query, params)
            conn.commit()
            return True
            
    except Exception as e:
        logger.error(f"âŒ ì¿¼ë¦¬ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        return False

def write_batch(rows: List[Dict[str, Any]], table: str, db_path: str = None, checkpoint: bool = False, verify: bool = False, max_retries: int = 5) -> int:
    """ë°°ì¹˜ ì“°ê¸°
    Args:
        rows: ì‚½ì…/ì—…ë°ì´íŠ¸í•  í–‰ ëª©ë¡
        table: ëŒ€ìƒ í…Œì´ë¸”ëª…
        db_path: ì„ íƒì  DB ê²½ë¡œ (ì—†ìœ¼ë©´ ì „ëµ DB ì‚¬ìš©)
        checkpoint: ì“°ê¸° ì´í›„ WAL ì²´í¬í¬ì¸íŠ¸ë¥¼ ê°•ì œ ìˆ˜í–‰í• ì§€ ì—¬ë¶€ (ê¸°ë³¸ False)
        verify: ì“°ê¸° ì§í›„ COUNT ê²€ì¦ì„ ìˆ˜í–‰í• ì§€ ì—¬ë¶€ (ê¸°ë³¸ False)
        max_retries: ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜
    """
    if not rows:
        return 0

    import time
    import random

    for attempt in range(max_retries):
        try:
            if db_path:
                pool = get_batch_loading_pool(db_path)
            else:
                pool = get_strategy_db_pool()

            # í…Œì´ë¸”ëª… ë§¤í•‘
            table_mapping = {
                'strategies': 'strategies',
                'rl_strategy_rollup': 'strategy_performance_rl',
                'coin_analysis_ratios': 'analysis_ratios',
                'coin_global_weights': 'symbol_global_weights'
            }
            actual_table = table_mapping.get(table, table)

            # ìŠ¤í‚¤ë§ˆ ë§¤í•‘ (coin â†’ symbol, market_type, market ì¶”ê°€)
            mapped_rows = _map_rows_to_schema(rows, table)

            with pool.get_connection() as conn:
                cursor = conn.cursor()

                # ì²« ë²ˆì§¸ í–‰ìœ¼ë¡œ ì»¬ëŸ¼ ì •ë³´ ì¶”ì¶œ
                columns = list(mapped_rows[0].keys())
                placeholders = ', '.join(['?' for _ in columns])
                query = f"INSERT OR REPLACE INTO {actual_table} ({', '.join(columns)}) VALUES ({placeholders})"
                
                # ë°°ì¹˜ ì‹¤í–‰
                batch_data = []
                for row in mapped_rows:
                    values = []
                    for col in columns:
                        value = row.get(col)
                        # íŠ¹ì • í•„ë“œëŠ” í¬ë§·íŒ… ì ìš©
                        if col in ['profit', 'win_rate', 'max_drawdown', 'sharpe_ratio', 'profit_factor']:
                            value = _format_decimal_precision(value, col)
                        elif isinstance(value, dict):
                            value = safe_json_dumps(value)
                        values.append(value)
                    batch_data.append(tuple(values))
                
                cursor.executemany(query, batch_data)
                logger.debug(f"ğŸ” executemany ì‹¤í–‰ ì™„ë£Œ: {len(batch_data)}ê°œ í–‰")

                conn.commit()
                logger.debug(f"ğŸ” ì»¤ë°‹ ì™„ë£Œ")

                # ê³ ë¹„ìš© ë™ì‘ì€ ì„ íƒì ìœ¼ë¡œ ìˆ˜í–‰
                if checkpoint:
                    conn.execute("PRAGMA wal_checkpoint(PASSIVE)")
                    logger.debug(f"ğŸ” WAL ì²´í¬í¬ì¸íŠ¸ ì™„ë£Œ (PASSIVE)")

                if verify:
                    cursor.execute(f"SELECT COUNT(*) FROM {actual_table}")
                    count = cursor.fetchone()[0]
                    logger.info(f"ğŸ” ì¦‰ì‹œ í™•ì¸: {actual_table} í…Œì´ë¸”ì— {count}ê°œ ë ˆì½”ë“œ ì¡´ì¬")

                logger.info(f"âœ… ë°°ì¹˜ ì“°ê¸° ì™„ë£Œ: {len(mapped_rows)}í–‰ -> {actual_table}")
                return len(mapped_rows)
                
        except Exception as e:
            is_locked = "database is locked" in str(e) or "disk I/O error" in str(e) or "attempt to write a readonly database" in str(e)
            
            if is_locked and attempt < max_retries - 1:
                wait_time = (2 ** attempt) + random.random()
                logger.warning(f"âš ï¸ DB ì“°ê¸° ì¼ì‹œì  ì‹¤íŒ¨ ({attempt+1}/{max_retries}), {wait_time:.2f}ì´ˆ í›„ ì¬ì‹œë„: {e}")
                time.sleep(wait_time)
                
                # ì»¤ë„¥ì…˜ í’€ ë¦¬ì…‹ ì‹œë„ (readonly ì—ëŸ¬ ë“± ëŒ€ì‘)
                try:
                    if db_path:
                        get_batch_loading_pool(db_path).close_all_connections()
                    else:
                        get_strategy_db_pool().close_all_connections()
                except:
                    pass
            else:
                logger.error(f"âŒ ë°°ì¹˜ ì“°ê¸° ìµœì¢… ì‹¤íŒ¨: {e}")
                raise DBWriteError(f"ë°°ì¹˜ ì“°ê¸° ì‹¤íŒ¨ ({table}): {e}") from e

def upsert(data: Dict[str, Any], table: str, key_columns: List[str], db_path: str = None) -> bool:
    """Upsert (INSERT OR REPLACE)"""
    try:
        if db_path:
            pool = get_strategy_db_pool(db_path)
        else:
            pool = get_strategy_db_pool()
        
        with pool.get_connection() as conn:
            cursor = conn.cursor()
            
            columns = list(data.keys())
            placeholders = ', '.join(['?' for _ in columns])
            query = f"INSERT OR REPLACE INTO {table} ({', '.join(columns)}) VALUES ({placeholders})"
            
            values = []
            for col in columns:
                value = data.get(col)
                if col in ['profit', 'win_rate', 'max_drawdown', 'sharpe_ratio', 'profit_factor']:
                    value = _format_decimal_precision(value, col)
                elif isinstance(value, dict):
                    value = safe_json_dumps(value)
                values.append(value)
            
            cursor.execute(query, tuple(values))
            conn.commit()
            
            logger.debug(f"âœ… Upsert ì™„ë£Œ: {table}")
            return True
            
    except Exception as e:
        logger.error(f"âŒ Upsert ì‹¤íŒ¨: {e}")
        raise DBWriteError(f"Upsert ì‹¤íŒ¨ ({table}): {e}") from e

def update_strategy_performance(strategy_id: str, performance_data: Dict[str, Any]) -> bool:
    """ì „ëµ ì„±ê³¼ ì—…ë°ì´íŠ¸"""
    try:
        pool = get_strategy_db_pool()
        
        with pool.get_connection() as conn:
            cursor = conn.cursor()
            
            # ì„±ê³¼ ë°ì´í„° í¬ë§·íŒ…
            formatted_data = {}
            for key, value in performance_data.items():
                if key in ['profit', 'win_rate', 'max_drawdown', 'sharpe_ratio', 'profit_factor']:
                    formatted_data[key] = _format_decimal_precision(value, key)
                else:
                    formatted_data[key] = value
            
            # ì—…ë°ì´íŠ¸ ì¿¼ë¦¬ ìƒì„±
            set_clauses = []
            values = []
            for key, value in formatted_data.items():
                set_clauses.append(f"{key} = ?")
                values.append(value)
            
            values.append(strategy_id)
            # strategies â†’ strategies
            query = f"UPDATE strategies SET {', '.join(set_clauses)} WHERE id = ?"
            
            cursor.execute(query, tuple(values))
            conn.commit()
            
            logger.debug(f"âœ… ì „ëµ ì„±ê³¼ ì—…ë°ì´íŠ¸ ì™„ë£Œ: {strategy_id}")
            return True
            
    except Exception as e:
        logger.error(f"âŒ ì „ëµ ì„±ê³¼ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
        raise DBWriteError(f"ì „ëµ ì„±ê³¼ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}") from e

def save_strategy_dna(coin: str, dna_data: Dict[str, Any]) -> bool:
    """ì „ëµ DNA ì €ì¥"""
    try:
        pool = get_strategy_db_pool()
        
        with pool.get_connection() as conn:
            cursor = conn.cursor()
            
            # DNA ë°ì´í„° ì¤€ë¹„ (interval ì¶”ì¶œ)
            interval = dna_data.get('interval')
            
            dna_record = {
                'coin': coin,
                'interval': interval,
                'dna_patterns': safe_json_dumps(dna_data.get('dna_patterns', {})),
                'dna_data': safe_json_dumps(dna_data),
                'created_at': pd.Timestamp.now().isoformat(),
                'quality_score': dna_data.get('quality_score', 0.0)
            }
            
            # Upsert ì‹¤í–‰
            columns = list(dna_record.keys())
            placeholders = ', '.join(['?' for _ in columns])
            query = f"INSERT OR REPLACE INTO strategy_dna ({', '.join(columns)}) VALUES ({placeholders})"
            
            values = [dna_record[col] for col in columns]
            cursor.execute(query, tuple(values))
            conn.commit()
            
            logger.info(f"âœ… ì „ëµ DNA ì €ì¥ ì™„ë£Œ: {coin} {interval}")
            return True
            
    except Exception as e:
        logger.error(f"âŒ ì „ëµ DNA ì €ì¥ ì‹¤íŒ¨: {e}")
        raise DBWriteError(f"ì „ëµ DNA ì €ì¥ ì‹¤íŒ¨: {e}") from e

def save_fractal_analysis(coin: str, interval: str, fractal_data: Dict[str, Any]) -> bool:
    """í”„ë™íƒˆ ë¶„ì„ ê²°ê³¼ ì €ì¥ - ê°œì„ ëœ ë²„ì „ (ëª¨ë“  ì»¬ëŸ¼ í¬í•¨)"""
    try:
        pool = get_strategy_db_pool()
        
        with pool.get_connection() as conn:
            cursor = conn.cursor()
            
            # í”„ë™íƒˆ ë°ì´í„° ì¤€ë¹„ (ëª¨ë“  ì»¬ëŸ¼ í¬í•¨)
            fractal_record = {
                'coin': coin,
                'interval': interval,
                'analysis_type': fractal_data.get('analysis_type', 'fractal_pattern'),
                'fractal_score': _format_decimal_precision(fractal_data.get('fractal_score', 0.0), 'fractal_score'),
                'pattern_distribution': safe_json_dumps(fractal_data.get('pattern_distribution', {})),
                'pruned_strategies_count': fractal_data.get('pruned_strategies_count', 0),
                'total_strategies': fractal_data.get('total_strategies', 0),
                'avg_profit': fractal_data.get('avg_profit', 0.0),
                'avg_win_rate': fractal_data.get('avg_win_rate', 0.0),
                'optimal_rsi_min': fractal_data.get('optimal_rsi_min', 30.0),
                'optimal_rsi_max': fractal_data.get('optimal_rsi_max', 70.0),
                'optimal_volume_ratio': fractal_data.get('optimal_volume_ratio', 1.0),
                'created_at': pd.Timestamp.now().isoformat()
            }
            
            # Upsert ì‹¤í–‰
            columns = list(fractal_record.keys())
            placeholders = ', '.join(['?' for _ in columns])
            query = f"INSERT OR REPLACE INTO fractal_analysis ({', '.join(columns)}) VALUES ({placeholders})"
            
            values = [fractal_record[col] for col in columns]
            cursor.execute(query, tuple(values))
            conn.commit()
            
            logger.info(f"âœ… í”„ë™íƒˆ ë¶„ì„ ì €ì¥ ì™„ë£Œ: {coin} {interval} (ì „ì²´ ì»¬ëŸ¼ í¬í•¨)")
            return True
            
    except Exception as e:
        logger.error(f"âŒ í”„ë™íƒˆ ë¶„ì„ ì €ì¥ ì‹¤íŒ¨: {e}")
        raise DBWriteError(f"í”„ë™íƒˆ ë¶„ì„ ì €ì¥ ì‹¤íŒ¨: {e}") from e

def save_synergy_analysis(coin: str, interval: str, synergy_data: Dict[str, Any]) -> bool:
    """ì‹œë„ˆì§€ ë¶„ì„ ê²°ê³¼ ì €ì¥"""
    try:
        pool = get_strategy_db_pool()
        
        with pool.get_connection() as conn:
            cursor = conn.cursor()
            
            # ì‹œë„ˆì§€ ë°ì´í„° ì¤€ë¹„
            synergy_record = {
                'coin': coin,
                'interval': interval,
                'synergy_score': _format_decimal_precision(synergy_data.get('synergy_score', 0.0), 'synergy_score'),
                'synergy_patterns': safe_json_dumps(synergy_data.get('synergy_patterns', {})),
                'created_at': pd.Timestamp.now().isoformat()
            }
            
            # Upsert ì‹¤í–‰
            columns = list(synergy_record.keys())
            placeholders = ', '.join(['?' for _ in columns])
            query = f"INSERT OR REPLACE INTO synergy_analysis ({', '.join(columns)}) VALUES ({placeholders})"
            
            values = [synergy_record[col] for col in columns]
            cursor.execute(query, tuple(values))
            conn.commit()
            
            logger.info(f"âœ… ì‹œë„ˆì§€ ë¶„ì„ ì €ì¥ ì™„ë£Œ: {coin} {interval}")
            return True
            
    except Exception as e:
        logger.error(f"âŒ ì‹œë„ˆì§€ ë¶„ì„ ì €ì¥ ì‹¤íŒ¨: {e}")
        raise DBWriteError(f"ì‹œë„ˆì§€ ë¶„ì„ ì €ì¥ ì‹¤íŒ¨: {e}") from e

def save_run_metadata(run_metadata: Dict[str, Any],
                     market_type: str = DEFAULT_MARKET_TYPE,
                     market: str = DEFAULT_MARKET) -> bool:
    """ì‹¤í–‰ ë©”íƒ€ë°ì´í„° ì €ì¥ (coin â†’ symbol ë§¤í•‘)"""
    try:
        pool = get_strategy_db_pool()

        # coin â†’ symbol
        symbol = run_metadata.get('symbol', run_metadata.get('coin'))

        with pool.get_connection() as conn:
            cursor = conn.cursor()

            # ì‹¤í–‰ ë©”íƒ€ë°ì´í„° ì¤€ë¹„
            run_record = {
                'run_id': run_metadata.get('run_id'),
                'market_type': market_type,
                'market': market,
                'symbol': symbol,
                'interval': run_metadata.get('interval'),
                'start_time': run_metadata.get('start_time'),
                'end_time': run_metadata.get('end_time'),
                'status': run_metadata.get('status', 'running'),
                'strategies_count': run_metadata.get('strategies_count', 0),
                'successful_strategies': run_metadata.get('successful_strategies', 0),
                'error_count': run_metadata.get('error_count', 0)
            }

            # Upsert ì‹¤í–‰
            columns = list(run_record.keys())
            placeholders = ', '.join(['?' for _ in columns])
            query = f"INSERT OR REPLACE INTO runs ({', '.join(columns)}) VALUES ({placeholders})"

            values = [run_record[col] for col in columns]
            cursor.execute(query, tuple(values))
            conn.commit()

            logger.info(f"âœ… ì‹¤í–‰ ë©”íƒ€ë°ì´í„° ì €ì¥ ì™„ë£Œ: {run_record['run_id']}")
            return True

    except Exception as e:
        logger.error(f"âŒ ì‹¤í–‰ ë©”íƒ€ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {e}")
        raise DBWriteError(f"ì‹¤í–‰ ë©”íƒ€ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {e}") from e

def delete_strategies(strategy_ids: List[str]) -> int:
    """ì „ëµ ì‚­ì œ"""
    if not strategy_ids:
        return 0
    
    try:
        pool = get_strategy_db_pool()
        
        with pool.get_connection() as conn:
            cursor = conn.cursor()
            
            placeholders = ', '.join(['?' for _ in strategy_ids])
            # strategies â†’ strategies
            query = f"DELETE FROM strategies WHERE id IN ({placeholders})"
            
            cursor.execute(query, tuple(strategy_ids))
            conn.commit()
            
            deleted_count = cursor.rowcount
            logger.info(f"âœ… ì „ëµ ì‚­ì œ ì™„ë£Œ: {deleted_count}ê°œ")
            return deleted_count
            
    except Exception as e:
        logger.error(f"âŒ ì „ëµ ì‚­ì œ ì‹¤íŒ¨: {e}")
        raise DBWriteError(f"ì „ëµ ì‚­ì œ ì‹¤íŒ¨: {e}") from e

def cleanup_old_data(table: str, days_to_keep: int = 30, db_path: str = None) -> int:
    """ì˜¤ë˜ëœ ë°ì´í„° ì •ë¦¬"""
    try:
        if db_path:
            pool = get_strategy_db_pool()
        else:
            pool = get_strategy_db_pool()
        
        with pool.get_connection() as conn:
            cursor = conn.cursor()
            
            query = f"DELETE FROM {table} WHERE created_at < datetime('now', '-{days_to_keep} days')"
            cursor.execute(query)
            conn.commit()
            
            deleted_count = cursor.rowcount
            logger.info(f"âœ… ì˜¤ë˜ëœ ë°ì´í„° ì •ë¦¬ ì™„ë£Œ: {table}ì—ì„œ {deleted_count}í–‰ ì‚­ì œ")
            return deleted_count
            
    except Exception as e:
        logger.error(f"âŒ ì˜¤ë˜ëœ ë°ì´í„° ì •ë¦¬ ì‹¤íŒ¨: {e}")
        raise DBWriteError(f"ì˜¤ë˜ëœ ë°ì´í„° ì •ë¦¬ ì‹¤íŒ¨: {e}") from e

# âš ï¸ DEPRECATED: ë¯¸ì‚¬ìš© í…Œì´ë¸” ê´€ë ¨ í•¨ìˆ˜ë“¤ (í…Œì´ë¸” ì œê±°ë¨)
# - market_condition_analysis: ë ˆê±°ì‹œ, ë¯¸ì‚¬ìš©
# - dna_market_analysis (strategy_dnaë¡œ ëŒ€ì²´)

# def save_market_condition_analysis(coin: str, interval: str, market_condition: str,
#                                   confidence: float, analysis_data: Dict[str, Any]) -> bool:
#     """ğŸ”´ DEPRECATED: ì‹œì¥ ìƒí™© ë¶„ì„ ê²°ê³¼ ì €ì¥ - market_condition_analysis í…Œì´ë¸” ì œê±°ë¨"""
#     logger.warning("âš ï¸ save_market_condition_analysisëŠ” ë” ì´ìƒ ì‚¬ìš©ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤ (í…Œì´ë¸” ì œê±°ë¨)")
#     return False
# - fractal_market_analysis (fractal_analysisë¡œ ëŒ€ì²´)
# - routing_market_analysis (regime_routing_resultsë¡œ ëŒ€ì²´)

def save_dna_by_market_condition(coin: str, interval: str, market_condition: str, 
                               dna_patterns: Dict[str, Any]) -> bool:
    """âš ï¸ DEPRECATED: ë¯¸ì‚¬ìš© í…Œì´ë¸” (dna_market_analysis) - í•¨ìˆ˜ëŠ” ìœ ì§€í•˜ë˜ ë™ì‘ ì•ˆí•¨"""
    logger.debug(f"âš ï¸ save_dna_by_market_condition í˜¸ì¶œë¨ (deprecated): {coin}-{interval}")
    return True  # í…Œì´ë¸”ì´ ì—†ìœ¼ë¯€ë¡œ ì„±ê³µìœ¼ë¡œ ë°˜í™˜

def save_fractal_by_market_condition(coin: str, interval: str, market_condition: str, 
                                   fractal_features: Dict[str, Any]) -> bool:
    """âš ï¸ DEPRECATED: ë¯¸ì‚¬ìš© í…Œì´ë¸” (fractal_market_analysis) - í•¨ìˆ˜ëŠ” ìœ ì§€í•˜ë˜ ë™ì‘ ì•ˆí•¨"""
    logger.debug(f"âš ï¸ save_fractal_by_market_condition í˜¸ì¶œë¨ (deprecated): {coin}-{interval}")
    return True  # í…Œì´ë¸”ì´ ì—†ìœ¼ë¯€ë¡œ ì„±ê³µìœ¼ë¡œ ë°˜í™˜

def save_routing_by_market_condition(coin: str, routing_results: Dict[str, Any], 
                                   integrated_routing: Dict[str, Any]) -> bool:
    """âš ï¸ DEPRECATED: ë¯¸ì‚¬ìš© í…Œì´ë¸” (routing_market_analysis) - í•¨ìˆ˜ëŠ” ìœ ì§€í•˜ë˜ ë™ì‘ ì•ˆí•¨"""
    logger.debug(f"âš ï¸ save_routing_by_market_condition í˜¸ì¶œë¨ (deprecated): {coin}")
    return True  # í…Œì´ë¸”ì´ ì—†ìœ¼ë¯€ë¡œ ì„±ê³µìœ¼ë¡œ ë°˜í™˜

@contextmanager
def transaction(db_path: str = None):
    """íŠ¸ëœì­ì…˜ ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì €"""
    from rl_pipeline.db.connection_pool import get_candle_db_pool
    
    # db_pathì— ë”°ë¼ ì ì ˆí•œ í’€ ì„ íƒ
    if db_path and 'candles' in db_path.lower():
        pool = get_candle_db_pool()
    else:
        pool = get_strategy_db_pool()
    
    with pool.get_connection() as conn:
        try:
            yield conn
            conn.commit()
        except Exception as e:
            conn.rollback()
            raise DBWriteError(f"íŠ¸ëœì­ì…˜ ì‹¤íŒ¨: {e}") from e

def save_coin_analysis_ratios(coin: str, interval: str, analysis_type: str,
                             ratios_data: Dict[str, Any],
                             market_type: str = DEFAULT_MARKET_TYPE,
                             market: str = DEFAULT_MARKET) -> bool:
    """ë¶„ì„ ë¹„ìœ¨ì„ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ (coin â†’ symbol ë§¤í•‘)"""
    try:
        pool = get_strategy_db_pool()

        # coin â†’ symbol
        symbol = coin

        with pool.get_connection() as conn:
            cursor = conn.cursor()

            # ê¸°ì¡´ ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸
            check_query = """
            SELECT id FROM analysis_ratios
            WHERE market_type = ? AND market = ? AND symbol = ? AND interval = ? AND analysis_type = ?
            """
            cursor.execute(check_query, (market_type, market, symbol, interval, analysis_type))
            existing = cursor.fetchone()

            if existing:
                # ì—…ë°ì´íŠ¸
                update_query = """
                UPDATE analysis_ratios SET
                    fractal_ratios = ?,
                    multi_timeframe_ratios = ?,
                    indicator_cross_ratios = ?,
                    symbol_specific_ratios = ?,
                    volatility_ratios = ?,
                    volume_ratios = ?,
                    optimal_modules = ?,
                    interval_weights = ?,
                    performance_score = ?,
                    accuracy_score = ?,
                    updated_at = CURRENT_TIMESTAMP
                WHERE market_type = ? AND market = ? AND symbol = ? AND interval = ? AND analysis_type = ?
                """

                cursor.execute(update_query, (
                    safe_json_dumps(ratios_data.get('fractal_ratios', {})),
                    safe_json_dumps(ratios_data.get('multi_timeframe_ratios', {})),
                    safe_json_dumps(ratios_data.get('indicator_cross_ratios', {})),
                    safe_json_dumps(ratios_data.get('coin_specific_ratios', ratios_data.get('symbol_specific_ratios', {}))),
                    safe_json_dumps(ratios_data.get('volatility_ratios', {})),
                    safe_json_dumps(ratios_data.get('volume_ratios', {})),
                    safe_json_dumps(ratios_data.get('optimal_modules', {})),
                    safe_json_dumps(ratios_data.get('interval_weights', {})),
                    ratios_data.get('performance_score', 0.0),
                    ratios_data.get('accuracy_score', 0.0),
                    market_type, market, symbol, interval, analysis_type
                ))
            else:
                # ìƒˆë¡œ ì‚½ì…
                insert_query = """
                INSERT INTO analysis_ratios (
                    market_type, market, symbol, interval, analysis_type,
                    fractal_ratios, multi_timeframe_ratios, indicator_cross_ratios,
                    symbol_specific_ratios, volatility_ratios, volume_ratios,
                    optimal_modules, interval_weights, performance_score, accuracy_score
                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                """

                cursor.execute(insert_query, (
                    market_type, market, symbol, interval, analysis_type,
                    safe_json_dumps(ratios_data.get('fractal_ratios', {})),
                    safe_json_dumps(ratios_data.get('multi_timeframe_ratios', {})),
                    safe_json_dumps(ratios_data.get('indicator_cross_ratios', {})),
                    safe_json_dumps(ratios_data.get('coin_specific_ratios', ratios_data.get('symbol_specific_ratios', {}))),
                    safe_json_dumps(ratios_data.get('volatility_ratios', {})),
                    safe_json_dumps(ratios_data.get('volume_ratios', {})),
                    safe_json_dumps(ratios_data.get('optimal_modules', {})),
                    safe_json_dumps(ratios_data.get('interval_weights', {})),
                    ratios_data.get('performance_score', 0.0),
                    ratios_data.get('accuracy_score', 0.0)
                ))

            conn.commit()
            logger.info(f"âœ… {symbol} {interval} ë¶„ì„ ë¹„ìœ¨ ì €ì¥ ì™„ë£Œ")
            return True

    except Exception as e:
        logger.error(f"âŒ {coin} {interval} ë¶„ì„ ë¹„ìœ¨ ì €ì¥ ì‹¤íŒ¨: {e}")
        return False

def save_coin_global_weights(coin: str, weights_data: Dict[str, Any],
                            market_type: str = DEFAULT_MARKET_TYPE,
                            market: str = DEFAULT_MARKET,
                            db_path: str = None) -> bool:
    """ì‹¬ë³¼ vs ê¸€ë¡œë²Œ ì „ëµ ê°€ì¤‘ì¹˜ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ (coin â†’ symbol ë§¤í•‘)

    Args:
        coin: ì‹¬ë³¼ ì´ë¦„ (ì˜ˆ: 'BTC') - v1 í˜¸í™˜ì„± ìœ ì§€
        weights_data: ê°€ì¤‘ì¹˜ ë°ì´í„°
            - coin_weight/symbol_weight: ê°œë³„ ì‹¬ë³¼ ì „ëµ ê°€ì¤‘ì¹˜ (0~1)
            - global_weight: ê¸€ë¡œë²Œ ì „ëµ ê°€ì¤‘ì¹˜ (0~1)
            - coin_score/symbol_score: ì‹¬ë³¼ ì „ëµ ì„±ëŠ¥ ì ìˆ˜
            - global_score: ê¸€ë¡œë²Œ ì „ëµ ì„±ëŠ¥ ì ìˆ˜
            - data_quality_score: ë°ì´í„° í’ˆì§ˆ ì ìˆ˜
            - coin_strategy_count/symbol_strategy_count: ì‹¬ë³¼ ì „ëµ ê°œìˆ˜
            - global_strategy_count: ê¸€ë¡œë²Œ ì „ëµ ê°œìˆ˜
            - coin_avg_profit/symbol_avg_profit: ì‹¬ë³¼ ì „ëµ í‰ê·  ìˆ˜ìµ
            - global_avg_profit: ê¸€ë¡œë²Œ ì „ëµ í‰ê·  ìˆ˜ìµ
            - coin_win_rate/symbol_win_rate: ì‹¬ë³¼ ì „ëµ ìŠ¹ë¥ 
            - global_win_rate: ê¸€ë¡œë²Œ ì „ëµ ìŠ¹ë¥ 
        market_type: ë§ˆì¼“ íƒ€ì… (ê¸°ë³¸: COIN)
        market: ë§ˆì¼“ (ê¸°ë³¸: BITHUMB)
        db_path: ì„ íƒì  DB ê²½ë¡œ (ì—†ìœ¼ë©´ ì „ëµ DB ì‚¬ìš©)

    Returns:
        bool: ì €ì¥ ì„±ê³µ ì—¬ë¶€
    """
    try:
        if db_path:
            pool = get_strategy_db_pool(db_path)
        else:
            pool = get_strategy_db_pool()

        # coin â†’ symbol
        symbol = coin

        with pool.get_connection() as conn:
            cursor = conn.cursor()

            # ê¸°ì¡´ ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸
            check_query = "SELECT symbol FROM symbol_global_weights WHERE market_type = ? AND market = ? AND symbol = ?"
            cursor.execute(check_query, (market_type, market, symbol))
            existing = cursor.fetchone()

            # í•„ë“œ í˜¸í™˜ì„± (coin_* â†’ symbol_*)
            symbol_weight = weights_data.get('symbol_weight', weights_data.get('coin_weight', 0.7))
            symbol_score = weights_data.get('symbol_score', weights_data.get('coin_score', 0.0))
            symbol_strategy_count = weights_data.get('symbol_strategy_count', weights_data.get('coin_strategy_count', 0))
            symbol_avg_profit = weights_data.get('symbol_avg_profit', weights_data.get('coin_avg_profit', 0.0))
            symbol_win_rate = weights_data.get('symbol_win_rate', weights_data.get('coin_win_rate', 0.0))

            if existing:
                # ì—…ë°ì´íŠ¸
                update_query = """
                UPDATE symbol_global_weights SET
                    symbol_weight = ?,
                    global_weight = ?,
                    symbol_score = ?,
                    global_score = ?,
                    data_quality_score = ?,
                    symbol_strategy_count = ?,
                    global_strategy_count = ?,
                    symbol_avg_profit = ?,
                    global_avg_profit = ?,
                    symbol_win_rate = ?,
                    global_win_rate = ?,
                    updated_at = CURRENT_TIMESTAMP
                WHERE market_type = ? AND market = ? AND symbol = ?
                """

                cursor.execute(update_query, (
                    symbol_weight,
                    weights_data.get('global_weight', 0.3),
                    symbol_score,
                    weights_data.get('global_score', 0.0),
                    weights_data.get('data_quality_score', 0.0),
                    symbol_strategy_count,
                    weights_data.get('global_strategy_count', 0),
                    symbol_avg_profit,
                    weights_data.get('global_avg_profit', 0.0),
                    symbol_win_rate,
                    weights_data.get('global_win_rate', 0.0),
                    market_type, market, symbol
                ))
            else:
                # ìƒˆë¡œ ì‚½ì…
                insert_query = """
                INSERT INTO symbol_global_weights (
                    market_type, market, symbol,
                    symbol_weight, global_weight,
                    symbol_score, global_score, data_quality_score,
                    symbol_strategy_count, global_strategy_count,
                    symbol_avg_profit, global_avg_profit,
                    symbol_win_rate, global_win_rate
                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                """

                cursor.execute(insert_query, (
                    market_type, market, symbol,
                    symbol_weight,
                    weights_data.get('global_weight', 0.3),
                    symbol_score,
                    weights_data.get('global_score', 0.0),
                    weights_data.get('data_quality_score', 0.0),
                    symbol_strategy_count,
                    weights_data.get('global_strategy_count', 0),
                    symbol_avg_profit,
                    weights_data.get('global_avg_profit', 0.0),
                    symbol_win_rate,
                    weights_data.get('global_win_rate', 0.0)
                ))

            conn.commit()
            logger.info(f"âœ… {symbol} ì‹¬ë³¼ vs ê¸€ë¡œë²Œ ê°€ì¤‘ì¹˜ ì €ì¥ ì™„ë£Œ (symbol: {symbol_weight:.2f}, global: {weights_data.get('global_weight', 0.3):.2f})")
            return True

    except Exception as e:
        logger.error(f"âŒ {symbol} ì‹¬ë³¼ vs ê¸€ë¡œë²Œ ê°€ì¤‘ì¹˜ ì €ì¥ ì‹¤íŒ¨: {e}")
        return False