"""
ë³´ìƒ ì—”ì§„ (Reward Engine)
ì˜ˆì¸¡ ì •í™•ë„ ê¸°ë°˜ ë³´ìƒ ê³„ì‚° ì‹œìŠ¤í…œ
"""

import numpy as np
import logging
from typing import Dict, Any, Optional
from dataclasses import dataclass

from rl_pipeline.engine.interval_profile import (
    get_interval_profile,
    get_risk_gamma,
    get_sigma_min
)

logger = logging.getLogger(__name__)

# ë™ì‹œ íˆíŠ¸ ì •ì±… ìƒìˆ˜
TIE_RULE = "SL_FIRST"  # TP/SL ë™ì‹œ ë°œìƒ ì‹œ SL ìš°ì„  (ë³´ìˆ˜ì  ì •ì±…)

# ê¸°ë³¸ ë³´ìƒ ê°€ì¤‘ì¹˜
DEFAULT_WEIGHTS = {
    'dir': 0.35,      # ë°©í–¥ ì •í™•ë„ (ê°€ì¥ ì¤‘ìš”)
    'price': 0.25,    # ëª©í‘œ ë‹¬ì„± (ê·¼ì ‘ë„)
    'time': 0.15,     # ì‹œê°„ ì •í™•ë„
    'trade': 0.15,    # ê±°ë˜ ì„±ê³¼
    'calib': 0.10     # ìº˜ë¦¬ë¸Œë ˆì´ì…˜
}

# ì‹œê°„ ê°ì‡  ëŒë‹¤ (ê¸°ë³¸ê°’)
DEFAULT_LAMBDA = 0.7


@dataclass
class RewardComponents:
    """ë³´ìƒ êµ¬ì„± ìš”ì†Œ"""
    reward_dir: float = 0.0      # ë°©í–¥ ì •í™•ë„ ë³´ìƒ
    reward_price: float = 0.0    # ëª©í‘œ ë‹¬ì„± ë³´ìƒ
    reward_time: float = 0.0     # ì‹œê°„ ì •í™•ë„ ë³´ìƒ
    reward_trade: float = 0.0    # ê±°ë˜ ì„±ê³¼ ë³´ìƒ
    reward_calib: float = 0.0    # ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ë³´ìƒ
    reward_risk: float = 0.0     # ë¦¬ìŠ¤í¬ í˜ë„í‹°
    reward_total: float = 0.0    # ì´ ë³´ìƒ


class RewardEngine:
    """ë³´ìƒ ì—”ì§„ - ì˜ˆì¸¡ ì •í™•ë„ ê¸°ë°˜ ë³´ìƒ ê³„ì‚°"""
    
    def __init__(
        self,
        weights: Optional[Dict[str, float]] = None,
        lambda_time: float = DEFAULT_LAMBDA
    ):
        """
        ì´ˆê¸°í™”
        
        Args:
            weights: ë³´ìƒ ê°€ì¤‘ì¹˜ (Noneì´ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©)
            lambda_time: ì‹œê°„ ê°ì‡  ëŒë‹¤ ê°’
        """
        self.weights = weights or DEFAULT_WEIGHTS.copy()
        self.lambda_time = lambda_time
        
        logger.info(f"âœ… Reward Engine ì´ˆê¸°í™” ì™„ë£Œ (lambda={lambda_time})")
    
    def compute_reward(
        self,
        predicted_dir: int,
        predicted_target: float,
        predicted_horizon: int,
        actual_dir: int,
        actual_move_pct: float,
        actual_horizon: int,
        first_event: str,
        dd_pct_norm: float = 0.0,
        interval: str = "15m",
        sigma: Optional[float] = None,
        atr_pct: Optional[float] = None,
        tier_reward_weight: float = 1.0  # ğŸ”¥ 3ë‹¨ê³„ ë³´ìƒ ê°€ì¤‘ì¹˜ (ê¸°ë³¸ê°’ 1.0)
    ) -> RewardComponents:
        """
        ë³´ìƒ ê³„ì‚°
        
        Args:
            predicted_dir: ì˜ˆì¸¡ ë°©í–¥ (+1/-1/0)
            predicted_target: ì˜ˆì¸¡ ëª©í‘œ ë³€ë™ë¥ 
            predicted_horizon: ì˜ˆì¸¡ ëª©í‘œ ìº”ë“¤ ìˆ˜
            actual_dir: ì‹¤ì œ ë°©í–¥ (+1/-1/0)
            actual_move_pct: ì‹¤ì œ ë³€ë™ë¥ 
            actual_horizon: ì‹¤ì œ ë„ë‹¬ ìº”ë“¤ ìˆ˜
            first_event: ì²« ì´ë²¤íŠ¸ ('TP', 'SL', 'expiry')
            dd_pct_norm: ì •ê·œí™”ëœ ë“œë¡œìš°ë‹¤ìš´
            interval: ì¸í„°ë²Œ
            sigma: ê·¼ì ‘ë„ ê³„ì‚°ìš© ì‹œê·¸ë§ˆ (Noneì´ë©´ interval_profileì—ì„œ ì¡°íšŒ)
            atr_pct: ATR ë¹„ìœ¨ (Noneì´ë©´ sigmaë§Œ ì‚¬ìš©)
        
        Returns:
            RewardComponents ê°ì²´
        """
        try:
            # 1. ë°©í–¥ ì •í™•ë„ ë³´ìƒ
            R_dir = self._compute_direction_reward(
                predicted_dir, actual_dir, predicted_target, actual_move_pct
            )
            
            # 2. ëª©í‘œ ë‹¬ì„± ë³´ìƒ (ê·¼ì ‘ë„)
            if sigma is None:
                sigma = self._get_sigma(interval, atr_pct)
            R_price = self._compute_price_reward(
                predicted_target, actual_move_pct, sigma
            )
            
            # 3. ì‹œê°„ ì •í™•ë„ ë³´ìƒ
            R_time = self._compute_time_reward(
                predicted_horizon, actual_horizon
            )
            
            # 4. ê±°ë˜ ì„±ê³¼ ë³´ìƒ
            R_trade = self._compute_trade_reward(
                first_event, actual_move_pct, predicted_dir
            )
            
            # 5. ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ë³´ìƒ
            R_calib = self._compute_calibration_reward(
                predicted_dir, actual_dir, actual_move_pct
            )
            
            # 6. ë¦¬ìŠ¤í¬ í˜ë„í‹°
            risk_gamma = get_risk_gamma(interval)
            R_risk = -risk_gamma * dd_pct_norm
            
            # 7. ì´ ë³´ìƒ ê³„ì‚°
            reward_total = (
                self.weights['dir'] * R_dir +
                self.weights['price'] * R_price +
                self.weights['time'] * R_time +
                self.weights['trade'] * R_trade +
                self.weights['calib'] * R_calib +
                R_risk
            )
            
            # ğŸ”¥ 3ë‹¨ê³„ ë³´ìƒ ê°€ì¤‘ì¹˜ ì ìš©
            reward_total = reward_total * tier_reward_weight
            
            # ğŸ”¥ ì¶”ê°€ ê°œì„ : Profit Factor ê¸°ë°˜ ë³´ìƒ ë³´ë„ˆìŠ¤ (ì¬í•™ìŠµ ê¶Œì¥ ë°˜ì˜)
            # ì‹¤ì œ ê±°ë˜ ì„±ê³¼ê°€ ì¢‹ì„ ë•Œ ì¶”ê°€ ë³´ìƒ ë¶€ì—¬
            if first_event == 'TP':
                # TP ë„ë‹¬ ì‹œ PF ê¸°ë°˜ ë³´ë„ˆìŠ¤ (PF > 1.0ì´ë©´ ë³´ë„ˆìŠ¤, < 1.0ì´ë©´ í˜ë„í‹°)
                # ì‹¤ì œ ê±°ë˜ì—ì„œ PFê°€ ë†’ì„ìˆ˜ë¡ ë” ë§ì€ ë³´ìƒ
                pf_bonus = min(0.3, actual_move_pct * 10) if actual_move_pct > 0 else 0.0
                reward_total += pf_bonus
            elif first_event == 'SL':
                # SL ë„ë‹¬ ì‹œ PF ê¸°ë°˜ í˜ë„í‹° (ë” í° ì†ì‹¤)
                pf_penalty = max(-0.3, actual_move_pct * 10) if actual_move_pct < 0 else 0.0
                reward_total += pf_penalty
            
            return RewardComponents(
                reward_dir=R_dir,
                reward_price=R_price,
                reward_time=R_time,
                reward_trade=R_trade,
                reward_calib=R_calib,
                reward_risk=R_risk,
                reward_total=reward_total
            )
            
        except Exception as e:
            logger.error(f"âŒ ë³´ìƒ ê³„ì‚° ì‹¤íŒ¨: {e}")
            # ì—ëŸ¬ ì‹œ ê¸°ë³¸ê°’ ë°˜í™˜
            return RewardComponents()
    
    def _compute_direction_reward(
        self,
        predicted_dir: int,
        actual_dir: int,
        predicted_target: float,
        actual_move_pct: float
    ) -> float:
        """
        ë°©í–¥ ì •í™•ë„ ë³´ìƒ
        
        Returns:
            0.0 ~ 1.0 (ì •í™•í• ìˆ˜ë¡ ë†’ìŒ)
        """
        # ë°©í–¥ ì¼ì¹˜ ì²´í¬
        if predicted_dir == +1:  # ìƒìŠ¹ ì˜ˆì¸¡
            if actual_move_pct > 0:
                return 1.0
            else:
                return 0.0
        elif predicted_dir == -1:  # í•˜ë½ ì˜ˆì¸¡
            if actual_move_pct < 0:
                return 1.0
            else:
                return 0.0
        else:  # predicted_dir == 0 (íš¡ë³´ ì˜ˆì¸¡)
            # íš¡ë³´ ì˜ˆì¸¡ì€ ì‘ì€ ë³€ë™ë¥ ì¼ ë•Œ ë³´ìƒ
            threshold = abs(predicted_target) * 0.5
            if abs(actual_move_pct) < threshold:
                return 0.5  # ë¶€ë¶„ ë³´ìƒ
            else:
                return 0.0
    
    def _compute_price_reward(
        self,
        predicted_target: float,
        actual_move_pct: float,
        sigma: float
    ) -> float:
        """
        ëª©í‘œ ë‹¬ì„± ë³´ìƒ (ê·¼ì ‘ë„)
        
        ê°€ìš°ì‹œì•ˆ ê·¼ì ‘ë„: exp(-(error/Ïƒ)Â²)
        
        Args:
            predicted_target: ì˜ˆì¸¡ ëª©í‘œ ë³€ë™ë¥ 
            actual_move_pct: ì‹¤ì œ ë³€ë™ë¥ 
            sigma: ê·¼ì ‘ë„ ê³„ì‚°ìš© ì‹œê·¸ë§ˆ
        
        Returns:
            0.0 ~ 1.0 (ê°€ê¹Œìš¸ìˆ˜ë¡ ë†’ìŒ)
        """
        error = abs(actual_move_pct - predicted_target)
        
        # ê°€ìš°ì‹œì•ˆ ê·¼ì ‘ë„
        try:
            prox = np.exp(-(error / sigma) ** 2)
            return float(prox)
        except (OverflowError, ZeroDivisionError):
            # ì—ëŸ¬ ì²˜ë¦¬
            if error == 0:
                return 1.0
            else:
                return 0.0
    
    def _compute_time_reward(
        self,
        predicted_horizon: int,
        actual_horizon: int
    ) -> float:
        """
        ì‹œê°„ ì •í™•ë„ ë³´ìƒ
        
        time_bonus = exp(-Î» Ã— time_error)
        time_error = |t_hit - horizon_k| / horizon_k
        
        Args:
            predicted_horizon: ì˜ˆì¸¡ ëª©í‘œ ìº”ë“¤ ìˆ˜
            actual_horizon: ì‹¤ì œ ë„ë‹¬ ìº”ë“¤ ìˆ˜
        
        Returns:
            0.0 ~ 1.0 (ì •í™•í• ìˆ˜ë¡ ë†’ìŒ)
        """
        if predicted_horizon <= 0:
            return 0.0
        
        time_error = abs(actual_horizon - predicted_horizon) / predicted_horizon
        
        try:
            time_bonus = np.exp(-self.lambda_time * time_error)
            return float(time_bonus)
        except (OverflowError, ZeroDivisionError):
            if actual_horizon == predicted_horizon:
                return 1.0
            else:
                return 0.0
    
    def _compute_trade_reward(
        self,
        first_event: str,
        actual_move_pct: float,
        predicted_dir: int
    ) -> float:
        """
        ê±°ë˜ ì„±ê³¼ ë³´ìƒ
        
        Args:
            first_event: ì²« ì´ë²¤íŠ¸ ('TP', 'SL', 'expiry')
            actual_move_pct: ì‹¤ì œ ë³€ë™ë¥ 
            predicted_dir: ì˜ˆì¸¡ ë°©í–¥
        
        Returns:
            0.0 ~ 1.0 (ë°˜ëŒ€ ë°©í–¥ ì‹œ ìŒìˆ˜ ê°€ëŠ¥)
        """
        if first_event == 'TP':
            # TP ë„ë‹¬ = ì„±ê³µ
            return 1.0
        elif first_event == 'SL':
            # SL ë„ë‹¬ = ì‹¤íŒ¨ (ë™ì‹œ íˆíŠ¸ ì‹œ SL ìš°ì„ : TIE_RULE)
            return 0.0
        elif first_event == 'expiry':
            # ğŸ”¥ ë§Œë£Œ ì‹œ ê°œì„ : ê°€ê²© ë³€í™”ê°€ 0%ì—¬ë„ í•™ìŠµ ê°€ëŠ¥í•˜ë„ë¡
            # ì‹¤ì œ ë³€ë™ë¥ ì´ ë§¤ìš° ì‘ì€ ê²½ìš°(0.001% ì´í•˜)ë„ ì²˜ë¦¬
            min_move_threshold = 0.00001  # 0.001% ë¯¸ë§Œì€ 0ìœ¼ë¡œ ê°„ì£¼
            
            if predicted_dir == +1 and actual_move_pct > min_move_threshold:
                # ìƒìŠ¹ ì˜ˆì¸¡, ì‹¤ì œ ìƒìŠ¹ (ë¶€ë¶„ ì„±ê³µ) - ë³€ë™ë¥  í¬ê¸°ì— ë”°ë¼ ë³´ìƒ ì°¨ë“±
                move_magnitude = min(abs(actual_move_pct) * 100, 1.0)  # 1%ê¹Œì§€ ì •ê·œí™”
                return 0.3 + (move_magnitude * 0.2)  # 0.3 ~ 0.5 (ë³€ë™ë¥ ì´ í´ìˆ˜ë¡ ë†’ì€ ë³´ìƒ)
            elif predicted_dir == -1 and actual_move_pct < -min_move_threshold:
                # í•˜ë½ ì˜ˆì¸¡, ì‹¤ì œ í•˜ë½ (ë¶€ë¶„ ì„±ê³µ)
                move_magnitude = min(abs(actual_move_pct) * 100, 1.0)
                return 0.3 + (move_magnitude * 0.2)  # 0.3 ~ 0.5
            elif predicted_dir == +1 and actual_move_pct < -min_move_threshold:
                # ğŸ”¥ ìƒìŠ¹ ì˜ˆì¸¡í–ˆëŠ”ë° ì‹¤ì œ í•˜ë½ (ëª…í™•í•œ ì‹¤íŒ¨, í˜ë„í‹°)
                opposite_move = abs(actual_move_pct)
                penalty = -min(opposite_move * 20, 0.5)  # ë°˜ëŒ€ ë°©í–¥ í˜ë„í‹°
                return penalty
            elif predicted_dir == -1 and actual_move_pct > min_move_threshold:
                # ğŸ”¥ í•˜ë½ ì˜ˆì¸¡í–ˆëŠ”ë° ì‹¤ì œ ìƒìŠ¹ (ëª…í™•í•œ ì‹¤íŒ¨, í˜ë„í‹°)
                opposite_move = abs(actual_move_pct)
                penalty = -min(opposite_move * 20, 0.5)
                return penalty
            elif abs(actual_move_pct) <= min_move_threshold:
                # ğŸ”¥ íš¡ë³´ (ê°€ê²© ë³€í™” ê±°ì˜ ì—†ìŒ) - ë°©í–¥ì„± ì˜ˆì¸¡ì— ë¶€ë¶„ ì ìˆ˜ ë¶€ì—¬
                # ë³€ë™ì„±ì´ ë‚®ì€ ì‹œì¥ì—ì„œë„ í•™ìŠµ ê°€ëŠ¥í•˜ë„ë¡ ìµœì†Œ ë³´ìƒ ì œê³µ
                if predicted_dir != 0:  # ë°©í–¥ ì˜ˆì¸¡ì´ ìˆì—ˆìœ¼ë©´
                    # ë°©í–¥ì´ ë§ë‹¤ë©´(ì‹œì¥ì´ ì›€ì§ì´ì§€ ì•Šì•˜ì§€ë§Œ ì˜ˆì¸¡ ë°©í–¥ì´ ë§ì•˜ë‹¤ë©´) ì‘ì€ ë³´ìƒ
                    # ì‹¤ì œë¡œëŠ” ì›€ì§ì´ì§€ ì•Šì•˜ìœ¼ë¯€ë¡œ ë§¤ìš° ì‘ì€ ë³´ìƒ (0.1)
                    return 0.1
                else:
                    # ì¤‘ë¦½ ì˜ˆì¸¡ì´ì—ˆê³  ì‹¤ì œë¡œ íš¡ë³´ì˜€ìœ¼ë©´ ì¤‘ê°„ ë³´ìƒ
                    return 0.25
            else:
                # ê¸°íƒ€ (ë§¤ìš° ì‘ì€ ë³€ë™)
                return 0.05  # ìµœì†Œ ë³´ìƒ (í•™ìŠµ ê°€ëŠ¥í•˜ë„ë¡)
        else:
            # ê¸°íƒ€ ì´ë²¤íŠ¸
            return 0.0
    
    def _compute_calibration_reward(
        self,
        predicted_dir: int,
        actual_dir: int,
        actual_move_pct: float
    ) -> float:
        """
        ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ë³´ìƒ
        
        ì˜ˆì¸¡ í™•ì‹ ë„ì™€ ì‹¤ì œ ê²°ê³¼ì˜ ì¼ì¹˜ë„
        
        Args:
            predicted_dir: ì˜ˆì¸¡ ë°©í–¥
            actual_dir: ì‹¤ì œ ë°©í–¥
            actual_move_pct: ì‹¤ì œ ë³€ë™ë¥ 
        
        Returns:
            0.0 ~ 1.0
        """
        # ë°©í–¥ ì¼ì¹˜ë„
        dir_match = 1.0 if predicted_dir == actual_dir else 0.0
        
        # ë³€ë™ë¥  í¬ê¸° ê³ ë ¤ (í° ë³€ë™ì¼ìˆ˜ë¡ ë†’ì€ ë³´ìƒ)
        magnitude = min(abs(actual_move_pct) * 10, 1.0)  # 0.1% = 1.0
        
        return (dir_match + magnitude) / 2.0
    
    def _get_sigma(self, interval: str, atr_pct: Optional[float]) -> float:
        """
        ê·¼ì ‘ë„ ê³„ì‚°ìš© ì‹œê·¸ë§ˆ ì¡°íšŒ
        
        sigma_minê³¼ ATR% ì¤‘ í° ê°’ ì‚¬ìš©
        """
        sigma_min = get_sigma_min(interval)
        
        if atr_pct is not None:
            return max(sigma_min, atr_pct)
        else:
            return sigma_min
    
    def compute_predictive_accuracy_flag(
        self,
        first_event: str,
        predicted_dir: int,
        actual_move_pct: float
    ) -> int:
        """
        ì˜ˆì¸¡ ì •í™•ë„ í”Œë˜ê·¸ ê³„ì‚°
        
        Returns:
            1 (ì •í™•) / 0 (ë¶€ì •í™•)
        """
        if first_event == 'TP':
            # TP ë„ë‹¬ = ì˜ˆì¸¡ ì„±ê³µ (ìƒìŠ¹/í•˜ë½/ì¤‘ë¦½ ëª¨ë‘)
            return 1
        elif first_event == 'SL':
            # SL ë„ë‹¬ = ì˜ˆì¸¡ ì‹¤íŒ¨
            return 0
        elif first_event == 'expiry':
            # ğŸ”¥ ë§Œë£Œ ì‹œ ê°œì„ : ê°€ê²© ë³€í™”ê°€ 0%ì—¬ë„ í•™ìŠµ ê°€ëŠ¥í•˜ë„ë¡
            min_move_threshold = 0.00001  # 0.001% ë¯¸ë§Œì€ 0ìœ¼ë¡œ ê°„ì£¼
            
            if predicted_dir == +1:
                # ìƒìŠ¹ ì˜ˆì¸¡: ì‹¤ì œ ë³€ë™ë¥ ì´ ì–‘ìˆ˜ë©´ ì„±ê³µ
                if actual_move_pct > min_move_threshold:
                    return 1  # ìƒìŠ¹ ì„±ê³µ
                elif actual_move_pct < -min_move_threshold:
                    return 0  # í•˜ë½ ì‹¤íŒ¨
                else:
                    # íš¡ë³´ (ê±°ì˜ ë³€í™” ì—†ìŒ) - ë°©í–¥ì„± ì˜ˆì¸¡ì— ë¶€ë¶„ ì ìˆ˜
                    # ë³€ë™ì„±ì´ ë‚®ì€ ì‹œì¥ì—ì„œë„ í•™ìŠµ ê°€ëŠ¥í•˜ë„ë¡
                    return 0  # 0% ë³€í™”ëŠ” ì •í™•ë„ 0 (í•˜ì§€ë§Œ ë³´ìƒì€ 0.1 ì œê³µ)
            elif predicted_dir == -1:
                # í•˜ë½ ì˜ˆì¸¡: ì‹¤ì œ ë³€ë™ë¥ ì´ ìŒìˆ˜ë©´ ì„±ê³µ
                if actual_move_pct < -min_move_threshold:
                    return 1  # í•˜ë½ ì„±ê³µ
                elif actual_move_pct > min_move_threshold:
                    return 0  # ìƒìŠ¹ ì‹¤íŒ¨
                else:
                    # íš¡ë³´
                    return 0  # 0% ë³€í™”ëŠ” ì •í™•ë„ 0 (í•˜ì§€ë§Œ ë³´ìƒì€ 0.1 ì œê³µ)
            elif predicted_dir == 0:
                # ğŸ”¥ ì¤‘ë¦½ ì˜ˆì¸¡: ì‘ì€ ë³€ë™ ë²”ìœ„ ë‚´ì— ìˆìœ¼ë©´ ì„±ê³µ
                # ì¤‘ë¦½ ë²”ìœ„: Â±0.5% ì´ë‚´ (ì¼ë°˜ì ì¸ ë…¸ì´ì¦ˆ ë²”ìœ„)
                neutral_threshold = 0.005  # 0.5%
                return 1 if abs(actual_move_pct) <= neutral_threshold else 0
            else:
                return 0
        else:
            return 0


# í¸ì˜ í•¨ìˆ˜
def compute_reward(
    predicted_dir: int,
    predicted_target: float,
    predicted_horizon: int,
    actual_dir: int,
    actual_move_pct: float,
    actual_horizon: int,
    first_event: str,
    interval: str = "15m",
    **kwargs
) -> RewardComponents:
    """ë³´ìƒ ê³„ì‚° í¸ì˜ í•¨ìˆ˜"""
    engine = RewardEngine()
    return engine.compute_reward(
        predicted_dir=predicted_dir,
        predicted_target=predicted_target,
        predicted_horizon=predicted_horizon,
        actual_dir=actual_dir,
        actual_move_pct=actual_move_pct,
        actual_horizon=actual_horizon,
        first_event=first_event,
        interval=interval,
        **kwargs
    )


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸
    engine = RewardEngine()
    
    # í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 1: TP ë„ë‹¬ (ì™„ì „ ì„±ê³µ)
    reward1 = engine.compute_reward(
        predicted_dir=+1,
        predicted_target=0.015,
        predicted_horizon=8,
        actual_dir=+1,
        actual_move_pct=0.015,
        actual_horizon=5,
        first_event='TP',
        interval='15m'
    )
    print(f"í…ŒìŠ¤íŠ¸ 1 (TP ë„ë‹¬): reward_total={reward1.reward_total:.3f}")
    
    # í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 2: SL ë„ë‹¬ (ì‹¤íŒ¨)
    reward2 = engine.compute_reward(
        predicted_dir=+1,
        predicted_target=0.015,
        predicted_horizon=8,
        actual_dir=-1,
        actual_move_pct=-0.02,
        actual_horizon=3,
        first_event='SL',
        interval='15m'
    )
    print(f"í…ŒìŠ¤íŠ¸ 2 (SL ë„ë‹¬): reward_total={reward2.reward_total:.3f}")

