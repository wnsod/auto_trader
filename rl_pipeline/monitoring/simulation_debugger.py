"""
Simulation ëª¨ë“ˆ ì „ìš© ë””ë²„ê·¸ ë¡œê±°
- Self-play ì—í”¼ì†Œë“œ ì¶”ì 
- ê±°ë˜ í–‰ë™ ìƒì„¸ ë¶„ì„
- ë³´ìƒ ë¶„í¬ ì¶”ì 
- ì—ì´ì „íŠ¸ ì„±ëŠ¥ ë¹„êµ
"""

from typing import Dict, List, Any, Optional
import numpy as np
from .debug_logger import DebugLogger


class SimulationDebugger(DebugLogger):
    """Simulation ëª¨ë“ˆ ì „ìš© ë””ë²„ê±°"""

    def __init__(self, session_id: str = None):
        super().__init__("simulation", session_id)

        # ì‹œë®¬ë ˆì´ì…˜ í†µê³„
        self.simulation_stats = {
            "total_episodes": 0,
            "total_trades": 0,
            "total_pnl": 0.0,
            "winning_trades": 0,
            "losing_trades": 0,
            "no_trade_episodes": 0
        }

    def log_selfplay_start(
        self,
        coin: str,
        interval: str,
        num_episodes: int,
        num_agents: int,
        candle_count: int,
        config: Dict[str, Any] = None
    ):
        """
        Self-play ì‹œì‘ ë¡œê¹…

        Args:
            coin: ì½”ì¸ ì‹¬ë³¼
            interval: ì¸í„°ë²Œ
            num_episodes: ì—í”¼ì†Œë“œ ìˆ˜
            num_agents: ì—ì´ì „íŠ¸ ìˆ˜
            candle_count: ìº”ë“¤ ë°ì´í„° ê°œìˆ˜
            config: ì‹œë®¬ë ˆì´ì…˜ ì„¤ì •
        """
        self.log({
            "event": "selfplay_start",
            "coin": coin,
            "interval": interval,
            "num_episodes": num_episodes,
            "num_agents": num_agents,
            "candle_count": candle_count,
            "config": config or {},
            "message": f"ğŸš€ Self-play ì‹œì‘: {coin}-{interval}"
        })

    def log_episode_start(
        self,
        episode: int,
        total_episodes: int,
        regime: str,
        initial_balance: float,
        strategies: List[Dict[str, Any]]
    ):
        """
        ì—í”¼ì†Œë“œ ì‹œì‘ ë¡œê¹…

        Args:
            episode: ì—í”¼ì†Œë“œ ë²ˆí˜¸
            total_episodes: ì „ì²´ ì—í”¼ì†Œë“œ ìˆ˜
            regime: í˜„ì¬ ë ˆì§
            initial_balance: ì´ˆê¸° ì”ì•¡
            strategies: ì‚¬ìš©í•  ì „ëµ ë¦¬ìŠ¤íŠ¸
        """
        self.simulation_stats["total_episodes"] += 1

        self.log({
            "event": "episode_start",
            "episode": episode,
            "total_episodes": total_episodes,
            "progress": f"{episode}/{total_episodes}",
            "regime": regime,
            "initial_balance": float(initial_balance),
            "num_strategies": len(strategies),
            "strategy_grades": [s.get("grade", "UNKNOWN") for s in strategies]
        })

    def log_agent_step(
        self,
        episode: int,
        step: int,
        agent_id: str,
        state: np.ndarray,
        action: int,
        action_probs: np.ndarray,
        reward: float,
        balance: float,
        position: Optional[Dict[str, Any]] = None,
        market_info: Optional[Dict[str, Any]] = None
    ):
        """
        ì—ì´ì „íŠ¸ ìŠ¤í… ìƒì„¸ ë¡œê¹…

        Args:
            episode: ì—í”¼ì†Œë“œ ë²ˆí˜¸
            step: ìŠ¤í… ë²ˆí˜¸
            agent_id: ì—ì´ì „íŠ¸ ID
            state: ìƒíƒœ ë²¡í„°
            action: ì„ íƒí•œ ì•¡ì…˜ (0=HOLD, 1=BUY, 2=SELL)
            action_probs: ì•¡ì…˜ í™•ë¥  ë¶„í¬
            reward: ë°›ì€ ë³´ìƒ
            balance: í˜„ì¬ ì”ì•¡
            position: í¬ì§€ì…˜ ì •ë³´ (ì˜µì…˜)
            market_info: ì‹œì¥ ì •ë³´ (ì˜µì…˜)
        """
        action_names = ["HOLD", "BUY", "SELL"]

        log_entry = {
            "event": "agent_step",
            "episode": episode,
            "step": step,
            "agent_id": agent_id,

            # ì•¡ì…˜ ì •ë³´
            "action": {
                "index": int(action),
                "name": action_names[action] if action < len(action_names) else f"action_{action}",
                "probabilities": {
                    action_names[i]: float(action_probs[i])
                    for i in range(min(len(action_names), len(action_probs)))
                },
                "confidence": float(action_probs[action]) if action < len(action_probs) else 0.0
            },

            # ìƒíƒœ ì •ë³´
            "state": {
                "dim": len(state),
                "values": state.tolist() if len(state) <= 20 else None,  # 20ì°¨ì› ì´í•˜ë§Œ ì „ì²´ ì¶œë ¥
                "mean": float(np.mean(state)),
                "std": float(np.std(state)),
                "min": float(np.min(state)),
                "max": float(np.max(state))
            },

            # ë³´ìƒ & ì”ì•¡
            "reward": float(reward),
            "balance": float(balance)
        }

        # í¬ì§€ì…˜ ì •ë³´
        if position:
            log_entry["position"] = {
                "has_position": position.get("has_position", False),
                "entry_price": float(position.get("entry_price", 0)),
                "quantity": float(position.get("quantity", 0)),
                "pnl": float(position.get("pnl", 0))
            }

        # ì‹œì¥ ì •ë³´
        if market_info:
            log_entry["market"] = {
                "price": float(market_info.get("price", 0)),
                "volume": float(market_info.get("volume", 0)),
                "rsi": float(market_info.get("rsi", 50)),
                "macd": float(market_info.get("macd", 0))
            }

        self.log(log_entry, level="DEBUG")  # ë„ˆë¬´ ë§ìœ¼ë¯€ë¡œ DEBUG ë ˆë²¨

    def log_trade_execution(
        self,
        episode: int,
        step: int,
        agent_id: str,
        trade_type: str,
        price: float,
        quantity: float,
        fee: float,
        balance_before: float,
        balance_after: float,
        reason: str = None
    ):
        """
        ê±°ë˜ ì²´ê²° ë¡œê¹… (ì¤‘ìš”!)

        Args:
            episode: ì—í”¼ì†Œë“œ ë²ˆí˜¸
            step: ìŠ¤í… ë²ˆí˜¸
            agent_id: ì—ì´ì „íŠ¸ ID
            trade_type: ê±°ë˜ ìœ í˜• (BUY, SELL)
            price: ì²´ê²° ê°€ê²©
            quantity: ê±°ë˜ëŸ‰
            fee: ìˆ˜ìˆ˜ë£Œ
            balance_before: ê±°ë˜ ì „ ì”ì•¡
            balance_after: ê±°ë˜ í›„ ì”ì•¡
            reason: ê±°ë˜ ì´ìœ  (ì˜µì…˜)
        """
        self.simulation_stats["total_trades"] += 1

        self.log({
            "event": "trade_execution",
            "episode": episode,
            "step": step,
            "agent_id": agent_id,
            "trade": {
                "type": trade_type,
                "price": float(price),
                "quantity": float(quantity),
                "fee": float(fee),
                "cost": float(price * quantity + fee)
            },
            "balance": {
                "before": float(balance_before),
                "after": float(balance_after),
                "change": float(balance_after - balance_before)
            },
            "reason": reason,
            "message": f"ğŸ’° {trade_type}: {quantity:.4f} @ {price:.2f}"
        })

    def log_episode_end(
        self,
        episode: int,
        agent_results: Dict[str, Dict[str, Any]],
        regime: str,
        total_steps: int
    ):
        """
        ì—í”¼ì†Œë“œ ì¢…ë£Œ ë¡œê¹… (í•µì‹¬!)

        Args:
            episode: ì—í”¼ì†Œë“œ ë²ˆí˜¸
            agent_results: ì—ì´ì „íŠ¸ë³„ ê²°ê³¼
                {
                    "agent_1": {
                        "total_trades": 3,
                        "win_rate": 0.66,
                        "total_pnl": 120.5,
                        "final_balance": 10120.5,
                        "sharpe_ratio": 0.5,
                        ...
                    }
                }
            regime: ë ˆì§
            total_steps: ì´ ìŠ¤í… ìˆ˜
        """
        # í†µê³„ ì—…ë°ì´íŠ¸
        for agent_id, result in agent_results.items():
            pnl = result.get("total_pnl", 0)
            self.simulation_stats["total_pnl"] += pnl

            trades = result.get("total_trades", 0)
            if trades == 0:
                self.simulation_stats["no_trade_episodes"] += 1
            else:
                win_rate = result.get("win_rate", 0)
                self.simulation_stats["winning_trades"] += int(trades * win_rate)
                self.simulation_stats["losing_trades"] += int(trades * (1 - win_rate))

        # ì—ì´ì „íŠ¸ë³„ ì„±ê³¼ ë¹„êµ
        pnls = [r.get("total_pnl", 0) for r in agent_results.values()]
        win_rates = [r.get("win_rate", 0) for r in agent_results.values()]
        sharpes = [r.get("sharpe_ratio", 0) for r in agent_results.values()]

        # ìµœê³  ì„±ê³¼ ì—ì´ì „íŠ¸
        best_agent_id = max(agent_results.keys(), key=lambda k: agent_results[k].get("total_pnl", 0))
        best_agent_result = agent_results[best_agent_id]

        self.log({
            "event": "episode_end",
            "episode": episode,
            "regime": regime,
            "total_steps": total_steps,

            # ì „ì²´ í†µê³„
            "summary": {
                "num_agents": len(agent_results),
                "avg_pnl": float(np.mean(pnls)),
                "avg_win_rate": float(np.mean(win_rates)),
                "avg_sharpe": float(np.mean(sharpes)),
                "best_pnl": float(np.max(pnls)),
                "worst_pnl": float(np.min(pnls)),
                "pnl_std": float(np.std(pnls))
            },

            # ìµœê³  ì—ì´ì „íŠ¸
            "best_agent": {
                "agent_id": best_agent_id,
                "pnl": float(best_agent_result.get("total_pnl", 0)),
                "win_rate": float(best_agent_result.get("win_rate", 0)),
                "trades": int(best_agent_result.get("total_trades", 0)),
                "sharpe": float(best_agent_result.get("sharpe_ratio", 0))
            },

            # ì—ì´ì „íŠ¸ë³„ ìƒì„¸ ê²°ê³¼
            "agent_results": {
                agent_id: {
                    "total_trades": int(result.get("total_trades", 0)),
                    "win_rate": float(result.get("win_rate", 0)),
                    "total_pnl": float(result.get("total_pnl", 0)),
                    "avg_pnl_per_trade": float(result.get("avg_pnl_per_trade", 0)),
                    "max_drawdown": float(result.get("max_drawdown", 0)),
                    "sharpe_ratio": float(result.get("sharpe_ratio", 0)),
                    "final_balance": float(result.get("final_balance", 10000))
                }
                for agent_id, result in agent_results.items()
            }
        })

    def log_reward_distribution(self, episode: int, rewards: List[float]):
        """
        ë³´ìƒ ë¶„í¬ ë¡œê¹… (ì¤‘ìš”!)

        Args:
            episode: ì—í”¼ì†Œë“œ ë²ˆí˜¸
            rewards: ë³´ìƒ ë¦¬ìŠ¤íŠ¸
        """
        if not rewards:
            return

        rewards = np.array(rewards)

        self.log({
            "event": "reward_distribution",
            "episode": episode,
            "count": len(rewards),
            "statistics": {
                "mean": float(np.mean(rewards)),
                "std": float(np.std(rewards)),
                "min": float(np.min(rewards)),
                "max": float(np.max(rewards)),
                "median": float(np.median(rewards)),
                "q25": float(np.percentile(rewards, 25)),
                "q75": float(np.percentile(rewards, 75))
            },
            "distribution": {
                "zero_ratio": float(np.mean(rewards == 0)),
                "positive_ratio": float(np.mean(rewards > 0)),
                "negative_ratio": float(np.mean(rewards < 0)),
                "large_positive": float(np.mean(rewards > 1.0)),
                "large_negative": float(np.mean(rewards < -1.0))
            },
            "warnings": {
                "too_sparse": np.mean(rewards == 0) > 0.9,  # 90% ì´ìƒì´ 0
                "low_variance": np.std(rewards) < 0.01,
                "has_outliers": np.max(np.abs(rewards)) > 10 * np.std(rewards)
            }
        })

    def log_action_sequence(self, episode: int, agent_id: str, actions: List[int], steps: List[int]):
        """
        ì•¡ì…˜ ì‹œí€€ìŠ¤ ë¡œê¹… (íŒ¨í„´ ë¶„ì„ìš©)

        Args:
            episode: ì—í”¼ì†Œë“œ ë²ˆí˜¸
            agent_id: ì—ì´ì „íŠ¸ ID
            actions: ì•¡ì…˜ ë¦¬ìŠ¤íŠ¸ [0, 0, 0, 1, 0, 2, ...]
            steps: ìŠ¤í… ë²ˆí˜¸ ë¦¬ìŠ¤íŠ¸
        """
        action_names = ["HOLD", "BUY", "SELL"]
        action_sequence = [action_names[a] if a < len(action_names) else f"action_{a}" for a in actions]

        # íŒ¨í„´ ë¶„ì„
        hold_streaks = []
        current_streak = 0
        for action in actions:
            if action == 0:  # HOLD
                current_streak += 1
            else:
                if current_streak > 0:
                    hold_streaks.append(current_streak)
                current_streak = 0

        self.log({
            "event": "action_sequence",
            "episode": episode,
            "agent_id": agent_id,
            "total_actions": len(actions),
            "sequence": action_sequence if len(actions) <= 100 else None,  # 100ê°œ ì´í•˜ë§Œ ì „ì²´ ì¶œë ¥
            "patterns": {
                "hold_ratio": float(np.mean(np.array(actions) == 0)),
                "buy_ratio": float(np.mean(np.array(actions) == 1)),
                "sell_ratio": float(np.mean(np.array(actions) == 2)),
                "avg_hold_streak": float(np.mean(hold_streaks)) if hold_streaks else 0,
                "max_hold_streak": int(np.max(hold_streaks)) if hold_streaks else 0,
                "action_switches": int(np.sum(np.diff(actions) != 0))
            }
        })

    def log_selfplay_end(
        self,
        coin: str,
        interval: str,
        total_episodes: int,
        summary: Dict[str, Any]
    ):
        """
        Self-play ì¢…ë£Œ ë¡œê¹…

        Args:
            coin: ì½”ì¸ ì‹¬ë³¼
            interval: ì¸í„°ë²Œ
            total_episodes: ì´ ì—í”¼ì†Œë“œ ìˆ˜
            summary: ì „ì²´ ìš”ì•½ í†µê³„
        """
        self.log({
            "event": "selfplay_end",
            "coin": coin,
            "interval": interval,
            "total_episodes": total_episodes,
            "summary": summary,
            "statistics": self.simulation_stats,
            "message": f"âœ… Self-play ì™„ë£Œ: {coin}-{interval}"
        })

        # í†µê³„ ì €ì¥
        self.stats.update(self.simulation_stats)
        self.save_stats()
