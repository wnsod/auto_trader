"""
Training ëª¨ë“ˆ ì „ìš© ë””ë²„ê·¸ ë¡œê±°
- PPO í•™ìŠµ ê³¼ì • ìƒì„¸ ì¶”ì 
- ì•¡ì…˜ ë‹¤ì–‘ì„± ë¬¸ì œ ê°ì§€
- ê·¸ëž˜ë””ì–¸íŠ¸ ì†Œì‹¤/í­ë°œ ê°ì§€
- í•™ìŠµ ì •ì²´ ê°ì§€
"""

from typing import Dict, List, Any, Optional
import numpy as np
from .debug_logger import DebugLogger


class TrainingDebugger(DebugLogger):
    """Training ëª¨ë“ˆ ì „ìš© ë””ë²„ê±°"""

    def __init__(self, session_id: str = None):
        super().__init__("training", session_id)

        # í•™ìŠµ ì¶”ì ìš© í†µê³„
        self.training_stats = {
            "total_epochs": 0,
            "total_batches": 0,
            "action_diversity_warnings": 0,
            "gradient_issues": 0,
            "loss_improvements": 0,
            "best_loss": float('inf')
        }

    def log_training_start(self, config: Dict[str, Any]):
        """
        í•™ìŠµ ì‹œìž‘ ë¡œê¹…

        Args:
            config: í•™ìŠµ ì„¤ì • (lr, epochs, batch_size ë“±)
        """
        self.log({
            "event": "training_start",
            "config": config,
            "message": "ðŸš€ PPO í•™ìŠµ ì‹œìž‘"
        })

    def log_epoch_start(self, epoch: int, total_epochs: int, learning_rate: float):
        """
        Epoch ì‹œìž‘ ë¡œê¹…

        Args:
            epoch: í˜„ìž¬ epoch ë²ˆí˜¸
            total_epochs: ì „ì²´ epoch ìˆ˜
            learning_rate: í˜„ìž¬ í•™ìŠµë¥ 
        """
        self.training_stats["total_epochs"] = epoch

        self.log({
            "event": "epoch_start",
            "epoch": epoch,
            "total_epochs": total_epochs,
            "learning_rate": learning_rate,
            "progress": f"{epoch}/{total_epochs}"
        })

    def log_batch_training(
        self,
        epoch: int,
        batch_idx: int,
        total_batches: int,
        loss: float,
        policy_loss: float,
        value_loss: float,
        entropy_loss: float,
        actions: List[int],
        action_probs: np.ndarray,
        entropy_coef: float,
        clip_ratio: float = None,
        kl_divergence: float = None
    ):
        """
        ë°°ì¹˜ í•™ìŠµ ìƒì„¸ ë¡œê¹… (ê°€ìž¥ ì¤‘ìš”!)

        Args:
            epoch: Epoch ë²ˆí˜¸
            batch_idx: ë°°ì¹˜ ì¸ë±ìŠ¤
            total_batches: ì „ì²´ ë°°ì¹˜ ìˆ˜
            loss: ì´ ì†ì‹¤
            policy_loss: ì •ì±… ì†ì‹¤
            value_loss: ê°€ì¹˜ í•¨ìˆ˜ ì†ì‹¤
            entropy_loss: ì—”íŠ¸ë¡œí”¼ ì†ì‹¤
            actions: ë°°ì¹˜ì˜ ì•¡ì…˜ ë¦¬ìŠ¤íŠ¸ [0, 0, 1, 2, ...]
            action_probs: ì•¡ì…˜ í™•ë¥  ë¶„í¬ (batch_size, num_actions)
            entropy_coef: ì—”íŠ¸ë¡œí”¼ ê³„ìˆ˜
            clip_ratio: PPO í´ë¦½ ë¹„ìœ¨ (ì˜µì…˜)
            kl_divergence: KL divergence (ì˜µì…˜)
        """
        self.training_stats["total_batches"] += 1

        # ì•¡ì…˜ ë¶„í¬ ë¶„ì„
        action_names = ["HOLD", "BUY", "SELL"]
        unique_actions = len(np.unique(actions))
        action_counts = np.bincount(actions, minlength=3)
        action_ratios = action_counts / len(actions)

        # ì•¡ì…˜ í™•ë¥  í†µê³„
        mean_action_probs = np.mean(action_probs, axis=0)
        max_action_prob = np.max(mean_action_probs)
        min_action_prob = np.min(mean_action_probs)

        # Entropy ê³„ì‚°
        probs = action_counts / len(actions)
        probs = probs[probs > 0]
        entropy = -np.sum(probs * np.log(probs + 1e-10))
        max_entropy = np.log(3)  # 3ê°œ ì•¡ì…˜
        normalized_entropy = entropy / max_entropy

        # ì•¡ì…˜ ë‹¤ì–‘ì„± ê²½ê³  ê°ì§€
        action_diversity_warning = False
        if unique_actions == 1 or action_ratios.max() > 0.95:
            self.training_stats["action_diversity_warnings"] += 1
            action_diversity_warning = True

        # ìƒì„¸ ë¡œê·¸
        self.log({
            "event": "batch_training",
            "epoch": epoch,
            "batch": batch_idx,
            "total_batches": total_batches,
            "progress": f"{batch_idx}/{total_batches}",

            # ì†ì‹¤ ì •ë³´
            "loss": {
                "total": float(loss),
                "policy": float(policy_loss),
                "value": float(value_loss),
                "entropy": float(entropy_loss)
            },

            # ì•¡ì…˜ ë¶„í¬ (í•µì‹¬!)
            "action_distribution": {
                action_names[i]: {
                    "count": int(action_counts[i]),
                    "ratio": float(action_ratios[i])
                }
                for i in range(len(action_names))
            },
            "action_stats": {
                "unique_actions": int(unique_actions),
                "total_actions": len(actions),
                "entropy": float(entropy),
                "normalized_entropy": float(normalized_entropy),
                "diversity_score": float(normalized_entropy),
                "dominant_action": action_names[np.argmax(action_ratios)],
                "dominant_ratio": float(action_ratios.max())
            },

            # ì•¡ì…˜ í™•ë¥  ë¶„ì„
            "action_probabilities": {
                "mean": {
                    action_names[i]: float(mean_action_probs[i])
                    for i in range(len(action_names))
                },
                "max_prob": float(max_action_prob),
                "min_prob": float(min_action_prob),
                "prob_spread": float(max_action_prob - min_action_prob)
            },

            # í•˜ì´í¼íŒŒë¼ë¯¸í„°
            "hyperparameters": {
                "entropy_coef": float(entropy_coef),
                "clip_ratio": float(clip_ratio) if clip_ratio is not None else None,
                "kl_divergence": float(kl_divergence) if kl_divergence is not None else None
            },

            # ê²½ê³ 
            "warnings": {
                "action_diversity_low": action_diversity_warning,
                "dominant_action_warning": action_ratios.max() > 0.90,
                "entropy_too_low": normalized_entropy < 0.3
            }
        })

    def log_gradient_update(
        self,
        epoch: int,
        batch_idx: int,
        gradients: Dict[str, np.ndarray],
        learning_rate: float,
        grad_norm: float = None,
        clipped: bool = False
    ):
        """
        ê·¸ëž˜ë””ì–¸íŠ¸ ì—…ë°ì´íŠ¸ ë¡œê¹…

        Args:
            epoch: Epoch ë²ˆí˜¸
            batch_idx: ë°°ì¹˜ ì¸ë±ìŠ¤
            gradients: ê·¸ëž˜ë””ì–¸íŠ¸ ë”•ì…”ë„ˆë¦¬
            learning_rate: í•™ìŠµë¥ 
            grad_norm: ê·¸ëž˜ë””ì–¸íŠ¸ norm
            clipped: ê·¸ëž˜ë””ì–¸íŠ¸ í´ë¦¬í•‘ ì—¬ë¶€
        """
        # ê·¸ëž˜ë””ì–¸íŠ¸ í†µê³„ ìžë™ ê³„ì‚°
        self.log_gradient_stats(gradients)

        # ê·¸ëž˜ë””ì–¸íŠ¸ ì´ìŠˆ ê°ì§€
        has_nan = any(np.any(np.isnan(g)) for g in gradients.values() if isinstance(g, np.ndarray))
        has_inf = any(np.any(np.isinf(g)) for g in gradients.values() if isinstance(g, np.ndarray))

        if has_nan or has_inf or (grad_norm and grad_norm > 1e3):
            self.training_stats["gradient_issues"] += 1

        self.log({
            "event": "gradient_update",
            "epoch": epoch,
            "batch": batch_idx,
            "learning_rate": float(learning_rate),
            "grad_norm": float(grad_norm) if grad_norm else None,
            "clipped": clipped,
            "warnings": {
                "has_nan": has_nan,
                "has_inf": has_inf,
                "exploding": grad_norm > 1e3 if grad_norm else False,
                "vanishing": grad_norm < 1e-6 if grad_norm else False
            }
        })

    def log_epoch_end(
        self,
        epoch: int,
        avg_loss: float,
        best_loss: float,
        improved: bool,
        no_improvement_count: int,
        learning_rate: float
    ):
        """
        Epoch ì¢…ë£Œ ë¡œê¹…

        Args:
            epoch: Epoch ë²ˆí˜¸
            avg_loss: í‰ê·  ì†ì‹¤
            best_loss: ìµœê³  ì†ì‹¤
            improved: ê°œì„  ì—¬ë¶€
            no_improvement_count: ê°œì„  ì—†ëŠ” epoch ìˆ˜
            learning_rate: í˜„ìž¬ í•™ìŠµë¥ 
        """
        if improved:
            self.training_stats["loss_improvements"] += 1
            self.training_stats["best_loss"] = best_loss

        self.log({
            "event": "epoch_end",
            "epoch": epoch,
            "avg_loss": float(avg_loss),
            "best_loss": float(best_loss),
            "improved": improved,
            "no_improvement_count": no_improvement_count,
            "learning_rate": float(learning_rate),
            "status": "âœ… ê°œì„ " if improved else f"âš ï¸ ê°œì„  ì—†ìŒ ({no_improvement_count}íšŒ)"
        })

    def log_early_stopping(self, epoch: int, reason: str, best_loss: float):
        """
        ì¡°ê¸° ì¢…ë£Œ ë¡œê¹…

        Args:
            epoch: ì¢…ë£Œëœ Epoch
            reason: ì¢…ë£Œ ì´ìœ 
            best_loss: ìµœê³  ì†ì‹¤
        """
        self.log({
            "event": "early_stopping",
            "epoch": epoch,
            "reason": reason,
            "best_loss": float(best_loss),
            "message": f"ðŸ›‘ ì¡°ê¸° ì¢…ë£Œ: {reason}"
        }, level="WARNING")

    def log_learning_rate_adjustment(self, old_lr: float, new_lr: float, reason: str):
        """
        í•™ìŠµë¥  ì¡°ì • ë¡œê¹…

        Args:
            old_lr: ì´ì „ í•™ìŠµë¥ 
            new_lr: ìƒˆ í•™ìŠµë¥ 
            reason: ì¡°ì • ì´ìœ 
        """
        self.log({
            "event": "learning_rate_adjustment",
            "old_lr": float(old_lr),
            "new_lr": float(new_lr),
            "change_ratio": float(new_lr / old_lr) if old_lr > 0 else 0,
            "reason": reason,
            "message": f"ðŸ“‰ í•™ìŠµë¥  ì¡°ì •: {old_lr:.6f} â†’ {new_lr:.6f}"
        })

    def log_training_data_stats(
        self,
        states: np.ndarray,
        actions: np.ndarray,
        rewards: np.ndarray,
        values: np.ndarray = None,
        advantages: np.ndarray = None,
        returns: np.ndarray = None
    ):
        """
        í•™ìŠµ ë°ì´í„° í†µê³„ ë¡œê¹…

        Args:
            states: ìƒíƒœ ë°°ì—´ (batch_size, state_dim)
            actions: ì•¡ì…˜ ë°°ì—´ (batch_size,)
            rewards: ë³´ìƒ ë°°ì—´ (batch_size,)
            values: ê°€ì¹˜ ì¶”ì • ë°°ì—´ (ì˜µì…˜)
            advantages: Advantage ë°°ì—´ (ì˜µì…˜)
            returns: Return ë°°ì—´ (ì˜µì…˜)
        """
        data_stats = {
            "event": "training_data_stats",
            "batch_size": len(states),
            "state_dim": states.shape[1] if len(states.shape) > 1 else 1,

            "states": {
                "shape": list(states.shape),
                "mean": float(np.mean(states)),
                "std": float(np.std(states)),
                "min": float(np.min(states)),
                "max": float(np.max(states)),
                "has_nan": bool(np.any(np.isnan(states))),
                "has_inf": bool(np.any(np.isinf(states)))
            },

            "actions": {
                "unique": len(np.unique(actions)),
                "distribution": np.bincount(actions.astype(int), minlength=3).tolist()
            },

            "rewards": {
                "mean": float(np.mean(rewards)),
                "std": float(np.std(rewards)),
                "min": float(np.min(rewards)),
                "max": float(np.max(rewards)),
                "zero_ratio": float(np.mean(rewards == 0)),
                "positive_ratio": float(np.mean(rewards > 0)),
                "negative_ratio": float(np.mean(rewards < 0))
            }
        }

        if values is not None:
            data_stats["values"] = {
                "mean": float(np.mean(values)),
                "std": float(np.std(values)),
                "min": float(np.min(values)),
                "max": float(np.max(values))
            }

        if advantages is not None:
            data_stats["advantages"] = {
                "mean": float(np.mean(advantages)),
                "std": float(np.std(advantages)),
                "min": float(np.min(advantages)),
                "max": float(np.max(advantages))
            }

        if returns is not None:
            data_stats["returns"] = {
                "mean": float(np.mean(returns)),
                "std": float(np.std(returns)),
                "min": float(np.min(returns)),
                "max": float(np.max(returns))
            }

        self.log(data_stats)

    def log_training_end(self, total_epochs: int, best_loss: float, final_loss: float, converged: bool):
        """
        í•™ìŠµ ì¢…ë£Œ ë¡œê¹…

        Args:
            total_epochs: ì´ í•™ìŠµ epoch ìˆ˜
            best_loss: ìµœê³  ì†ì‹¤
            final_loss: ìµœì¢… ì†ì‹¤
            converged: ìˆ˜ë ´ ì—¬ë¶€
        """
        summary = {
            "event": "training_end",
            "total_epochs": total_epochs,
            "best_loss": float(best_loss),
            "final_loss": float(final_loss),
            "converged": converged,
            "statistics": self.training_stats,
            "message": "âœ… í•™ìŠµ ì™„ë£Œ" if converged else "âš ï¸ í•™ìŠµ ë¯¸ìˆ˜ë ´"
        }

        self.log(summary)

        # í†µê³„ ì €ìž¥
        self.stats.update(self.training_stats)
        self.save_stats()
