"""
í•˜ì´ë¸Œë¦¬ë“œ ì •ì±… ì—ì´ì „íŠ¸
ê·œì¹™ ê¸°ë°˜ + ì‹ ê²½ë§ ê¸°ë°˜ í†µí•©
"""

import logging
from typing import Dict, Any, Optional, List
import numpy as np

from rl_pipeline.simulation.agent import StrategyAgent
from rl_pipeline.simulation.market_models import MarketState, Action

logger = logging.getLogger(__name__)


class HybridPolicyAgent(StrategyAgent):
    """
    í•˜ì´ë¸Œë¦¬ë“œ ì •ì±… ì—ì´ì „íŠ¸
    
    ê·œì¹™ ê¸°ë°˜ ì •ì±…ê³¼ ì‹ ê²½ë§ ì •ì±…ì„ ê²°í•©í•˜ì—¬ ì˜ì‚¬ê²°ì •
    - ëª…í™•í•œ ì‹ í˜¸: ê·œì¹™ ê¸°ë°˜ ì¦‰ì‹œ ê²°ì •
    - ì• ë§¤í•œ êµ¬ê°„: ì‹ ê²½ë§ ì •ì±… ì‚¬ìš©
    - í´ë°±: ì‹ ê²½ë§ ë¶€ì¬/ì‹¤íŒ¨ ì‹œ ê·œì¹™ ê¸°ë°˜
    """
    
    def __init__(
        self,
        agent_id: str,
        strategy_params: Dict[str, Any],
        neural_policy: Optional[Dict[str, Any]] = None,
        use_neural_threshold: float = 0.3,
        enable_neural: bool = False,
        max_latency_ms: float = 10.0
    ):
        """
        ì´ˆê¸°í™”
        
        Args:
            agent_id: ì—ì´ì „íŠ¸ ID
            strategy_params: ì „ëµ íŒŒë¼ë¯¸í„° (ê·œì¹™ ê¸°ë°˜ìš©)
            neural_policy: ì‹ ê²½ë§ ì •ì±… ëª¨ë¸ (Noneì´ë©´ ì‹ ê²½ë§ ë¯¸ì‚¬ìš©)
            use_neural_threshold: ì‹ ê²½ë§ ì‚¬ìš© ìµœì†Œ ì‹ ë¢°ë„ (0~1)
            enable_neural: ì‹ ê²½ë§ í™œì„±í™” ì—¬ë¶€
            max_latency_ms: ìµœëŒ€ í—ˆìš© ì§€ì—° ì‹œê°„ (ë°€ë¦¬ì´ˆ)
        """
        # ê¸°ì¡´ StrategyAgent ì´ˆê¸°í™”
        super().__init__(agent_id, strategy_params)
        
        self.neural_policy = neural_policy
        self.use_neural_threshold = use_neural_threshold
        self.enable_neural = enable_neural and (neural_policy is not None)
        self.max_latency_ms = max_latency_ms
        
        # ì˜ì‚¬ê²°ì • ë¡œê·¸ (ë””ë²„ê¹…/ë¶„ì„ìš©)
        self.decision_log: List[Dict[str, Any]] = []
        self.stats = {
            'rule_decisions': 0,
            'neural_decisions': 0,
            'neural_errors': 0,
            'clear_signal_count': 0
        }
    
    def decide_action(self, market_state: MarketState) -> Action:
        """
        í•˜ì´ë¸Œë¦¬ë“œ ì˜ì‚¬ê²°ì •
        
        íë¦„:
        1. ëª…í™•í•œ ì‹ í˜¸ ì²´í¬ (ê·œì¹™ ê¸°ë°˜)
        2. ì‹ ê²½ë§ ì‚¬ìš© ê°€ëŠ¥í•˜ê³  í™œì„±í™”ëœ ê²½ìš° ì‹ ê²½ë§ íŒë‹¨
        3. ê¸°ë³¸ê°’: ê·œì¹™ ê¸°ë°˜ (ê¸°ì¡´ ë¡œì§)
        
        Returns:
            Action: BUY/SELL/HOLD
        """
        try:
            # 1. ëª…í™•í•œ ì‹ í˜¸ ì²´í¬ (ê·œì¹™ ê¸°ë°˜, ë¹ ë¥¸ ì²˜ë¦¬)
            clear_action = self._check_clear_signals(market_state)
            if clear_action is not None:
                self._log_decision('rule', 'clear_signal', clear_action, 1.0)
                self.stats['clear_signal_count'] += 1
                self.stats['rule_decisions'] += 1
                return clear_action
            
            # 2. ì‹ ê²½ë§ ì‚¬ìš© ê°€ëŠ¥í•˜ê³  í™œì„±í™”ëœ ê²½ìš°
            if self.enable_neural and self.neural_policy is not None:
                try:
                    import time
                    start_time = time.time()
                    
                    # ğŸ”¥ í‰ê°€ ë‹¨ê³„ì—ì„œëŠ” deterministic=Trueë¡œ ì¼ê´€ëœ ì•¡ì…˜ ìƒì„±
                    # í•™ìŠµ ë‹¨ê³„ì—ì„œëŠ” deterministic=Falseë¡œ íƒí—˜ í—ˆìš©
                    neural_result = self._get_neural_action(market_state, deterministic=True)
                    
                    latency_ms = (time.time() - start_time) * 1000
                    
                    # ì§€ì—° ì‹œê°„ ì²´í¬
                    if latency_ms > self.max_latency_ms:
                        logger.warning(f"âš ï¸ ì‹ ê²½ë§ ì§€ì—° ì‹œê°„ ì´ˆê³¼: {latency_ms:.2f}ms > {self.max_latency_ms}ms, ê·œì¹™ìœ¼ë¡œ í´ë°±")
                        self.stats['neural_errors'] += 1
                        rule_action = super().decide_action(market_state)
                        self._log_decision('rule', 'latency_exceeded', rule_action, 0.0)
                        self.stats['rule_decisions'] += 1
                        return rule_action
                    
                    # ğŸ”¥ ì‹ ë¢°ë„ ì²´í¬ ì™„í™”: í‰ê°€ ë‹¨ê³„ì—ì„œëŠ” ì‹ ê²½ë§ì„ ë” ì ê·¹ì ìœ¼ë¡œ ì‚¬ìš©
                    # use_neural_thresholdë¥¼ ë‚®ì¶°ì„œ ì‹ ê²½ë§ ì‚¬ìš© ë¹ˆë„ ì¦ê°€
                    effective_threshold = max(0.1, self.use_neural_threshold * 0.5)  # ìµœì†Œ 0.1, ê¸°ë³¸ê°’ì˜ 50%
                    
                    if neural_result['confidence'] >= effective_threshold:
                        self._log_decision(
                            'neural',
                            f"confidence_{neural_result['confidence']:.2f}",
                            neural_result['action'],
                            neural_result['confidence']
                        )
                        self.stats['neural_decisions'] += 1
                        return neural_result['action']
                    else:
                        # ì‹ ë¢°ë„ ë‚®ìœ¼ë©´ ê·œì¹™ìœ¼ë¡œ í´ë°±
                        logger.debug(f"ì‹ ê²½ë§ ì‹ ë¢°ë„ ë‚®ìŒ: {neural_result['confidence']:.2f} < {effective_threshold:.2f}")
                
                except Exception as e:
                    logger.warning(f"âš ï¸ ì‹ ê²½ë§ ì¶”ë¡  ì‹¤íŒ¨, ê·œì¹™ìœ¼ë¡œ í´ë°±: {e}")
                    self.stats['neural_errors'] += 1
            
            # 3. ê¸°ë³¸ê°’: ê·œì¹™ ê¸°ë°˜ (ê¸°ì¡´ ë¡œì§ ì‚¬ìš©)
            rule_action = super().decide_action(market_state)
            self._log_decision('rule', 'default', rule_action, 0.5)
            self.stats['rule_decisions'] += 1
            return rule_action
            
        except Exception as e:
            logger.error(f"âŒ í•˜ì´ë¸Œë¦¬ë“œ ì˜ì‚¬ê²°ì • ì‹¤íŒ¨: {e}")
            # ìµœì¢… í´ë°±: ê·œì¹™ ê¸°ë°˜
            rule_action = super().decide_action(market_state)
            self._log_decision('rule', 'error_fallback', rule_action, 0.0)
            return rule_action
    
    def _check_clear_signals(self, market_state: MarketState) -> Optional[Action]:
        """
        ëª…í™•í•œ ì‹ í˜¸ ì²´í¬ (ê·œì¹™ ê¸°ë°˜)
        
        ë§¤ìš° ê°•í•œ ì‹ í˜¸ëŠ” ì¦‰ì‹œ ê²°ì •í•˜ì—¬ ì²˜ë¦¬ ì†ë„ í–¥ìƒ
        
        Returns:
            Action ë˜ëŠ” None
        """
        try:
            # ë§¤ìš° ê°•í•œ ë§¤ìˆ˜ ì‹ í˜¸
            # ì¡°ê±´: RSI ë§¤ìš° ë‚®ìŒ + MACD ê°•í•œ ìƒìŠ¹ + ê±°ë˜ëŸ‰ ê¸‰ì¦ + ë ˆì§ ì‹ ë¢°ë„ ë†’ìŒ
            if (market_state.rsi < 20 and 
                market_state.macd > market_state.macd_signal * 1.5 and
                market_state.volume_ratio > 2.0 and
                market_state.regime_confidence > 0.7 and
                market_state.regime_stage >= 4):  # ì¤‘ë¦½ ì´ìƒ
                return Action.BUY
            
            # ë§¤ìš° ê°•í•œ ë§¤ë„ ì‹ í˜¸
            # ì¡°ê±´: RSI ë§¤ìš° ë†’ìŒ + MACD ê°•í•œ í•˜ë½ + ë ˆì§ ì‹ ë¢°ë„ ë†’ìŒ
            if (market_state.rsi > 80 and
                market_state.macd < market_state.macd_signal * 0.5 and
                market_state.regime_confidence > 0.7 and
                market_state.regime_stage <= 4):  # ì¤‘ë¦½ ì´í•˜
                return Action.SELL
            
            return None
            
        except Exception as e:
            logger.debug(f"ëª…í™•í•œ ì‹ í˜¸ ì²´í¬ ì‹¤íŒ¨: {e}")
            return None
    
    def _get_neural_action(self, market_state: MarketState, deterministic: bool = False) -> Dict[str, Any]:
        """
        ì‹ ê²½ë§ìœ¼ë¡œ ì•¡ì…˜ ê²°ì •
        
        Args:
            market_state: ì‹œì¥ ìƒíƒœ
            deterministic: Trueë©´ ìµœëŒ€ í™•ë¥  ì•¡ì…˜, Falseë©´ ìƒ˜í”Œë§ (í‰ê°€ ì‹œ True ê¶Œì¥)
        
        Returns:
            {
                'action': Action,
                'confidence': float,  # 0~1
                'action_probs': np.ndarray,  # (3,) ì•¡ì…˜ë³„ í™•ë¥ 
                'value': float  # ìƒíƒœ ê°€ì¹˜
            }
        """
        from rl_pipeline.hybrid.features import build_state_vector
        from rl_pipeline.hybrid.neural_policy_jax import apply
        import jax.random as jrandom
        
        # ìƒíƒœ ë²¡í„° ë³€í™˜
        state_vec = build_state_vector(market_state)
        
        # JAX ëœë¤ í‚¤ ìƒì„± (ì—ì´ì „íŠ¸ë³„ ê³ ìœ  í‚¤)
        agent_hash = hash(self.agent_id) % (2**31)
        rng_key = jrandom.PRNGKey(agent_hash)
        
        # ğŸ”¥ ì‹ ê²½ë§ ì¶”ë¡  (í‰ê°€ ì‹œ deterministic=Trueë¡œ ì¼ê´€ëœ ì•¡ì…˜ ìƒì„±)
        result = apply(self.neural_policy, state_vec, rng_key, deterministic=deterministic)
        
        # Action enumìœ¼ë¡œ ë³€í™˜
        action_map = {
            0: Action.HOLD,
            1: Action.BUY,
            2: Action.SELL
        }
        
        return {
            'action': action_map.get(result['action'], Action.HOLD),
            'confidence': result['confidence'],
            'action_probs': result['action_probs'],
            'value': result['value']
        }
    
    def _log_decision(self, method: str, reason: str, action: Action, confidence: float):
        """
        ì˜ì‚¬ê²°ì • ë¡œê·¸ ì €ì¥ (ë””ë²„ê¹…/ë¶„ì„ìš©)
        
        Args:
            method: 'rule' or 'neural'
            reason: ê²°ì • ì‚¬ìœ 
            action: ì„ íƒëœ ì•¡ì…˜
            confidence: ì‹ ë¢°ë„ (0~1)
        """
        self.decision_log.append({
            'method': method,
            'reason': reason,
            'action': action.value,
            'confidence': confidence
        })
        
        # ë¡œê·¸ê°€ ë„ˆë¬´ ê¸¸ì–´ì§€ë©´ ìµœê·¼ Nê°œë§Œ ìœ ì§€
        if len(self.decision_log) > 1000:
            self.decision_log = self.decision_log[-500:]
    
    def get_stats(self) -> Dict[str, Any]:
        """ì˜ì‚¬ê²°ì • í†µê³„ ë°˜í™˜"""
        total = self.stats['rule_decisions'] + self.stats['neural_decisions']
        
        return {
            **self.stats,
            'total_decisions': total,
            'rule_ratio': self.stats['rule_decisions'] / total if total > 0 else 0.0,
            'neural_ratio': self.stats['neural_decisions'] / total if total > 0 else 0.0,
        }

