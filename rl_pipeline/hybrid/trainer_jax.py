"""
PPO í•™ìŠµê¸° (JAX ê¸°ë°˜)
Self-play ë°ì´í„°ë¥¼ í™œìš©í•œ ê°•í™”í•™ìŠµ
"""

import logging
import os
import json
import uuid
import math
import copy
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime
import numpy as np
import pandas as pd

logger = logging.getLogger(__name__)

# JAX ê°€ìš©ì„± í™•ì¸
try:
    import jax
    import jax.numpy as jnp
    # JAX tree ìœ í‹¸ë¦¬í‹° í™•ì¸ (ë²„ì „ë³„ í˜¸í™˜ì„±)
    try:
        # ìµœì‹  ë²„ì „ (v0.4.25+): jax.tree
        _ = jax.tree
        USE_JAX_TREE = True
    except AttributeError:
        # êµ¬ë²„ì „: jax.tree_util
        try:
            from jax import tree_util
            USE_JAX_TREE = False
            JAX_TREE_UTIL = tree_util
        except ImportError:
            USE_JAX_TREE = False
            JAX_TREE_UTIL = None
    
    JAX_AVAILABLE = True
    logger.debug("âœ… trainer_jax: JAX/Flax ì„í¬íŠ¸ ì„±ê³µ")
except ImportError as e:
    JAX_AVAILABLE = False
    logger.warning(f"âš ï¸ trainer_jax: JAXê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: {e}")
    jax = None
    jnp = None
    USE_JAX_TREE = False
    JAX_TREE_UTIL = None

# neural_policy_jax ëª¨ë“ˆ ì„í¬íŠ¸ (JAX_AVAILABLE ì²´í¬ í¬í•¨)
try:
    from rl_pipeline.hybrid.neural_policy_jax import init_model, apply, save_ckpt, PolicyNetwork, JAX_AVAILABLE as NEURAL_JAX_AVAILABLE
    # neural_policy_jaxì˜ JAX_AVAILABLEë„ í™•ì¸
    if not NEURAL_JAX_AVAILABLE:
        logger.warning("âš ï¸ trainer_jax: neural_policy_jaxì—ì„œ JAX ì‚¬ìš© ë¶ˆê°€")
        if JAX_AVAILABLE:
            # ë¡œì»¬ì—ì„œëŠ” JAXê°€ ìˆì§€ë§Œ neural_policy_jaxì—ì„œëŠ” ì—†ëŠ” ê²½ìš°
            logger.warning("âš ï¸ trainer_jax: ë¡œì»¬ JAXëŠ” ìˆì§€ë§Œ neural_policy_jax ëª¨ë“ˆì—ì„œ ì‚¬ìš© ë¶ˆê°€")
        JAX_AVAILABLE = False
except ImportError as e:
    logger.warning(f"âš ï¸ trainer_jax: neural_policy_jax ì„í¬íŠ¸ ì‹¤íŒ¨: {e}")
    JAX_AVAILABLE = False
    init_model = None
    apply = None
    save_ckpt = None
    PolicyNetwork = None

from rl_pipeline.hybrid.features import (
    build_state_vector,
    build_state_vector_with_analysis,
    build_state_vector_with_strategy,  # ğŸš€ ë©”íƒ€ í•™ìŠµ
    build_state_vector_with_analysis_and_strategy,  # ğŸš€ ë©”íƒ€ í•™ìŠµ
    FEATURE_DIM,
    FEATURE_DIM_WITH_ANALYSIS,
    FEATURE_DIM_WITH_STRATEGY,  # ğŸš€ ë©”íƒ€ í•™ìŠµ: 30ì°¨ì›
    FEATURE_DIM_WITH_ANALYSIS_AND_STRATEGY  # ğŸš€ ë©”íƒ€ í•™ìŠµ: 35ì°¨ì›
)
from rl_pipeline.engine.reward_engine import RewardEngine
from rl_pipeline.db.connection_pool import get_strategy_db_pool

# ğŸ”¥ ë””ë²„ê·¸ ë¡œê¹… ì‹œìŠ¤í…œ
try:
    from rl_pipeline.monitoring import TrainingDebugger
    DEBUG_AVAILABLE = True
except ImportError:
    logger.warning("âš ï¸ ë””ë²„ê·¸ ë¡œê¹… ì‹œìŠ¤í…œì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
    DEBUG_AVAILABLE = False
    TrainingDebugger = None

# optaxëŠ” Flax 0.10+ ë²„ì „ì—ì„œ í•„ìˆ˜ (optimizer ì—­í• )
if JAX_AVAILABLE:
    try:
        import optax
        logger.debug("âœ… trainer_jax: optax ì„í¬íŠ¸ ì„±ê³µ")
    except ImportError as e:
        logger.error(f"âŒ trainer_jax: optax ì„í¬íŠ¸ ì‹¤íŒ¨: {e}")
        logger.error("âŒ optaxëŠ” í•„ìˆ˜ì…ë‹ˆë‹¤. pip install optax")
        JAX_AVAILABLE = False
        optax = None
else:
    optax = None

# ì•¡ì…˜/ìƒ˜í”Œë§ ê¸°ë³¸ ìƒìˆ˜ (í™˜ê²½ë³€ìˆ˜ë¡œ ì¡°ì • ê°€ëŠ¥)
NEUTRAL_TARGET_RATIO = float(os.getenv('NEUTRAL_TARGET_RATIO', '0.2'))
MIN_NEUTRAL_SAMPLES = int(os.getenv('MIN_NEUTRAL_SAMPLES', '3'))
MAX_NEUTRAL_SAMPLES = int(os.getenv('MAX_NEUTRAL_SAMPLES', '12'))
MAX_DIRECTION_SAMPLES = int(os.getenv('MAX_DIRECTION_SAMPLES', '12'))
NEUTRAL_ACTION_BONUS = float(os.getenv('NEUTRAL_ACTION_BONUS', '0.2'))
UPDOWN_ACTION_BONUS = float(os.getenv('UPDOWN_ACTION_BONUS', '0.1'))
NEUTRAL_PRICE_CHANGE_THRESHOLD = float(os.getenv('NEUTRAL_PRICE_CHANGE_THRESHOLD', '0.005'))
MAX_SYNTHETIC_NEUTRAL = int(os.getenv('MAX_SYNTHETIC_NEUTRAL', '20'))
MIN_DIRECTIONAL_SAMPLES = int(os.getenv('MIN_DIRECTIONAL_SAMPLES', '2'))
MAX_SYNTHETIC_DIRECTIONAL = int(os.getenv('MAX_SYNTHETIC_DIRECTIONAL', '12'))


class PPOTrainer:
    """
    PPO (Proximal Policy Optimization) í•™ìŠµê¸° - ì˜ˆì¸¡ ì „ëµ
    
    Self-play ë°ì´í„°ë¥¼ í™œìš©í•˜ì—¬ ë°©í–¥ ì˜ˆì¸¡ ì •ì±… í•™ìŠµ
    - ì•¡ì…˜ ê³µê°„: NEUTRAL(0), UP(1), DOWN(2) - ì˜ˆì¸¡ ë°©í–¥
    - ë³´ìƒ: ì˜ˆì¸¡ ì •í™•ë„ ê¸°ë°˜ (ë°©í–¥ ë§ì¶¤/í‹€ë¦¼)
    """
    
    def __init__(self, config: Dict[str, Any], session_id: Optional[str] = None):
        """
        ì´ˆê¸°í™”

        Args:
            config: í•™ìŠµ ì„¤ì • ë”•ì…”ë„ˆë¦¬
            session_id: ë””ë²„ê·¸ ì„¸ì…˜ ID (ì„ íƒì )
        """
        if not JAX_AVAILABLE:
            error_msg = "JAXê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install jax flax"
            logger.error(f"âŒ {error_msg}")
            raise ImportError(error_msg)

        # neural_policy_jax í•¨ìˆ˜ë“¤ì´ ì‚¬ìš© ê°€ëŠ¥í•œì§€ í™•ì¸
        if init_model is None or apply is None or save_ckpt is None:
            error_msg = "neural_policy_jax ëª¨ë“ˆì—ì„œ í•„ìš”í•œ í•¨ìˆ˜ë¥¼ ì„í¬íŠ¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            logger.error(f"âŒ {error_msg}")
            raise ImportError(error_msg)

        self.config = config
        self.train_config = config.get('train', {})
        self.reward_engine = RewardEngine()

        # ğŸ”¥ ë””ë²„ê±° ì´ˆê¸°í™”
        self.debug = None
        if DEBUG_AVAILABLE and session_id:
            try:
                self.debug = TrainingDebugger(session_id=session_id)
                logger.debug(f"âœ… Training ë””ë²„ê±° ì´ˆê¸°í™” ì™„ë£Œ (session: {session_id})")
            except Exception as e:
                logger.warning(f"âš ï¸ Training ë””ë²„ê±° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        
        # ëª¨ë¸ ì´ˆê¸°í™” (ë¶„ì„ ì ìˆ˜ í¬í•¨ ì°¨ì›ìœ¼ë¡œ ê¸°ë³¸ ì„¤ì •)
        # ğŸ”¥ jaxëŠ” ëª¨ë“ˆ ë ˆë²¨ì—ì„œ ì´ë¯¸ importë˜ì–´ ìˆìŒ - global ì„ ì–¸ìœ¼ë¡œ ëª…ì‹œ
        try:
            # global ì„ ì–¸ìœ¼ë¡œ ëª¨ë“ˆ ë ˆë²¨ì˜ jaxë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì‚¬ìš© (ìŠ¤ì½”í”„ ë¬¸ì œ ì™„ì „ í•´ê²°)
            global jax, jnp
            # JAX_AVAILABLEì´ Trueì´ë¯€ë¡œ jaxëŠ” Noneì´ ì•„ë‹ˆì–´ì•¼ í•¨
            if jax is None:
                raise ImportError("JAXê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. JAX_AVAILABLE ì²´í¬ë¥¼ í†µê³¼í–ˆì§€ë§Œ jaxê°€ Noneì…ë‹ˆë‹¤.")
            
            # ğŸ”¥ JAX ë°±ì—”ë“œ í™•ì¸ ë° CPU í´ë°± ì²˜ë¦¬ (RTX 5090 í˜¸í™˜)
            try:
                # JAX í”Œë«í¼ í™•ì¸ (ì—ëŸ¬ ë°œìƒ ì‹œ CPUë¡œ í´ë°±)
                devices = jax.devices()
                logger.debug(f"ğŸ” JAX ë””ë°”ì´ìŠ¤: {devices}")
                rng_key = jax.random.PRNGKey(config.get('seed', 42))
            except RuntimeError as backend_err:
                # CUDA ë°±ì—”ë“œ ì´ˆê¸°í™” ì‹¤íŒ¨ ì‹œ CPUë¡œ í´ë°±
                if 'cuda' in str(backend_err).lower() or 'backend' in str(backend_err).lower():
                    logger.warning(f"âš ï¸ JAX CUDA ë°±ì—”ë“œ ì‚¬ìš© ë¶ˆê°€: {backend_err}")
                    logger.info("ğŸ’» JAX CPU ëª¨ë“œë¡œ ì „í™˜ ì¤‘...")
                    try:
                        jax.config.update('jax_platform_name', 'cpu')
                        # JAX ì¬ì´ˆê¸°í™” (CPU ëª¨ë“œ)
                        devices = jax.devices()
                        logger.info(f"âœ… JAX CPU ëª¨ë“œë¡œ ì „í™˜ ì™„ë£Œ: {devices}")
                        rng_key = jax.random.PRNGKey(config.get('seed', 42))
                    except Exception as cpu_fallback_err:
                        logger.error(f"âŒ JAX CPU ëª¨ë“œ ì „í™˜ë„ ì‹¤íŒ¨: {cpu_fallback_err}")
                        raise
                else:
                    raise
            # ğŸ”¥ ì˜ˆì¸¡ ì „ëµ: ì•¡ì…˜ ê³µê°„ì„ ë°©í–¥ ì˜ˆì¸¡ìœ¼ë¡œ ì •ì˜
            # action_dim=3: 0=NEUTRAL(ì¤‘ë¦½), 1=UP(ìƒìŠ¹ ì˜ˆì¸¡), 2=DOWN(í•˜ë½ ì˜ˆì¸¡)
            # ê±°ë˜ ì•¡ì…˜(BUY/SELL/HOLD)ì´ ì•„ë‹Œ ì˜ˆì¸¡ ë°©í–¥ìœ¼ë¡œ í•™ìŠµ
            # ğŸš€ ë©”íƒ€ í•™ìŠµ: Stateì— ì „ëµ íŒŒë¼ë¯¸í„° í¬í•¨ (35ì°¨ì›)
            self.model = init_model(
                rng_key,
                obs_dim=FEATURE_DIM_WITH_ANALYSIS_AND_STRATEGY,  # ğŸš€ 35ì°¨ì› (ë¶„ì„+ì „ëµ íŒŒë¼ë¯¸í„°)
                action_dim=3,  # ğŸ”¥ ì˜ˆì¸¡ ë°©í–¥: NEUTRAL(0), UP(1), DOWN(2)
                hidden_dim=self.train_config.get('hidden_dim', 128)
            )
            
            # ğŸ”¥ ëª¨ë¸ íŒŒë¼ë¯¸í„° ì´ˆê¸° ê²€ì¦ (NaN/Inf ì²´í¬)
            try:
                # ëª¨ë“ˆ ë ˆë²¨ì˜ jax, jnp ì‚¬ìš© (í•¨ìˆ˜ ë‚´ë¶€ì—ì„œ ë‹¤ì‹œ importí•˜ì§€ ì•ŠìŒ)
                def validate_params(p):
                    """ì¬ê·€ì ìœ¼ë¡œ íŒŒë¼ë¯¸í„° ê²€ì¦"""
                    if isinstance(p, dict):
                        return all(validate_params(v) for v in p.values())
                    elif hasattr(p, 'shape') and hasattr(p, 'size'):
                        if p.size > 0:
                            if not jnp.all(jnp.isfinite(p)):
                                logger.warning(f"âš ï¸ ì´ˆê¸°í™” ì‹œ íŒŒë¼ë¯¸í„°ì— NaN/Inf ë°œê²¬: shape={p.shape}")
                                return False
                        return True
                    return True
                
                if not validate_params(self.model.get('params', {})):
                    logger.error("âŒ ëª¨ë¸ ì´ˆê¸°í™” í›„ íŒŒë¼ë¯¸í„° ê²€ì¦ ì‹¤íŒ¨")
                    raise ValueError("ëª¨ë¸ íŒŒë¼ë¯¸í„°ì— NaN/Inf í¬í•¨")
            except Exception as param_check_err:
                logger.warning(f"âš ï¸ íŒŒë¼ë¯¸í„° ê²€ì¦ ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œ): {param_check_err}")
                
        except Exception as e:
            logger.error(f"âŒ ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            import traceback
            logger.error(f"ì´ˆê¸°í™” ì‹¤íŒ¨ ìƒì„¸:\n{traceback.format_exc()}")
            raise
        
        # ì˜µí‹°ë§ˆì´ì € ì´ˆê¸°í™” (optax ì‚¬ìš© - Flax 0.10+ ë²„ì „)
        if optax is None:
            raise ImportError("optaxê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install optax")
        
        # ğŸ”¥ í•™ìŠµ ì„±ëŠ¥ ê°œì„ : í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ë§ ë° ì¡°ì •
        base_lr = self.train_config.get('lr', 0.0003)
        # í•™ìŠµë¥  ì¦ê°€: Neural networkê°€ í•™ìŠµí•˜ì§€ ëª»í•˜ëŠ” ë¬¸ì œ í•´ê²°
        # ì´ì „ 0.000075 (7.5e-5)ëŠ” ë„ˆë¬´ ë‚®ì•„ì„œ ì¡°ê¸° ì¢…ë£Œë¨
        learning_rate = base_lr  # 0.0003 (3e-4) - ì ì ˆí•œ í•™ìŠµì„ ìœ„í•´ ì¦ê°€
        
        # ğŸ”¥ í•™ìŠµë¥  ìë™ ì¡°ì •ì„ ìœ„í•œ ê¸°ë³¸ê°’ ì €ì¥
        self.base_learning_rate = learning_rate
        self.current_learning_rate = learning_rate

        # ğŸ”¥ Entropy coefficient ìë™ ì¡°ì •ì„ ìœ„í•œ ê¸°ë³¸ê°’ ì €ì¥
        base_entropy_coef = self.train_config.get('entropy_coef', 0.15)
        self.base_entropy_coef = base_entropy_coef
        self.current_entropy_coef = base_entropy_coef

        # ğŸ”¥ Gradient clipping ê°•í™” + í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ë§
        # optax.adamì€ learning_rateë¥¼ ìœ„ì¹˜ ì¸ìë¡œ ë°›ìŒ
        # ë” ì•ˆì •ì ì¸ í•™ìŠµì„ ìœ„í•´ gradient clipping ê°•í™”
        self.optimizer = optax.chain(
            optax.clip_by_global_norm(0.5),  # Gradient clipping ê°•í™” (ê¸°ì¡´ë³´ë‹¤ ë” ì‘ê²Œ)
            optax.scale_by_adam(),
            optax.scale(-learning_rate)  # í•™ìŠµë¥  ì ìš©
        )
        
        self.opt_state = self.optimizer.init(self.model['params'])
        self._step_count = 0  # ìŠ¤í… ì¹´ìš´í„°
        
        # í•™ìŠµ í†µê³„
        self.training_history = []

        # ì•¡ì…˜/ë³´ìƒ ë‹¤ì–‘ì„± ì„¤ì •
        self.neutral_target_ratio = float(self.train_config.get('neutral_target_ratio', NEUTRAL_TARGET_RATIO))
        self.min_neutral_samples = int(self.train_config.get('min_neutral_samples', MIN_NEUTRAL_SAMPLES))
        self.max_neutral_samples = int(self.train_config.get('max_neutral_samples', MAX_NEUTRAL_SAMPLES))
        self.max_direction_samples = int(self.train_config.get('max_direction_samples', MAX_DIRECTION_SAMPLES))
        self.neutral_action_bonus = float(self.train_config.get('neutral_action_bonus', NEUTRAL_ACTION_BONUS))
        self.direction_action_bonus = float(self.train_config.get('direction_action_bonus', UPDOWN_ACTION_BONUS))
        self.neutral_direction_threshold = float(
            self.train_config.get('neutral_direction_threshold', NEUTRAL_PRICE_CHANGE_THRESHOLD)
        )
        self.max_synthetic_neutral = int(
            self.train_config.get('max_synthetic_neutral', MAX_SYNTHETIC_NEUTRAL)
        )
        self.min_directional_samples = int(
            self.train_config.get('min_directional_samples', MIN_DIRECTIONAL_SAMPLES)
        )
        self.max_synthetic_directional = int(
            self.train_config.get('max_synthetic_directional', MAX_SYNTHETIC_DIRECTIONAL)
        )
        # ìµœì†Œ/ìµœëŒ€ ìœ íš¨ ë²”ìœ„ ë³´ì •
        if self.max_neutral_samples < self.min_neutral_samples:
            self.max_neutral_samples = self.min_neutral_samples
        self.neutral_target_ratio = max(0.05, min(0.5, self.neutral_target_ratio))
        self.neutral_direction_threshold = max(1e-4, self.neutral_direction_threshold)
        self.max_synthetic_neutral = max(0, self.max_synthetic_neutral)
        self.min_directional_samples = max(1, self.min_directional_samples)
        self.max_synthetic_directional = max(self.min_directional_samples, self.max_synthetic_directional)
        
        logger.info(f"âœ… PPO Trainer ì´ˆê¸°í™” ì™„ë£Œ (lr={learning_rate:.6f})")
    
    def train_from_selfplay_data(
        self,
        episodes_data: List[Dict[str, Any]],
        db_path: Optional[str] = None,
        analysis_data: Optional[Dict[str, Any]] = None  # ğŸ”¥ ì¶”ê°€: í†µí•© ë¶„ì„ ê²°ê³¼
    ) -> str:
        """
        Self-play ë°ì´í„°ë¡œ í•™ìŠµ (ë¶„ì„ ë°ì´í„° í¬í•¨ ê°€ëŠ¥)
        
        Args:
            episodes_data: Self-play ê²°ê³¼ ë°ì´í„°
            db_path: DB ê²½ë¡œ (ëª¨ë¸ ì €ì¥ìš©, Noneì´ë©´ configì—ì„œ ê°€ì ¸ì˜´)
            analysis_data: í†µí•© ë¶„ì„ ê²°ê³¼ (ì„ íƒì )
                {
                    'fractal_score': float,
                    'multi_timeframe_score': float,
                    'indicator_cross_score': float,
                    'ensemble_score': float,
                    'ensemble_confidence': float
                }
        
        Returns:
            model_id: í•™ìŠµëœ ëª¨ë¸ ID
        """
        try:
            logger.info(f"ğŸš€ PPO í•™ìŠµ ì‹œì‘: {len(episodes_data)}ê°œ ì—í”¼ì†Œë“œ" +
                       (", ë¶„ì„ ë°ì´í„° í¬í•¨" if analysis_data else ""))

            # ğŸ”¥ ë””ë²„ê±° ë¡œê¹…: í•™ìŠµ ì‹œì‘
            if self.debug:
                try:
                    self.debug.log_training_start({
                        "learning_rate": self.current_learning_rate,
                        "epochs": self.train_config.get('epochs', 30),
                        "batch_size": self.train_config.get('batch_size', 4096),
                        "num_episodes": len(episodes_data),
                        "has_analysis_data": analysis_data is not None,
                        "clip_epsilon": self.train_config.get('clip_epsilon', 0.2),
                        "entropy_coef": self.train_config.get('entropy_coef', 0.05),
                        "value_loss_coef": self.train_config.get('value_loss_coef', 0.5),
                        "gamma": self.train_config.get('gamma', 0.99),
                        "gae_lambda": self.train_config.get('gae_lambda', 0.95)
                    })
                except Exception as debug_err:
                    logger.debug(f"âš ï¸ í•™ìŠµ ì‹œì‘ ë””ë²„ê·¸ ë¡œê¹… ì‹¤íŒ¨ (ë¬´ì‹œ): {debug_err}")

            # 1. Self-play ë°ì´í„°ì—ì„œ ê²½í—˜ ì¶”ì¶œ (ë¶„ì„ ë°ì´í„° í¬í•¨)
            experiences = self._extract_experiences(episodes_data, analysis_data)
            # ğŸš€ ë©”íƒ€ í•™ìŠµ: ì „ëµ íŒŒë¼ë¯¸í„° í¬í•¨ (35ì°¨ì› or 30ì°¨ì›)
            feature_dim = FEATURE_DIM_WITH_ANALYSIS_AND_STRATEGY if analysis_data else FEATURE_DIM_WITH_STRATEGY
            logger.info(f"ğŸ“Š ì¶”ì¶œëœ ê²½í—˜: {len(experiences)}ê°œ (ì°¨ì›: {feature_dim})")

            if analysis_data:
                logger.info(f"ğŸš€ 35ì°¨ì› ë©”íƒ€ í•™ìŠµ í™œì„±í™” - í™•ì¥ ì§€í‘œ + ë¶„ì„ ì ìˆ˜ + ì „ëµ íŒŒë¼ë¯¸í„° í¬í•¨")
                logger.info(f"   í”„ë™íƒˆ: {analysis_data.get('fractal_score', 0.5):.3f}, "
                          f"ë©€í‹°TF: {analysis_data.get('multi_timeframe_score', 0.5):.3f}, "
                          f"ì§€í‘œêµì°¨: {analysis_data.get('indicator_cross_score', 0.5):.3f}")
            else:
                logger.info(f"ğŸš€ 30ì°¨ì› ë©”íƒ€ í•™ìŠµ (í™•ì¥ ì§€í‘œ + ì „ëµ íŒŒë¼ë¯¸í„° í¬í•¨, ë¶„ì„ ì ìˆ˜ ì—†ìŒ)")
            
            # ğŸ”¥ í•™ìŠµ ì „ ë°ì´í„° ê²€ì¦ ê°•í™” (ê°œì„ : ìµœì†Œ ìš”êµ¬ëŸ‰ ì™„í™”)
            if len(experiences) < 50:  # ğŸ”¥ ê°œì„ : 100 â†’ 50 (ë” ë¹ ë¥¸ í•™ìŠµ ì‹œì‘)
                logger.warning(f"âš ï¸ ê²½í—˜ ë°ì´í„°ê°€ ë§¤ìš° ë¶€ì¡±í•©ë‹ˆë‹¤ ({len(experiences)}ê°œ). ìµœì†Œ 50ê°œ ê¶Œì¥")
            elif len(experiences) < 100:
                logger.info(f"â„¹ï¸ ê²½í—˜ ë°ì´í„°: {len(experiences)}ê°œ (ê¶Œì¥: 100ê°œ ì´ìƒ)")
            
            # ì•¡ì…˜ ë‹¤ì–‘ì„± ìµœì¢… ê²€ì¦
            if experiences:
                actions = [exp.get('action', 0) for exp in experiences]
                unique_actions = set(actions)
                action_counts = {action: actions.count(action) for action in unique_actions}
                
                if len(unique_actions) < 2:
                    logger.error(f"âŒ í•™ìŠµ ì „ ê²€ì¦ ì‹¤íŒ¨: ì•¡ì…˜ ë‹¤ì–‘ì„± ë¶€ì¡± (ê³ ìœ  ì•¡ì…˜: {len(unique_actions)}ê°œ)")
                    logger.error(f"   ì•¡ì…˜ ë¶„í¬: {action_counts}")
                    logger.error(f"   í•™ìŠµì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤. ë°ì´í„° ìˆ˜ì§‘ ë¡œì§ì„ í™•ì¸í•˜ì„¸ìš”.")
                    raise ValueError(f"ì•¡ì…˜ ë‹¤ì–‘ì„± ë¶€ì¡±: ê³ ìœ  ì•¡ì…˜ {len(unique_actions)}ê°œë§Œ ì¡´ì¬ (ìµœì†Œ 2ê°œ í•„ìš”)")
                elif len(unique_actions) == 2:
                    logger.warning(f"âš ï¸ í•™ìŠµ ì „ ê²€ì¦: ì•¡ì…˜ ë‹¤ì–‘ì„± ì œí•œì  (ê³ ìœ  ì•¡ì…˜: {len(unique_actions)}ê°œ)")
                    logger.warning(f"   ì•¡ì…˜ ë¶„í¬: {action_counts}")
                    logger.warning(f"   í•™ìŠµì€ ê³„ì† ì§„í–‰ë˜ì§€ë§Œ, entropy_coefê°€ ìë™ìœ¼ë¡œ ì¦ê°€í•©ë‹ˆë‹¤.")
                else:
                    logger.info(f"âœ… í•™ìŠµ ì „ ê²€ì¦ í†µê³¼: ì•¡ì…˜ ë‹¤ì–‘ì„± ìš°ìˆ˜ (ê³ ìœ  ì•¡ì…˜: {len(unique_actions)}ê°œ)")
            else:
                logger.error("âŒ í•™ìŠµ ì „ ê²€ì¦ ì‹¤íŒ¨: ê²½í—˜ ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")
                raise ValueError("ê²½í—˜ ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")
            
            # 2. PPO ì—…ë°ì´íŠ¸
            epochs = self.train_config.get('epochs', 30)
            batch_size = self.train_config.get('batch_size', 4096)
            
            # ğŸ”¥ ì ì‘í˜• ë°°ì¹˜ í¬ê¸°: ë°ì´í„° í¬ê¸°ì— ë”°ë¼ ë™ì  ì¡°ì •
            # ë„ˆë¬´ ì‘ìœ¼ë©´ í•™ìŠµì´ ë¶ˆì•ˆì •í•˜ê³ , ë„ˆë¬´ í¬ë©´ ë©”ëª¨ë¦¬/ì»´íŒŒì¼ ë¬¸ì œ ë°œìƒ
            data_size = len(experiences)

            if data_size < 1000:
                optimal_batch_size = 64
            elif data_size < 5000:
                optimal_batch_size = 128
            else:
                # ğŸ”¥ ì„±ëŠ¥ ê°œì„ : ì•ˆì „ ì œí•œì„ ëŒ€í­ ì™„í™” (256 -> 2048)
                # ìµœì‹  GPUì—ì„œëŠ” ì¶©ë¶„íˆ ê°ë‹¹ ê°€ëŠ¥í•˜ë©° í•™ìŠµ ì†ë„ë¥¼ ìœ„í•´ í•„ìš”
                optimal_batch_size = 2048

            # ì„¤ì •ëœ batch_sizeì™€ optimal_batch_size ì¤‘ ì‘ì€ ê°’ ì‚¬ìš©
            if batch_size > optimal_batch_size:
                logger.info(f"ğŸ“Š ë°°ì¹˜ í¬ê¸° ìµœì í™”: {batch_size} â†’ {optimal_batch_size} (ë°ì´í„° í¬ê¸°: {data_size})")
                batch_size = optimal_batch_size
            
            # ê²½í—˜ ìˆ˜ì— ë”°ë¼ ë™ì  ì¡°ì •
            if len(experiences) < batch_size:
                # ë°ì´í„°ê°€ ì ìœ¼ë©´ ë°°ì¹˜ í¬ê¸° ì¤„ì´ê¸°
                batch_size = min(batch_size, len(experiences))
                if batch_size == 0:
                    logger.warning("âš ï¸ ê²½í—˜ ë°ì´í„° ì—†ìŒ, í•™ìŠµ ê±´ë„ˆëœ€")
                    return None
                logger.info(f"ğŸ“Š ë°°ì¹˜ í¬ê¸° ì¡°ì •: {batch_size} (ê²½í—˜ ìˆ˜: {len(experiences)})")
            
            eval_every = self.train_config.get('eval_every_epochs', 5)
            
            # ğŸ”¥ ì¡°ê¸° ì¢…ë£Œ ì„¤ì • (ì˜ˆì¸¡ ì „ëµ: ì•¡ì…˜ ë‹¤ì–‘ì„± ë¶€ì¡± ì‹œ ë” ë§ì€ epoch í—ˆìš©)
            # ê¸°ë³¸ê°’: 10 epoch (ì´ì „ 5ì—ì„œ ì¦ê°€) - ì˜ˆì¸¡ ì „ëµ í•™ìŠµì— ë” ë§ì€ ì‹œê°„ í•„ìš”
            early_stop_patience = self.train_config.get('early_stop_patience', 10)  # 10 epoch ë™ì•ˆ ê°œì„  ì—†ìœ¼ë©´ ì¢…ë£Œ
            early_stop_min_delta = self.train_config.get('early_stop_min_delta', 0.0005)  # ìµœì†Œ ê°œì„  ì„ê³„ê°’ (0.001 â†’ 0.0005ë¡œ ì™„í™”)
            best_loss = float('inf')
            no_improvement_count = 0
            
            # ğŸ”¥ ì´ˆê¸° ì†ì‹¤ ê¸°ë¡ (í•™ìŠµ ì „) - ê°œì„ : ë” ë‚˜ì€ ì´ˆê¸°í™”
            if experiences:
                # ë” ë§ì€ ìƒ˜í”Œë¡œ ì´ˆê¸° ì†ì‹¤ ì¸¡ì • (ì•ˆì •ì„± í–¥ìƒ)
                sample_size = min(200, len(experiences))
                sample_batch = experiences[:sample_size]
                
                # ì´ˆê¸° ì†ì‹¤ ì¸¡ì • (í•™ìŠµ ì „)
                if sample_batch:
                    try:
                        initial_loss = self._update_policy(sample_batch)
                        # ì´ˆê¸° ì†ì‹¤ì´ ë¹„ì •ìƒì ìœ¼ë¡œ í¬ë©´ ê²½ê³ 
                        if initial_loss > 10.0 or initial_loss < -10.0:
                            logger.warning(f"âš ï¸ ì´ˆê¸° ì†ì‹¤ì´ ë¹„ì •ìƒì : {initial_loss:.4f}, ì •ê·œí™” ì‹œë„")
                            # ì†ì‹¤ ì •ê·œí™” (í° ê°’ ì œí•œ)
                            initial_loss = max(-5.0, min(5.0, initial_loss))
                    except Exception as init_err:
                        logger.warning(f"âš ï¸ ì´ˆê¸° ì†ì‹¤ ì¸¡ì • ì‹¤íŒ¨: {init_err}, ê¸°ë³¸ê°’ ì‚¬ìš©")
                        initial_loss = 0.0
                else:
                    initial_loss = 0.0
                
                logger.info(f"ğŸ“Š ì´ˆê¸° ì†ì‹¤ (í•™ìŠµ ì „): {initial_loss:.4f} (ìƒ˜í”Œ: {sample_size}ê°œ)")
                best_loss = initial_loss
            else:
                logger.warning("âš ï¸ í•™ìŠµ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤ (experiencesê°€ ë¹„ì–´ìˆìŒ)")
                best_loss = 0.0
            
            for epoch in range(epochs):
                # ğŸ”¥ ë””ë²„ê±°: í˜„ì¬ epoch ì„¤ì • (ë°°ì¹˜ ë¡œê¹…ì—ì„œ ì‚¬ìš©)
                if self.debug:
                    self._debug_current_epoch = epoch + 1
                    self._debug_batch_idx = 0  # ë°°ì¹˜ ì¸ë±ìŠ¤ ì´ˆê¸°í™”

                # ë°°ì¹˜ ìƒì„±
                batches = self._create_batches(experiences, batch_size)

                # ğŸ”¥ ë””ë²„ê±°: ì´ ë°°ì¹˜ ìˆ˜ ì„¤ì •
                if self.debug:
                    self._debug_total_batches = len(batches)

                # ğŸ”¥ ë””ë²„ê±° ë¡œê¹…: Epoch ì‹œì‘
                if self.debug:
                    try:
                        self.debug.log_epoch_start(
                            epoch=epoch + 1,
                            total_epochs=epochs,
                            learning_rate=self.current_learning_rate
                        )
                    except Exception as debug_err:
                        logger.debug(f"âš ï¸ Epoch ì‹œì‘ ë””ë²„ê·¸ ë¡œê¹… ì‹¤íŒ¨ (ë¬´ì‹œ): {debug_err}")

                epoch_loss = 0.0
                successful_updates = 0
                # ê° ë°°ì¹˜ë¡œ ì—…ë°ì´íŠ¸
                for batch_idx, batch in enumerate(batches):
                    try:
                        loss = self._update_policy(batch)
                        if loss is not None and not (isinstance(loss, float) and (loss != loss or loss == float('inf'))):  # NaN/Inf ì²´í¬
                            epoch_loss += loss
                            successful_updates += 1
                        else:
                            logger.debug(f"âš ï¸ Epoch {epoch+1}, Batch {batch_idx+1}: Loss ê°’ ì´ìƒ (NaN/Inf), ìŠ¤í‚µ")
                    except Exception as batch_err:
                        logger.warning(f"âš ï¸ Epoch {epoch+1}, Batch {batch_idx+1} ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {batch_err}")
                        # ê³„ì† ì§„í–‰ (ë‹¤ìŒ ë°°ì¹˜ ì²˜ë¦¬)
                        continue
                
                # ì„±ê³µí•œ ì—…ë°ì´íŠ¸ê°€ ì—†ìœ¼ë©´ ê²½ê³ 
                if successful_updates == 0:
                    logger.warning(f"âš ï¸ Epoch {epoch+1}: ëª¨ë“  ë°°ì¹˜ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨")
                
                avg_loss = epoch_loss / successful_updates if successful_updates > 0 else 0.0
                
                # ì†ì‹¤ ë³€í™” ì¶”ì 
                if epoch == 0:
                    self._initial_epoch_loss = avg_loss

                # ğŸ”¥ ì¡°ê¸° ì¢…ë£Œ ì²´í¬
                improved = False
                if avg_loss < best_loss - early_stop_min_delta:
                    # ê°œì„ ë¨
                    best_loss = avg_loss
                    no_improvement_count = 0
                    improvement_msg = "âœ… ê°œì„ "
                    improved = True
                else:
                    # ê°œì„  ì—†ìŒ
                    no_improvement_count += 1
                    improvement_msg = f"âš ï¸ ê°œì„  ì—†ìŒ ({no_improvement_count}/{early_stop_patience})"

                # ğŸ”¥ ë””ë²„ê±° ë¡œê¹…: Epoch ì¢…ë£Œ
                if self.debug:
                    try:
                        self.debug.log_epoch_end(
                            epoch=epoch + 1,
                            avg_loss=avg_loss,
                            best_loss=best_loss,
                            improved=improved,
                            no_improvement_count=no_improvement_count,
                            learning_rate=self.current_learning_rate
                        )
                    except Exception as debug_err:
                        logger.debug(f"âš ï¸ Epoch ì¢…ë£Œ ë””ë²„ê·¸ ë¡œê¹… ì‹¤íŒ¨ (ë¬´ì‹œ): {debug_err}")

                if epoch == epochs - 1:
                    loss_change = avg_loss - self._initial_epoch_loss
                    loss_change_pct = (loss_change / self._initial_epoch_loss * 100) if self._initial_epoch_loss > 0 else 0.0
                    logger.info(f"ğŸ“ˆ Epoch {epoch+1}/{epochs}: í‰ê·  Loss = {avg_loss:.4f} "
                              f"(ë³€í™”: {loss_change:+.4f}, {loss_change_pct:+.2f}%) {improvement_msg}")
                else:
                    logger.info(f"ğŸ“ˆ Epoch {epoch+1}/{epochs}: í‰ê·  Loss = {avg_loss:.4f} {improvement_msg}")

                # ğŸ”¥ í•™ìŠµë¥  ìë™ ì¡°ì • (ê°œì„  ì—†ì„ ë•Œ)
                if no_improvement_count >= 3 and no_improvement_count % 2 == 1:  # 3, 5, 7... epochë§ˆë‹¤
                    # í•™ìŠµë¥  10% ê°ì†Œ
                    self.current_learning_rate *= 0.9
                    # ì˜µí‹°ë§ˆì´ì € ì¬ìƒì„± (í•™ìŠµë¥  ë³€ê²½)
                    self.optimizer = optax.chain(
                        optax.clip_by_global_norm(0.5),
                        optax.scale_by_adam(),
                        optax.scale(-self.current_learning_rate)
                    )
                    logger.info(f"ğŸ“‰ í•™ìŠµë¥  ìë™ ì¡°ì •: {self.current_learning_rate:.6f} (ê°œì„  ì—†ìŒ {no_improvement_count}íšŒ)")

                # ğŸ”¥ ì¡°ê¸° ì¢…ë£Œ ì²´í¬
                if no_improvement_count >= early_stop_patience:
                    logger.warning(f"ğŸ›‘ ì¡°ê¸° ì¢…ë£Œ: {early_stop_patience} epoch ë™ì•ˆ ê°œì„  ì—†ìŒ (ìµœê³  Loss: {best_loss:.4f})")
                    break

                # í‰ê°€ (ì£¼ê¸°ì ìœ¼ë¡œ)
                if (epoch + 1) % eval_every == 0:
                    eval_result = self._evaluate_model(epoch, experiences[:100])
                    logger.info(f"ğŸ“Š Epoch {epoch+1} í‰ê°€: í‰ê·  ë³´ìƒ = {eval_result.get('avg_reward', 0.0):.4f}")

                # í•™ìŠµ íˆìŠ¤í† ë¦¬ ì €ì¥
                self.training_history.append({
                    'epoch': epoch + 1,
                    'loss': avg_loss,
                    'experiences_count': len(experiences)
                })
            
            # ğŸ”¥ ë””ë²„ê±° ë¡œê¹…: í•™ìŠµ ì¢…ë£Œ
            if self.debug:
                try:
                    # ìˆ˜ë ´ ì—¬ë¶€ íŒë‹¨ (ì¡°ê¸° ì¢…ë£Œ ë˜ëŠ” ìµœì¢… lossê°€ ì¶©ë¶„íˆ ë‚®ìŒ)
                    converged = (no_improvement_count >= early_stop_patience) or (avg_loss < 0.01)

                    self.debug.log_training_end(
                        total_epochs=len(self.training_history),
                        best_loss=best_loss,
                        final_loss=avg_loss if 'avg_loss' in locals() else best_loss,
                        converged=converged
                    )
                except Exception as debug_err:
                    logger.debug(f"âš ï¸ í•™ìŠµ ì¢…ë£Œ ë””ë²„ê·¸ ë¡œê¹… ì‹¤íŒ¨ (ë¬´ì‹œ): {debug_err}")

            # 3. ëª¨ë¸ ì €ì¥
            if db_path is None:
                db_path = self.config.get('paths', {}).get('db', None)

            model_id = self._save_model(db_path)
            logger.info(f"âœ… í•™ìŠµ ì™„ë£Œ: model_id={model_id}")

            return model_id
            
        except Exception as e:
            logger.error(f"âŒ í•™ìŠµ ì‹¤íŒ¨: {e}")
            raise
    
    def _extract_experiences(
        self, 
        episodes_data: List[Dict[str, Any]],
        analysis_data: Optional[Dict[str, Any]] = None
    ) -> List[Dict[str, Any]]:
        """
        Self-play ê²°ê³¼ì—ì„œ ê²½í—˜ ì¶”ì¶œ (ë¶„ì„ ë°ì´í„° í¬í•¨ ë²„ì „)
        
        Args:
            episodes_data: Self-play ì—í”¼ì†Œë“œ ë°ì´í„°
            analysis_data: í†µí•© ë¶„ì„ ê²°ê³¼ (ì„ íƒì )
                {
                    'fractal_score': float,
                    'multi_timeframe_score': float,
                    'indicator_cross_score': float,
                    'ensemble_score': float,
                    'ensemble_confidence': float
                }
        
        Returns:
            ê²½í—˜ ë¦¬ìŠ¤íŠ¸ [{state, action, reward, log_prob, value, done}, ...]
        """
        # ë¶„ì„ ë°ì´í„°ê°€ ìˆìœ¼ë©´ í™•ì¥ ë²„ì „ ì‚¬ìš©
        if analysis_data:
            return self._extract_experiences_with_analysis(episodes_data, analysis_data)
        
        # ê¸°ë³¸ ë²„ì „ (í•˜ìœ„ í˜¸í™˜ì„±)
        return self._extract_experiences_basic(episodes_data)
    
    def _extract_experiences_basic(self, episodes_data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        Self-play ê²°ê³¼ì—ì„œ ê²½í—˜ ì¶”ì¶œ (ê¸°ë³¸ ë²„ì „ - 20ì°¨ì›: ê¸°ë³¸ 15ê°œ + í™•ì¥ ì§€í‘œ 5ê°œ)
        """
        experiences = []

        try:
            for episode in episodes_data:
                results = episode.get('results', {})
                episode_num = episode.get('episode', 0)

                for agent_id, agent_result in results.items():
                    # ì—ì´ì „íŠ¸ë³„ ì„±ê³¼ì—ì„œ ê²½í—˜ ì¶”ì¶œ
                    trades = agent_result.get('trades', [])
                    total_pnl = agent_result.get('total_pnl', 0.0)
                    win_rate = agent_result.get('win_rate', 0.0)
                    total_trades = agent_result.get('total_trades', 0)  # ğŸ”¥ total_trades ì¶”ì¶œ
                    profit_factor = agent_result.get('profit_factor', 0.0)
                    strategy_direction = agent_result.get('strategy_direction', 'neutral')  # ğŸ”¥ ì „ëµ ë°©í–¥ ('buy', 'sell', 'neutral')
                    predicted_conf = agent_result.get('predicted_conf', 0.5)  # ğŸ”¥ ì˜ˆì¸¡ ì‹ ë¢°ë„
                    strategy_params = agent_result.get('strategy_params', {})  # ğŸš€ ë©”íƒ€ í•™ìŠµ: ì „ëµ íŒŒë¼ë¯¸í„°

                    # ğŸ”¥ ì˜ˆì¸¡ ì „ëµ: ê±°ë˜ê°€ ì—†ì–´ë„ í•™ìŠµ ë°ì´í„°ë¡œ í™œìš©
                    # ì˜ˆì¸¡ ì „ëµì€ ê±°ë˜ ê²°ê³¼ë³´ë‹¤ ì˜ˆì¸¡ ì •í™•ë„ê°€ ì¤‘ìš”í•˜ë¯€ë¡œ í•„í„° ì™„í™”
                    # ê±°ë˜ê°€ ì—†ëŠ” ê²½ìš°ì—ë„ ì‹œì¥ ìƒíƒœ ê¸°ë°˜ ì˜ˆì¸¡ ê²½í—˜ ìƒì„±
                    
                    # í’ˆì§ˆ í•„í„°ë§ ì™„í™”: ê±°ë˜ê°€ ì—†ì–´ë„ í•™ìŠµì— í¬í•¨
                    # (ì˜ˆì¸¡ ì „ëµì€ ê±°ë˜ ì‹¤í–‰ ì—¬ë¶€ë³´ë‹¤ ì˜ˆì¸¡ ì •í™•ë„ê°€ ì¤‘ìš”)
                    quality_check_passed = True  # ëª¨ë“  ì—í”¼ì†Œë“œ í¬í•¨
                    
                    # ğŸ”¥ í•™ìŠµ ì„±ëŠ¥ ê°œì„ : íŠ¸ë ˆì´ë“œë³„ ê²½í—˜ ìƒì„± (ì•¡ì…˜ ë‹¤ì–‘ì„± ìš°ì„ )
                    if trades:
                        # ğŸ”¥ ì˜ˆì¸¡ ì „ëµ: UP/DOWN ì˜ˆì¸¡ì„ ìš°ì„ ì ìœ¼ë¡œ ìˆ˜ì§‘ (NEUTRALì€ ì œí•œ)
                        # ê±°ë˜ ë°ì´í„°ì˜ BUY/SELL/HOLDë¥¼ ì˜ˆì¸¡ ë°©í–¥ìœ¼ë¡œ ë³€í™˜
                        buy_trades = [t for t in trades if t.get('direction') == 'BUY']  # â†’ UP ì˜ˆì¸¡
                        sell_trades = [t for t in trades if t.get('direction') == 'SELL']  # â†’ DOWN ì˜ˆì¸¡
                        hold_trades = [t for t in trades if t.get('direction') != 'BUY' and t.get('direction') != 'SELL']  # â†’ NEUTRAL ì˜ˆì¸¡
                        hold_trades = self._ensure_neutral_trade_pool(
                            buy_trades,
                            sell_trades,
                            hold_trades,
                            episode_num=episode_num,
                            agent_id=agent_id
                        )
                        buy_trades, sell_trades = self._ensure_directional_trade_pool(
                            buy_trades,
                            sell_trades,
                            hold_trades,
                            episode_num=episode_num,
                            agent_id=agent_id
                        )
                        
                        selected_trades = self._select_trades_with_diversity(buy_trades, sell_trades, hold_trades)
                        
                        # ê° íŠ¸ë ˆì´ë“œì—ì„œ ê²½í—˜ ì¶”ì¶œ
                        for trade in selected_trades:
                            # ğŸ”¥ ì¸í„°ë²Œ ì—­í• ë³„ ë³´ìƒ ê°€ì¤‘ì¹˜ ì„¤ì •
                            interval_role_weights = {
                                'Macro Regime': {'direction': 2.0, 'hold': 0.5, 'profit': 1.0},  # ë°©í–¥ì„± ì •í™•ë„ê°€ ê°€ì¥ ì¤‘ìš”
                                'Trend Structure': {'direction': 1.5, 'hold': 0.8, 'profit': 1.2},  # ì¶”ì„¸ íŒŒì•… ì¤‘ìš”
                                'Micro Trend': {'direction': 1.2, 'hold': 1.0, 'profit': 1.5},  # ë‹¨ê¸° ì¶”ì„¸ ë° ìˆ˜ìµ ì¤‘ìš”
                                'Execution': {'direction': 1.0, 'hold': 1.2, 'profit': 2.0},  # ìˆ˜ìµ ì‹¤í˜„(íƒ€ì´ë°)ì´ ê°€ì¥ ì¤‘ìš”
                                'Timing': {'direction': 1.0, 'hold': 1.2, 'profit': 2.0}  # Executionê³¼ ë™ì¼
                            }
                            # interval_roleì´ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’(Execution) ì‚¬ìš©
                            role_weight = interval_role_weights.get(interval_role, interval_role_weights['Execution'])

                            # Market state ì¬êµ¬ì„± (í™•ì¥ ì§€í‘œ í¬í•¨)
                            # ì‹¤ì œë¡œëŠ” tradeì— state ì •ë³´ê°€ í¬í•¨ë˜ì–´ì•¼ í•¨
                            state = {
                                'rsi': trade.get('rsi', 50.0),
                                'macd': trade.get('macd', 0.0),
                                'volume_ratio': trade.get('volume_ratio', 1.0),
                                'atr': trade.get('atr', 0.02),
                                'adx': trade.get('adx', 25.0),
                                'mfi': trade.get('mfi', 50.0),
                                'bb_upper': trade.get('bb_upper', 1.0),
                                'bb_middle': trade.get('bb_middle', 1.0),
                                'bb_lower': trade.get('bb_lower', 1.0),
                                'macd_signal': trade.get('macd_signal', 0.0),
                                'close': trade.get('close', 1.0),
                                'open': trade.get('open', 1.0),
                                'high': trade.get('high', 1.0),
                                'low': trade.get('low', 1.0),
                                'volume': trade.get('volume', 1.0),
                                'volatility': trade.get('volatility', 0.02),
                                'regime_stage': trade.get('regime_stage', 3),
                                'regime_confidence': trade.get('regime_confidence', 0.5),
                                # ğŸš€ í™•ì¥ ì§€í‘œ ì¶”ê°€ (1ë‹¨ê³„ í™•ì¥)
                                'wave_progress': trade.get('wave_progress', 0.5),
                                'pattern_confidence': trade.get('pattern_confidence', 0.5),
                                'structure_score': trade.get('structure_score', 0.5),
                                'sentiment': trade.get('sentiment', 0.0),
                                'regime_transition_prob': trade.get('regime_transition_prob', 0.05)
                            }
                            
                            # ğŸ”¥ ì˜ˆì¸¡ ì „ëµ: ì•¡ì…˜ì„ ë°©í–¥ ì˜ˆì¸¡ìœ¼ë¡œ ë³€í™˜
                            # BUY â†’ UP(1): ìƒìŠ¹ ì˜ˆì¸¡, SELL â†’ DOWN(2): í•˜ë½ ì˜ˆì¸¡, HOLD â†’ NEUTRAL(0): ì¤‘ë¦½ ì˜ˆì¸¡
                            trade_direction = trade.get('direction', 'HOLD')
                            if trade_direction == 'BUY':
                                action = 1  # UP: ìƒìŠ¹ ì˜ˆì¸¡
                                predicted_direction = 'UP'
                            elif trade_direction == 'SELL':
                                action = 2  # DOWN: í•˜ë½ ì˜ˆì¸¡
                                predicted_direction = 'DOWN'
                            else:
                                action = 0  # NEUTRAL: ì¤‘ë¦½ ì˜ˆì¸¡
                                predicted_direction = 'NEUTRAL'
                            
                            # ğŸ”¥ ì˜ˆì¸¡ ì •í™•ë„ ê¸°ë°˜ ë³´ìƒ ì‹œìŠ¤í…œ (HOLDë¥¼ ì‹¤ì œ ë°©í–¥ìœ¼ë¡œ ì¬í‰ê°€)
                            # ì‹¤ì œ ê°€ê²© ë³€í™”ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì˜ˆì¸¡ ì •í™•ë„ í‰ê°€
                            price_change = float(trade.get('price_change', 0.0) or 0.0)  # ì‹¤ì œ ê°€ê²© ë³€í™”ìœ¨
                            threshold = self.neutral_direction_threshold
                            actual_direction = (
                                'UP' if price_change > threshold
                                else ('DOWN' if price_change < -threshold else 'NEUTRAL')
                            )

                            # ğŸ”¥ ì¤‘ìš” ìˆ˜ì •: ì˜ˆì¸¡ ëª©í‘œ ê°•í™” (Hindsight Labeling)
                            # ì „ëµì´ HOLD í–ˆë”ë¼ë„, ì‹œì¥ì´ ì›€ì§ì˜€ë‹¤ë©´ ê·¸ ì›€ì§ì„ì„ ì •ë‹µìœ¼ë¡œ í•™ìŠµ
                            # ì „ëµì˜ ì†Œê·¹ì  íƒœë„(Risk Aversion)ê°€ ì˜ˆì¸¡ ëŠ¥ë ¥ ì €í•˜ë¡œ ì´ì–´ì§€ì§€ ì•Šë„ë¡ í•¨
                            
                            if predicted_direction == 'NEUTRAL' and actual_direction != 'NEUTRAL':
                                # ì „ëµì€ ê´€ë§í–ˆì§€ë§Œ ì‹œì¥ì€ ì›€ì§ì„ -> ì‹¤ì œ ë°©í–¥ìœ¼ë¡œ ë¼ë²¨ ìˆ˜ì • (Oracle Learning)
                                if actual_direction == 'UP':
                                    action = 1 # UP
                                    predicted_direction = 'UP'
                                elif actual_direction == 'DOWN':
                                    action = 2 # DOWN
                                    predicted_direction = 'DOWN'
                                # NEUTRAL ë¼ë²¨ì„ ì œê±°í•˜ê³  ë°©í–¥ì„± ë¼ë²¨ë¡œ ëŒ€ì²´í•˜ì—¬ ì ê·¹ì  ì˜ˆì¸¡ ìœ ë„
                            
                            # ğŸ”¥ ì¤‘ìš” ìˆ˜ì •: ì „ëµì˜ ì¡°ê¸° ì²­ì‚°(Take-Profit/Stop-Loss)ìœ¼ë¡œ ì¸í•œ ì˜ˆì¸¡ ì™œê³¡ ë°©ì§€
                            # ì „ëµì´ 20% ìƒìŠ¹ì„ ëª©í‘œë¡œ í–ˆìœ¼ë‚˜ 5%ì—ì„œ ìµì ˆí–ˆë‹¤ë©´, ì‹¤ì œë¡œëŠ” ë” ì˜¬ë¼ê°”ì„ ìˆ˜ ìˆìŒ
                            # ë”°ë¼ì„œ 'UP' ì˜ˆì¸¡ì´ì—ˆëŠ”ë° ìµì ˆë¡œ ëë‚œ ê²½ìš°, ì´í›„ ê°€ê²© ì¶”ì´(ì ì¬ì  ìµœëŒ€ ë³€ë™í­)ë¥¼ ê³ ë ¤í•´ì•¼ í•˜ì§€ë§Œ,
                            # í˜„ì¬ trade ì •ë³´ë§Œìœ¼ë¡œëŠ” ì•Œ ìˆ˜ ì—†ìœ¼ë¯€ë¡œ, 'ì´ìµì´ ë‚¬ë‹¤'ëŠ” ì‚¬ì‹¤ ìì²´ë¥¼ ê¸ì •ì ìœ¼ë¡œ í‰ê°€.
                            
                            # ë˜í•œ, ê°•ì œ ì²­ì‚°(Stop Loss)ì˜ ê²½ìš°ì—ë„ ë°©í–¥ ì˜ˆì¸¡ì€ ë§ì•˜ìœ¼ë‚˜ ë³€ë™í­ì´ ì»¤ì„œ í„¸ë¦° ê²½ìš°ì¼ ìˆ˜ ìˆìŒ.
                            # í•˜ì§€ë§Œ ì˜ˆì¸¡ ê´€ì ì—ì„œëŠ” 'ê²°ê³¼ì ì¸ ê°€ê²© ë³€í™”'ê°€ ì¤‘ìš”í•˜ë¯€ë¡œ actual_directionì„ ë”°ë¥´ëŠ” ê²ƒì´ ê¸°ë³¸.
                            
                            # ë‹¨, 'UP' ì˜ˆì¸¡ì„ í–ˆëŠ”ë° ì‹¤ì œë¡œëŠ” 'NEUTRAL' ìˆ˜ì¤€ì˜ ì‘ì€ ì´ìµë§Œ ë³´ê³  ëë‚œ ê²½ìš° (ì¡°ê¸° ìµì ˆ),
                            # ì´ë¥¼ 'í‹€ë¦¼'ìœ¼ë¡œ ì²˜ë¦¬í•˜ë©´ ì–µìš¸í•¨. ì´ìµì´ ë‚¬ë‹¤ë©´(price_change > 0) UP ì˜ˆì¸¡ì— ëŒ€í•´ ë¶€ë¶„ ì ìˆ˜ ë¶€ì—¬.
                            
                            # ğŸš€ 1. ë³´ìƒ ê¸°ë³¸ ì„¤ì •
                            # ğŸ”¥ NEUTRAL(0)ë„ ëª…í™•í•œ ì˜ˆì¸¡(ë°©í–¥ì„± ì—†ìŒ/ê´€ë§)ìœ¼ë¡œ í‰ê°€
                            direction_reward = 0.0
                            
                            if predicted_direction == actual_direction:
                                # 1. ì™„ì „ ì¼ì¹˜
                                if predicted_direction == 'UP':
                                    direction_reward = 1.5 * role_weight['direction'] # ìƒìŠ¹ì¥ ì˜ˆì¸¡ ì„±ê³µì€ ë†’ì€ ë³´ìƒ
                                elif predicted_direction == 'DOWN':
                                    direction_reward = 1.5 * role_weight['direction'] # í•˜ë½ì¥ ì˜ˆì¸¡ ì„±ê³µë„ ë†’ì€ ë³´ìƒ
                                else: # NEUTRAL
                                    # ğŸ”¥ NEUTRALë„ "ë°©í–¥ì„± ì—†ìŒ"ì„ ë§ì¶˜ ê²ƒì´ë¯€ë¡œ ë³´ìƒ ë¶€ì—¬
                                    # ë‹¨, ì ê·¹ì  ì˜ˆì¸¡ë³´ë‹¤ëŠ” ì•½ê°„ ë‚®ê²Œ (ë„ˆë¬´ ì†Œê·¹ì ì´ì§€ ì•Šë„ë¡)
                                    direction_reward = 0.8 * role_weight['hold'] 
                            
                            elif predicted_direction == 'UP':
                                # 2. ìƒìŠ¹ ì˜ˆì¸¡í–ˆìœ¼ë‚˜...
                                if actual_direction == 'NEUTRAL' and price_change > 0:
                                    # ì¡°ê¸ˆì´ë¼ë„ ì˜¬ëìœ¼ë©´ ë¶€ë¶„ ì ìˆ˜ (0.5 -> 0.3ìœ¼ë¡œ ì¡°ì •í•˜ì—¬ ì •í™•ì„± ìœ ë„)
                                    direction_reward = 0.3 * role_weight['direction']
                                elif actual_direction == 'DOWN':
                                    # ë°˜ëŒ€ë¡œ ê° -> ê°•í•œ í˜ë„í‹°
                                    direction_reward = -1.2 * role_weight['direction']
                                else:
                                    direction_reward = -0.5 * role_weight['direction']
                                    
                            elif predicted_direction == 'DOWN':
                                # 3. í•˜ë½ ì˜ˆì¸¡í–ˆìœ¼ë‚˜...
                                if actual_direction == 'NEUTRAL' and price_change < 0:
                                    # ì¡°ê¸ˆì´ë¼ë„ ë‚´ë ¸ìœ¼ë©´ ë¶€ë¶„ ì ìˆ˜
                                    direction_reward = 0.3 * role_weight['direction']
                                elif actual_direction == 'UP':
                                    # ë°˜ëŒ€ë¡œ ê° -> ê°•í•œ í˜ë„í‹°
                                    direction_reward = -1.2 * role_weight['direction']
                                else:
                                    direction_reward = -0.5 * role_weight['direction']
                                    
                            else: # predicted == NEUTRAL
                                # 4. ì¤‘ë¦½ ì˜ˆì¸¡í–ˆìœ¼ë‚˜...
                                # ë°©í–¥ì„±ì¥(UP/DOWN)ì—ì„œ ê´€ë§ì€ "ê¸°íšŒ ì†ì‹¤" -> í˜ë„í‹°
                                if actual_direction == 'UP' or actual_direction == 'DOWN':
                                    direction_reward = -0.8 * role_weight['hold']
                                else:
                                    # ì• ë§¤í•œ ìƒí™©ì—ì„œ ê´€ë§ -> ì¤‘ë¦½ ë³´ìƒ (0.0)
                                    direction_reward = 0.0

                            # ğŸš€ 3. ì¸í„°ë²Œ ì—­í• ë³„ ì¶”ê°€ ë³´ì •
                            # ìœ„ì—ì„œ role_weightë¥¼ ì´ë¯¸ ê³±í–ˆìœ¼ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” íŠ¹ìˆ˜ ìƒí™©ë§Œ ì²˜ë¦¬
                            
                            # ì˜ˆì¸¡ ì‹ ë¢°ë„ ê¸°ë°˜ ë³´ì • (win_rate í™œìš©)
                            confidence_bonus = (win_rate - 0.5) * 0.5  # -0.25 ~ +0.25
                            
                            # ìµœì¢… ë³´ìƒ: ì˜ˆì¸¡ ì •í™•ë„ + ì‹ ë¢°ë„ ë³´ë„ˆìŠ¤
                            reward = direction_reward + confidence_bonus

                            # ğŸ†• Policy Collapse ë°©ì§€: NEUTRAL ì•¡ì…˜ ë³´ë„ˆìŠ¤ ì œê±°
                            # ì´ì œ NEUTRALë„ ì •ë‹¹í•œ ì˜ˆì¸¡ìœ¼ë¡œ í‰ê°€ë°›ìœ¼ë¯€ë¡œ ì¸ìœ„ì  ë³´ë„ˆìŠ¤ ë¶ˆí•„ìš”
                            # ëŒ€ì‹  ë°©í–¥ì„± ì˜ˆì¸¡(UP/DOWN)ì— ì•½ê°„ì˜ ì¸ì„¼í‹°ë¸Œë¥¼ ì£¼ì–´ ì ê·¹ì„± ìœ ë„
                            if predicted_direction != 'NEUTRAL':
                                reward += 0.1  # ì ê·¹ì  ì˜ˆì¸¡ ì¸ì„¼í‹°ë¸Œ

                            # ğŸ†• Policy Collapse ë°©ì§€: NEUTRAL ì•¡ì…˜ ë³´ë„ˆìŠ¤ ì¶”ê°€
                            # NEUTRAL ì•¡ì…˜ì´ ì ê²Œ ì„ íƒë˜ëŠ” ê²½ìš° ë³´ë„ˆìŠ¤ ì œê³µ
                            if predicted_direction == 'NEUTRAL':
                                reward += self.neutral_action_bonus
                            else:
                                reward += self.direction_action_bonus
                            
                            # ê¸°ë³¸ log_prob (ê· ë“± ë¶„í¬ ê°€ì •: log(1/3) â‰ˆ -1.1)
                            log_prob = -1.1
                            
                            # ê¸°ë³¸ value estimate (ë³´ìƒ ê¸°ë°˜)
                            value = reward * 0.9  # ê°„ë‹¨í•œ ì¶”ì •

                            # ğŸš€ ë©”íƒ€ í•™ìŠµ: ìƒíƒœ ë²¡í„° ìƒì„± (30ì°¨ì›: 20 base + 10 strategy params)
                            state_vec = build_state_vector_with_strategy(state, strategy_params)
                            
                            experience = {
                                'episode': episode_num,
                                'agent_id': agent_id,
                                'state': state_vec,  # ğŸ”¥ numpy arrayë¡œ ë³€í™˜
                                'action': action,
                                'reward': reward,
                                'log_prob': log_prob,
                                'value': value,
                                'done': False  # ë‹¨ì¼ íŠ¸ë ˆì´ë“œëŠ” ì™„ë£Œë¡œ ê°„ì£¼
                            }
                            experiences.append(experience)
                        
                        # ğŸ”¥ UP/DOWN ì˜ˆì¸¡ tradesê°€ ìˆìœ¼ë©´ ë” ì´ìƒ ì¶”ê°€í•˜ì§€ ì•ŠìŒ (ì˜ˆì¸¡ ë‹¤ì–‘ì„± í™•ë³´) -> ì œê±°: ëª¨ë“  ì„ íƒëœ trades í™œìš©
                        # break  # ëª¨ë“  ì„ íƒëœ íŠ¸ë ˆì´ë“œë¥¼ í™œìš©í•˜ì—¬ ì•¡ì…˜ ë‹¤ì–‘ì„± í™•ë³´

                    
                    # ğŸ”¥ tradesê°€ ì—†ëŠ” ê²½ìš°: total_tradesê°€ ìˆìœ¼ë©´ ê²½í—˜ ìƒì„±
                    elif total_trades > 0:
                        # total_tradesëŠ” ìˆì§€ë§Œ trades ë¦¬ìŠ¤íŠ¸ëŠ” ì—†ëŠ” ê²½ìš°
                        # (ì˜ˆì¸¡ self-playì—ì„œ predictionsë¥¼ tradesë¡œ ë³€í™˜í–ˆì§€ë§Œ ì‹¤ì œ ê±°ë˜ëŠ” ì—†ìŒ)
                        # ğŸ”¥ ì „ëµ ë°©í–¥ì„±ê³¼ ì˜ˆì¸¡ ì‹ ë¢°ë„ë¥¼ í™œìš©í•˜ì—¬ ë‹¤ì–‘í•œ ì•¡ì…˜ ìƒì„±
                        # ğŸ”¥ í•©ì„± ë°ì´í„° ìƒì„±ëŸ‰ ì¦ê°€: ìµœì†Œ 5ê°œ ë³´ì¥, ìµœëŒ€ 20ê°œ (í•™ìŠµ ë°ì´í„° ì¦ê°€)
                        num_experiences = max(5, min(total_trades, 20))

                        # ğŸ”¥ ì „ëµ ë°©í–¥ì„± ê¸°ë°˜ ì•¡ì…˜ ë¶„í¬ ìƒì„± (ë‹¤ì–‘ì„± í™•ë³´ - ê° ë°©í–¥ ìµœì†Œ 1ê°œ ë³´ì¥)
                        # ê° ì•¡ì…˜ì„ ì•ì— ë°°ì¹˜í•˜ì—¬ num_experiencesê°€ ì‘ì•„ë„ ëª¨ë“  ì•¡ì…˜ í¬í•¨ ë³´ì¥
                        if strategy_direction == 'buy':
                            # ë§¤ìˆ˜ ì „ëµ: UP ìš°ì„¸, DOWN/NEUTRAL í¬í•¨ (ë‹¤ì–‘ì„±)
                            # ìµœì†Œ ê° ë°©í–¥ 1ê°œì”© ë¨¼ì € ë³´ì¥ (ì• 3ê°œ), ë‚˜ë¨¸ì§€ UP ìœ„ì£¼ë¡œ ì±„ì›€
                            action_distribution = [0, 1, 2] + [1] * 4 + [2] * 2 + [1]  # NEUTRAL, UP, DOWN ê° 1ê°œ ë¨¼ì €, ë‚˜ë¨¸ì§€ UP ìœ„ì£¼
                        elif strategy_direction == 'sell':
                            # ë§¤ë„ ì „ëµ: DOWN ìš°ì„¸, UP/NEUTRAL í¬í•¨ (ë‹¤ì–‘ì„±)
                            # ìµœì†Œ ê° ë°©í–¥ 1ê°œì”© ë¨¼ì € ë³´ì¥ (ì• 3ê°œ), ë‚˜ë¨¸ì§€ DOWN ìœ„ì£¼ë¡œ ì±„ì›€
                            action_distribution = [0, 1, 2] + [2] * 4 + [1] * 2 + [2]  # NEUTRAL, UP, DOWN ê° 1ê°œ ë¨¼ì €, ë‚˜ë¨¸ì§€ DOWN ìœ„ì£¼
                        else:
                            # ì¤‘ë¦½ ì „ëµ: ê· í˜•ì¡íŒ ë¶„í¬ (ê° ë°©í–¥ ìµœì†Œ 1ê°œ ë³´ì¥)
                            action_distribution = [0, 1, 2] + [0] * 2 + [1] * 2 + [2] * 2 + [0, 1, 2]  # ê· ë“± ë¶„í¬
                        
                        for exp_idx in range(num_experiences):
                            # ê°„ë‹¨í•œ ìƒíƒœ ë²¡í„° (ì„±ê³¼ ê¸°ë°˜ ì¶”ì •)
                            state = {
                                'rsi': 50.0, 'macd': 0.0, 'volume_ratio': 1.0, 'atr': 0.02,
                                'adx': 25.0, 'mfi': 50.0, 'bb_upper': 1.0, 'bb_middle': 1.0,
                                'bb_lower': 1.0, 'macd_signal': 0.0, 'close': 1.0, 'open': 1.0,
                                'high': 1.0, 'low': 1.0, 'volume': 1.0, 'volatility': 0.02,
                                'regime_stage': 3, 'regime_confidence': 0.5,
                                # ğŸš€ í™•ì¥ ì§€í‘œ ì¶”ê°€
                                'wave_progress': 0.5, 'pattern_confidence': predicted_conf,
                                'structure_score': 0.5, 'sentiment': 0.0, 'regime_transition_prob': 0.05
                            }
                            
                            # ğŸ”¥ ì•¡ì…˜ ë¶„í¬ì—ì„œ ì„ íƒ (ë‹¤ì–‘ì„± í™•ë³´)
                            action = action_distribution[exp_idx % len(action_distribution)]

                            # ğŸš€ ë©”íƒ€ í•™ìŠµ: ìƒíƒœ ë²¡í„° ìƒì„± (30ì°¨ì›)
                            state_vec = build_state_vector_with_strategy(state, strategy_params)

                            # ğŸ”¥ ì˜ˆì¸¡ ì‹ ë¢°ë„ ê¸°ë°˜ ë³´ìƒ (win_rateì™€ predicted_conf í™œìš©)
                            base_reward = win_rate - 0.5  # -0.5 ~ 0.5
                            confidence_bonus = (predicted_conf - 0.5) * 0.3  # -0.15 ~ 0.15
                            
                            # ì•¡ì…˜ê³¼ ì „ëµ ë°©í–¥ ì¼ì¹˜ ì—¬ë¶€ì— ë”°ë¥¸ ë³´ì •
                            if (action == 1 and strategy_direction == 'buy') or \
                               (action == 2 and strategy_direction == 'sell') or \
                               (action == 0 and strategy_direction == 'neutral'):
                                direction_bonus = 0.1  # ë°©í–¥ ì¼ì¹˜ ë³´ë„ˆìŠ¤
                            else:
                                direction_bonus = -0.05  # ë°©í–¥ ë¶ˆì¼ì¹˜ ì‘ì€ í˜ë„í‹°
                            
                            reward = base_reward + confidence_bonus + direction_bonus
                            
                            experience = {
                                'episode': episode_num,
                                'agent_id': agent_id,
                                'state': state_vec,
                                'action': action,
                                'reward': reward,
                                'log_prob': -1.1,
                                'value': reward * 0.9,
                                'done': True
                            }
                            experiences.append(experience)
                        
                        # í•œ ì—ì´ì „íŠ¸ë‹¹ ìµœëŒ€ 10ê°œ ìƒì„±í–ˆìœ¼ë¯€ë¡œ breakí•˜ì—¬ ë‹¤ìŒ ì—ì´ì „íŠ¸ë¡œ
                        break
                    
                    else:
                        # ğŸ”¥ ê±°ë˜ê°€ ì—†ëŠ” ê²½ìš°: ì „ëµ ë°©í–¥ì„±ê³¼ ì‹œì¥ ìƒíƒœ ê¸°ë°˜ ì˜ˆì¸¡ ê²½í—˜ ìƒì„±
                        # ì˜ˆì¸¡ ì „ëµì€ ê±°ë˜ê°€ ì—†ì–´ë„ ì‹œì¥ ìƒíƒœë¥¼ í•™ìŠµí•  ìˆ˜ ìˆì–´ì•¼ í•¨
                        # ë‹¤ì–‘í•œ ì˜ˆì¸¡ ë°©í–¥(UP/DOWN/NEUTRAL)ì„ ê· í˜•ìˆê²Œ ìƒì„±
                        
                        # ì—ì´ì „íŠ¸ ê²°ê³¼ì—ì„œ ì‹œì¥ ì •ë³´ ì¶”ì¶œ (ê°€ëŠ¥í•œ ê²½ìš°)
                        market_info = agent_result.get('market_info', {})
                        regime = agent_result.get('regime', 'neutral')
                        
                        # ğŸ”¥ ì „ëµ ë°©í–¥ì„±ê³¼ ë ˆì§ì„ ì¢…í•©í•˜ì—¬ ì˜ˆì¸¡ ë°©í–¥ ì¶”ì • (ë‹¤ì–‘ì„± í™•ë³´ - ê° ë°©í–¥ ìµœì†Œ 1ê°œ ë³´ì¥)
                        # ê° ì•¡ì…˜ì„ ì•ì— ë°°ì¹˜í•˜ì—¬ num_experiencesê°€ ì‘ì•„ë„ ëª¨ë“  ì•¡ì…˜ í¬í•¨ ë³´ì¥
                        if strategy_direction == 'buy':
                            # ë§¤ìˆ˜ ì „ëµ: UP ìš°ì„¸, DOWN/NEUTRAL í¬í•¨ (ê° ë°©í–¥ ìµœì†Œ 1ê°œ ë³´ì¥)
                            if 'bull' in regime.lower():
                                predicted_actions = [0, 1, 2] + [1] * 4 + [2] * 2 + [1]  # UP ìœ„ì£¼
                            elif 'bear' in regime.lower():
                                predicted_actions = [0, 1, 2] + [1] * 2 + [2] * 3 + [1]  # DOWN í˜¼í•©
                            else:
                                predicted_actions = [0, 1, 2] + [1] * 3 + [2] * 2 + [1, 2]  # ê· í˜•
                        elif strategy_direction == 'sell':
                            # ë§¤ë„ ì „ëµ: DOWN ìš°ì„¸, UP/NEUTRAL í¬í•¨ (ê° ë°©í–¥ ìµœì†Œ 1ê°œ ë³´ì¥)
                            if 'bear' in regime.lower():
                                predicted_actions = [0, 1, 2] + [2] * 4 + [1] * 2 + [2]  # DOWN ìœ„ì£¼
                            elif 'bull' in regime.lower():
                                predicted_actions = [0, 1, 2] + [2] * 2 + [1] * 3 + [2]  # UP í˜¼í•©
                            else:
                                predicted_actions = [0, 1, 2] + [2] * 3 + [1] * 2 + [2, 1]  # ê· í˜•
                        else:
                            # ì¤‘ë¦½ ì „ëµ: ê· í˜•ì¡íŒ ë¶„í¬ (ê° ë°©í–¥ ìµœì†Œ 1ê°œ ë³´ì¥)
                            if 'bull' in regime.lower():
                                predicted_actions = [0, 1, 2] + [1] * 3 + [0, 2] * 2 + [1]  # UP ìœ„ì£¼
                            elif 'bear' in regime.lower():
                                predicted_actions = [0, 1, 2] + [2] * 3 + [0, 1] * 2 + [2]  # DOWN ìœ„ì£¼
                            else:
                                predicted_actions = [0, 1, 2] + [0] * 2 + [1] * 2 + [2] * 2 + [0]  # ê· ë“±
                        
                        # ğŸ”¥ ê° ì˜ˆì¸¡ ë°©í–¥ë³„ë¡œ ê²½í—˜ ìƒì„± (ìµœì†Œ 5ê°œ, ìµœëŒ€ 20ê°œ, í•™ìŠµ ë°ì´í„° ì¦ê°€)
                        num_pred_experiences = max(5, min(len(predicted_actions), 20))
                        for action in predicted_actions[:num_pred_experiences]:
                            # ì‹œì¥ ìƒíƒœ ì¶”ì • (ì—ì´ì „íŠ¸ ê²°ê³¼ ê¸°ë°˜, í™•ì¥ ì§€í‘œ í¬í•¨)
                            state = {
                                'rsi': market_info.get('rsi', 50.0),
                                'macd': market_info.get('macd', 0.0),
                                'volume_ratio': market_info.get('volume_ratio', 1.0),
                                'atr': market_info.get('atr', 0.02),
                                'adx': market_info.get('adx', 25.0),
                                'mfi': market_info.get('mfi', 50.0),
                                'bb_upper': market_info.get('bb_upper', 1.0),
                                'bb_middle': market_info.get('bb_middle', 1.0),
                                'bb_lower': market_info.get('bb_lower', 1.0),
                                'macd_signal': market_info.get('macd_signal', 0.0),
                                'close': market_info.get('close', 1.0),
                                'open': market_info.get('open', 1.0),
                                'high': market_info.get('high', 1.0),
                                'low': market_info.get('low', 1.0),
                                'volume': market_info.get('volume', 1.0),
                                'volatility': market_info.get('volatility', 0.02),
                                'regime_stage': market_info.get('regime_stage', 3),
                                'regime_confidence': market_info.get('regime_confidence', 0.5),
                                # ğŸš€ í™•ì¥ ì§€í‘œ ì¶”ê°€
                                'wave_progress': market_info.get('wave_progress', 0.5),
                                'pattern_confidence': market_info.get('pattern_confidence', predicted_conf),
                                'structure_score': market_info.get('structure_score', 0.5),
                                'sentiment': market_info.get('sentiment', 0.0),
                                'regime_transition_prob': market_info.get('regime_transition_prob', 0.05)
                            }

                            # ğŸš€ ë©”íƒ€ í•™ìŠµ: ìƒíƒœ ë²¡í„° ìƒì„± (30ì°¨ì›)
                            state_vec = build_state_vector_with_strategy(state, strategy_params)

                            # ğŸ”¥ [Update] ì˜ˆì¸¡ ì „ëµ: ë ˆì§(ì‹œì¥ ìƒí™©) ê¸°ë°˜ ê°•ë ¥í•œ ë³´ìƒ (Effort & Recovery ì² í•™ ë°˜ì˜)
                            # ê±°ë˜ê°€ ì—†ì–´ë„ ì‹œì¥ ë°©í–¥(Regime)ì— ë§ëŠ” ì•¡ì…˜ì„ ì·¨í–ˆëŠ”ì§€ í‰ê°€
                            
                            base_reward = 0.0
                            
                            # ë ˆì§ íŒë‹¨
                            is_bull = 'bull' in regime.lower()
                            is_bear = 'bear' in regime.lower()
                            is_neutral = 'sideways' in regime.lower() or 'neutral' in regime.lower()
                            
                            if is_bull:
                                if action == 1: # UP (ì •ë‹µ)
                                    base_reward = 1.5
                                elif action == 0: # NEUTRAL (ê¸°íšŒ ë†“ì¹¨)
                                    base_reward = -2.0
                                else: # DOWN (í‹€ë¦¼ - ë„ì „)
                                    base_reward = -0.2
                            elif is_bear:
                                if action == 2: # DOWN (ì •ë‹µ)
                                    base_reward = 1.5
                                elif action == 0: # NEUTRAL (ê¸°íšŒ ë†“ì¹¨)
                                    base_reward = -2.0
                                else: # UP (í‹€ë¦¼ - ë„ì „)
                                    base_reward = -0.2
                            else: # Neutral/Sideways
                                if action == 0: # NEUTRAL (ì •ë‹µ - Safe Hold)
                                    base_reward = 0.2
                                else: # UP/DOWN (í‹€ë¦¼ - ë„ì „)
                                    base_reward = -0.2
                            
                            # ì „ëµ ë°©í–¥ ì¼ì¹˜ ë³´ë„ˆìŠ¤ (ë³´ì¡°)
                            if (action == 1 and strategy_direction == 'buy') or \
                               (action == 2 and strategy_direction == 'sell'):
                                base_reward += 0.1
                            
                            # ì˜ˆì¸¡ ì‹ ë¢°ë„ ë³´ì •
                            confidence_bonus = (predicted_conf - 0.5) * 0.2  # -0.1 ~ 0.1
                            
                            reward = base_reward + confidence_bonus
                            
                            experience = {
                                'episode': episode_num,
                                'agent_id': agent_id,
                                'state': state_vec,
                                'action': action,  # ğŸ”¥ ë‹¤ì–‘í•œ ì•¡ì…˜ (UP/DOWN/NEUTRAL)
                                'reward': reward,
                                'log_prob': -1.1,
                                'value': reward * 0.9,
                                'done': True
                            }
                            experiences.append(experience)
                        
                        # ğŸ”¥ ì²« ë²ˆì§¸ ì—ì´ì „íŠ¸ë§Œ ì‚¬ìš© (ì¤‘ë³µ ë°©ì§€)
                        break
        
        except Exception as e:
            logger.warning(f"âš ï¸ ê²½í—˜ ì¶”ì¶œ ì¤‘ ì¼ë¶€ ë°ì´í„° ì†ì‹¤: {e}")
            import traceback
            logger.debug(f"ê²½í—˜ ì¶”ì¶œ ìƒì„¸ ì—ëŸ¬:\n{traceback.format_exc()}")
        
        # ğŸ”¥ ë°ì´í„° ê²€ì¦ ê°•í™”: ì•¡ì…˜ ë‹¤ì–‘ì„± ì²´í¬
        if experiences:
            actions = [exp.get('action', 0) for exp in experiences]
            unique_actions = set(actions)
            action_counts = {action: actions.count(action) for action in unique_actions}
            
            # ì•¡ì…˜ ë¶„í¬ ë¡œê¹…
            logger.info(f"ğŸ“Š ê²½í—˜ ì¶”ì¶œ ì™„ë£Œ: ì´ {len(experiences)}ê°œ, ê³ ìœ  ì•¡ì…˜: {len(unique_actions)}ê°œ")
            logger.info(f"   ì•¡ì…˜ ë¶„í¬: NEUTRAL(0)={action_counts.get(0, 0)}, UP(1)={action_counts.get(1, 0)}, DOWN(2)={action_counts.get(2, 0)}")
            
            # ğŸ”¥ ì•¡ì…˜ ë‹¤ì–‘ì„± ê²€ì¦
            if len(unique_actions) < 2:
                logger.warning(f"âš ï¸ ì•¡ì…˜ ë‹¤ì–‘ì„± ë¶€ì¡±: ê³ ìœ  ì•¡ì…˜ {len(unique_actions)}ê°œë§Œ ì¡´ì¬")
                logger.warning(f"   ì•¡ì…˜ ë¶„í¬: {action_counts}")
                
                # ì•¡ì…˜ ë‹¤ì–‘ì„± ë¶€ì¡± ì‹œ ê²½ê³ ë§Œ ì¶œë ¥ (í•™ìŠµì€ ê³„ì† ì§„í–‰)
                # í•™ìŠµ ê³¼ì •ì—ì„œ entropy_coefê°€ ìë™ìœ¼ë¡œ ì¦ê°€í•˜ì—¬ íƒí—˜ì„ ê°•í™”í•¨
            elif len(unique_actions) == 2:
                logger.info(f"âœ… ì•¡ì…˜ ë‹¤ì–‘ì„± ì–‘í˜¸: ê³ ìœ  ì•¡ì…˜ {len(unique_actions)}ê°œ")
            else:
                logger.info(f"âœ… ì•¡ì…˜ ë‹¤ì–‘ì„± ìš°ìˆ˜: ê³ ìœ  ì•¡ì…˜ {len(unique_actions)}ê°œ (ëª¨ë“  ë°©í–¥ í¬í•¨)")
        else:
            logger.warning("âš ï¸ ê²½í—˜ ì¶”ì¶œ ê²°ê³¼ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")
        
        logger.debug(f"âœ… ì´ {len(experiences)}ê°œ ê²½í—˜ ì¶”ì¶œ ì™„ë£Œ")
        return experiences
    
    def _extract_experiences_with_analysis(
        self,
        episodes_data: List[Dict[str, Any]],
        analysis_data: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """
        Self-play ê²°ê³¼ + ë¶„ì„ ë°ì´í„°ì—ì„œ ê²½í—˜ ì¶”ì¶œ (25ì°¨ì›)
        
        Args:
            episodes_data: Self-play ì—í”¼ì†Œë“œ ë°ì´í„°
            analysis_data: í†µí•© ë¶„ì„ ê²°ê³¼
                {
                    'fractal_score': float,
                    'multi_timeframe_score': float,
                    'indicator_cross_score': float,
                    'ensemble_score': float,
                    'ensemble_confidence': float
                }
        
        Returns:
            ê²½í—˜ ë¦¬ìŠ¤íŠ¸ (stateëŠ” 35ì°¨ì› ë²¡í„°: 20 base + 5 analysis + 10 strategy params)
        """
        experiences = []
        
        try:
            # ë¶„ì„ ì ìˆ˜ ì¶”ì¶œ (ê¸°ë³¸ê°’ 0.5)
            fractal_score = analysis_data.get('fractal_score', 0.5)
            multi_timeframe_score = analysis_data.get('multi_timeframe_score', 0.5)
            indicator_cross_score = analysis_data.get('indicator_cross_score', 0.5)
            ensemble_score = analysis_data.get('ensemble_score', 0.5)
            ensemble_confidence = analysis_data.get('ensemble_confidence', 0.5)
            
            for episode in episodes_data:
                results = episode.get('results', {})
                episode_num = episode.get('episode', 0)
                
                for agent_id, agent_result in results.items():
                    # ì—ì´ì „íŠ¸ë³„ ì„±ê³¼ì—ì„œ ê²½í—˜ ì¶”ì¶œ
                    trades = agent_result.get('trades', [])
                    total_pnl = agent_result.get('total_pnl', 0.0)
                    win_rate = agent_result.get('win_rate', 0.0)
                    total_trades = agent_result.get('total_trades', 0)
                    profit_factor = agent_result.get('profit_factor', 0.0)
                    strategy_direction = agent_result.get('strategy_direction', 'neutral')  # ğŸ”¥ ì „ëµ ë°©í–¥
                    predicted_conf = agent_result.get('predicted_conf', 0.5)  # ğŸ”¥ ì˜ˆì¸¡ ì‹ ë¢°ë„
                    strategy_params = agent_result.get('strategy_params', {})  # ğŸš€ ë©”íƒ€ í•™ìŠµ: ì „ëµ íŒŒë¼ë¯¸í„°

                    # ğŸ”¥ ì˜ˆì¸¡ ì „ëµ: ê±°ë˜ê°€ ì—†ì–´ë„ í•™ìŠµ ë°ì´í„°ë¡œ í™œìš©
                    # ì˜ˆì¸¡ ì „ëµì€ ê±°ë˜ ê²°ê³¼ë³´ë‹¤ ì˜ˆì¸¡ ì •í™•ë„ê°€ ì¤‘ìš”í•˜ë¯€ë¡œ í•„í„° ì™„í™”
                    quality_check_passed = True  # ëª¨ë“  ì—í”¼ì†Œë“œ í¬í•¨

                    # ğŸ”¥ í•™ìŠµ ì„±ëŠ¥ ê°œì„ : íŠ¸ë ˆì´ë“œë³„ ê²½í—˜ ìƒì„± (ì•¡ì…˜ ë‹¤ì–‘ì„± ìš°ì„ )
                    if trades:
                        # ğŸ”¥ ì˜ˆì¸¡ ì „ëµ: UP/DOWN ì˜ˆì¸¡ì„ ìš°ì„ ì ìœ¼ë¡œ ìˆ˜ì§‘ (NEUTRALì€ ì œí•œ)
                        # ê±°ë˜ ë°ì´í„°ì˜ BUY/SELL/HOLDë¥¼ ì˜ˆì¸¡ ë°©í–¥ìœ¼ë¡œ ë³€í™˜
                        buy_trades = [t for t in trades if t.get('direction') == 'BUY']  # â†’ UP ì˜ˆì¸¡
                        sell_trades = [t for t in trades if t.get('direction') == 'SELL']  # â†’ DOWN ì˜ˆì¸¡
                        hold_trades = [t for t in trades if t.get('direction') != 'BUY' and t.get('direction') != 'SELL']  # â†’ NEUTRAL ì˜ˆì¸¡
                        hold_trades = self._ensure_neutral_trade_pool(
                            buy_trades,
                            sell_trades,
                            hold_trades,
                            episode_num=episode_num,
                            agent_id=agent_id
                        )
                        buy_trades, sell_trades = self._ensure_directional_trade_pool(
                            buy_trades,
                            sell_trades,
                            hold_trades,
                            episode_num=episode_num,
                            agent_id=agent_id
                        )
                        
                        selected_trades = self._select_trades_with_diversity(buy_trades, sell_trades, hold_trades)
                        
                        # ğŸ”¥ ë””ë²„ê¹…: ì„ íƒëœ íŠ¸ë ˆì´ë“œ ìƒì„¸ í™•ì¸
                        # BUY/SELLì´ ìˆëŠ”ë° ì„ íƒëœ íŠ¸ë ˆì´ë“œì— ì—†ëŠ” ê²½ìš° í™•ì¸
                        has_pool_diversity = len(buy_trades) > 0 or len(sell_trades) > 0
                        selected_dirs = [t.get('direction') for t in selected_trades]
                        has_selected_diversity = any(d in ['BUY', 'SELL'] for d in selected_dirs)
                        
                        if has_pool_diversity and not has_selected_diversity:
                             logger.error(f"âŒ {agent_id}: Pool has diversity (B:{len(buy_trades)}, S:{len(sell_trades)}) but selected_trades DOES NOT. Selected dirs: {selected_dirs[:20]}")

                        # ğŸ”¥ ì•¡ì…˜ ë‹¤ì–‘ì„± ê°•ì œ ë³´ì •
                        # ë§Œì•½ NEUTRAL(HOLD)ë§Œ ìˆë‹¤ë©´, ê°•ì œë¡œ BUY/SELL í•©ì„± ê²½í—˜ ì¶”ê°€
                        # ì•ˆì „í•œ í™•ì¸ì„ ìœ„í•´ ëª¨ë‘ ë¬¸ìì—´ ë³€í™˜ ë° ëŒ€ë¬¸ì ì²˜ë¦¬
                        unique_actions_set = set(str(d).upper() for d in selected_dirs)
                        
                        # ë””ë²„ê¹… ë¡œê·¸ (ì²˜ìŒ 5ê°œ ì—ì´ì „íŠ¸ë§Œ)
                        if len(experiences) < 5:
                             logger.info(f"ğŸ” ë””ë²„ê¹…: Agent={agent_id}, Dirs={list(unique_actions_set)}")

                        # 'HOLD' ë˜ëŠ” 'neutral'ë§Œ ìˆëŠ” ê²½ìš° (BUY/SELLì´ ì—†ìŒ)
                        has_directional = any(d in ['BUY', 'SELL'] for d in unique_actions_set)
                        
                        if not has_directional:
                             # BUY/SELL ê°•ì œ ì£¼ì… (stateëŠ” ë§ˆì§€ë§‰ trade ë³µì‚¬í•˜ë˜ ë³€í˜•)
                             if selected_trades:
                                 base_trade = selected_trades[0]
                                 
                                 # Synthetic BUY
                                 synthetic_buy = copy.deepcopy(base_trade)
                                 synthetic_buy['direction'] = 'BUY'
                                 # ğŸ”¥ ê°•ì œ ì£¼ì…ëœ ë°ì´í„°ëŠ” threshold ì´ìƒìœ¼ë¡œ ì„¤ì •í•˜ì—¬ ëª…í™•í•œ BUYë¡œ ì¸ì‹ë˜ê²Œ í•¨
                                 synthetic_buy['price_change'] = self.neutral_direction_threshold * 2.0
                                 synthetic_buy['synthetic_forced'] = True
                                 selected_trades.append(synthetic_buy)
                                 
                                 # Synthetic SELL
                                 synthetic_sell = copy.deepcopy(base_trade)
                                 synthetic_sell['direction'] = 'SELL' 
                                 # ğŸ”¥ ê°•ì œ ì£¼ì…ëœ ë°ì´í„°ëŠ” threshold ì´ìƒìœ¼ë¡œ ì„¤ì •í•˜ì—¬ ëª…í™•í•œ SELLë¡œ ì¸ì‹ë˜ê²Œ í•¨
                                 synthetic_sell['price_change'] = -self.neutral_direction_threshold * 2.0
                                 synthetic_sell['synthetic_forced'] = True
                                 selected_trades.append(synthetic_sell)
                                 
                                 logger.info(f"ğŸ”§ {agent_id}: ì•¡ì…˜ ë‹¤ì–‘ì„± ë¶€ì¡±ìœ¼ë¡œ ê°•ì œ í•©ì„± ë°ì´í„°(BUY/SELL) ì£¼ì… (Dirs: {unique_actions_set})")

                        for trade in selected_trades:
                            # Market state ì¬êµ¬ì„± (í™•ì¥ ì§€í‘œ í¬í•¨)
                            state = {
                                'rsi': trade.get('rsi', 50.0),
                                'macd': trade.get('macd', 0.0),
                                'volume_ratio': trade.get('volume_ratio', 1.0),
                                'atr': trade.get('atr', 0.02),
                                'adx': trade.get('adx', 25.0),
                                'mfi': trade.get('mfi', 50.0),
                                'bb_upper': trade.get('bb_upper', 1.0),
                                'bb_middle': trade.get('bb_middle', 1.0),
                                'bb_lower': trade.get('bb_lower', 1.0),
                                'macd_signal': trade.get('macd_signal', 0.0),
                                'close': trade.get('close', 1.0),
                                'open': trade.get('open', 1.0),
                                'high': trade.get('high', 1.0),
                                'low': trade.get('low', 1.0),
                                'volume': trade.get('volume', 1.0),
                                'volatility': trade.get('volatility', 0.02),
                                'regime_stage': trade.get('regime_stage', 3),
                                'regime_confidence': trade.get('regime_confidence', 0.5),
                                # ğŸš€ í™•ì¥ ì§€í‘œ ì¶”ê°€ (1ë‹¨ê³„ í™•ì¥)
                                'wave_progress': trade.get('wave_progress', 0.5),
                                'pattern_confidence': trade.get('pattern_confidence', 0.5),
                                'structure_score': trade.get('structure_score', 0.5),
                                'sentiment': trade.get('sentiment', 0.0),
                                'regime_transition_prob': trade.get('regime_transition_prob', 0.05)
                            }
                            
                            # ğŸ”¥ ë©”íƒ€ í•™ìŠµ: ë¶„ì„+ì „ëµ íŒŒë¼ë¯¸í„° í¬í•¨ ìƒíƒœ ë²¡í„° ìƒì„± (35ì°¨ì›)
                            # í”„ë™íƒˆ/ë©€í‹°íƒ€ì„í”„ë ˆì„/ì§€í‘œêµì°¨ ì ìˆ˜ + ì „ëµ íŒŒë¼ë¯¸í„° í¬í•¨í•˜ì—¬ ë” ê°•ë ¥í•œ í•™ìŠµ
                            enhanced_state_vec = build_state_vector_with_analysis_and_strategy(
                                state,
                                strategy_params,
                                fractal_score=fractal_score,
                                multi_timeframe_score=multi_timeframe_score,
                                indicator_cross_score=indicator_cross_score,
                                ensemble_score=ensemble_score,
                                ensemble_confidence=ensemble_confidence
                            )
                            
                            # ğŸ”¥ ì˜ˆì¸¡ ì „ëµ: ì•¡ì…˜ì„ ë°©í–¥ ì˜ˆì¸¡ìœ¼ë¡œ ë³€í™˜
                            # BUY â†’ UP(1): ìƒìŠ¹ ì˜ˆì¸¡, SELL â†’ DOWN(2): í•˜ë½ ì˜ˆì¸¡, HOLD â†’ NEUTRAL(0): ì¤‘ë¦½ ì˜ˆì¸¡
                            # ì•ˆì „ì„ ìœ„í•´ ë¬¸ìì—´ ë³€í™˜ ë° ëŒ€ë¬¸ìí™”
                            trade_direction = str(trade.get('direction', 'HOLD')).upper()
                            
                            if trade_direction == 'BUY':
                                action = 1  # UP: ìƒìŠ¹ ì˜ˆì¸¡
                                predicted_direction = 'UP'
                            elif trade_direction == 'SELL':
                                action = 2  # DOWN: í•˜ë½ ì˜ˆì¸¡
                                predicted_direction = 'DOWN'
                            else:
                                action = 0  # NEUTRAL: ì¤‘ë¦½ ì˜ˆì¸¡
                                predicted_direction = 'NEUTRAL'
                                
                                # ğŸ”¥ ë””ë²„ê¹…: Synthetic tradeì¸ë° NEUTRALë¡œ ë§¤í•‘ëœ ê²½ìš° í™•ì¸
                                if trade.get('synthetic_forced'):
                                     logger.error(f"âŒ {agent_id}: Synthetic FORCED trade mapped to NEUTRAL! Dir={trade_direction}")
                                elif trade.get('synthetic_directional'):
                                     logger.error(f"âŒ {agent_id}: Synthetic directional trade mapped to NEUTRAL! Dir={trade_direction}")
                            
                            # ğŸ”¥ ì˜ˆì¸¡ ì •í™•ë„ ê¸°ë°˜ ë³´ìƒ ì‹œìŠ¤í…œ (HOLDë¥¼ ì‹¤ì œ ë°©í–¥ìœ¼ë¡œ ì¬í‰ê°€)
                            # ì‹¤ì œ ê°€ê²© ë³€í™”ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì˜ˆì¸¡ ì •í™•ë„ í‰ê°€
                            price_change = float(trade.get('price_change', 0.0) or 0.0)  # ì‹¤ì œ ê°€ê²© ë³€í™”ìœ¨
                            threshold = self.neutral_direction_threshold
                            actual_direction = (
                                'UP' if price_change > threshold
                                else ('DOWN' if price_change < -threshold else 'NEUTRAL')
                            )

                            # ğŸ”¥ ì¤‘ìš” ìˆ˜ì •: ì˜ˆì¸¡ ëª©í‘œ ê°•í™” (Hindsight Labeling)
                            # ì „ëµì´ HOLD í–ˆë”ë¼ë„, ì‹œì¥ì´ ì›€ì§ì˜€ë‹¤ë©´ ê·¸ ì›€ì§ì„ì„ ì •ë‹µìœ¼ë¡œ í•™ìŠµ
                            # ì „ëµì˜ ì†Œê·¹ì  íƒœë„(Risk Aversion)ê°€ ì˜ˆì¸¡ ëŠ¥ë ¥ ì €í•˜ë¡œ ì´ì–´ì§€ì§€ ì•Šë„ë¡ í•¨
                            
                            if predicted_direction == 'NEUTRAL' and actual_direction != 'NEUTRAL':
                                # ì „ëµì€ ê´€ë§í–ˆì§€ë§Œ ì‹œì¥ì€ ì›€ì§ì„ -> ì‹¤ì œ ë°©í–¥ìœ¼ë¡œ ë¼ë²¨ ìˆ˜ì • (Oracle Learning)
                                if actual_direction == 'UP':
                                    action = 1 # UP
                                    predicted_direction = 'UP'
                                elif actual_direction == 'DOWN':
                                    action = 2 # DOWN
                                    predicted_direction = 'DOWN'
                                # NEUTRAL ë¼ë²¨ì„ ì œê±°í•˜ê³  ë°©í–¥ì„± ë¼ë²¨ë¡œ ëŒ€ì²´í•˜ì—¬ ì ê·¹ì  ì˜ˆì¸¡ ìœ ë„
                            
                            # ğŸ”¥ ì¤‘ìš” ìˆ˜ì •: ì „ëµì˜ ì¡°ê¸° ì²­ì‚°(Take-Profit/Stop-Loss)ìœ¼ë¡œ ì¸í•œ ì˜ˆì¸¡ ì™œê³¡ ë°©ì§€
                            # ì „ëµì´ 20% ìƒìŠ¹ì„ ëª©í‘œë¡œ í–ˆìœ¼ë‚˜ 5%ì—ì„œ ìµì ˆí–ˆë‹¤ë©´, ì‹¤ì œë¡œëŠ” ë” ì˜¬ë¼ê°”ì„ ìˆ˜ ìˆìŒ
                            # ë”°ë¼ì„œ 'UP' ì˜ˆì¸¡ì´ì—ˆëŠ”ë° ìµì ˆë¡œ ëë‚œ ê²½ìš°, ì´í›„ ê°€ê²© ì¶”ì´(ì ì¬ì  ìµœëŒ€ ë³€ë™í­)ë¥¼ ê³ ë ¤í•´ì•¼ í•˜ì§€ë§Œ,
                            # í˜„ì¬ trade ì •ë³´ë§Œìœ¼ë¡œëŠ” ì•Œ ìˆ˜ ì—†ìœ¼ë¯€ë¡œ, 'ì´ìµì´ ë‚¬ë‹¤'ëŠ” ì‚¬ì‹¤ ìì²´ë¥¼ ê¸ì •ì ìœ¼ë¡œ í‰ê°€.
                            
                            # ë˜í•œ, ê°•ì œ ì²­ì‚°(Stop Loss)ì˜ ê²½ìš°ì—ë„ ë°©í–¥ ì˜ˆì¸¡ì€ ë§ì•˜ìœ¼ë‚˜ ë³€ë™í­ì´ ì»¤ì„œ í„¸ë¦° ê²½ìš°ì¼ ìˆ˜ ìˆìŒ.
                            # í•˜ì§€ë§Œ ì˜ˆì¸¡ ê´€ì ì—ì„œëŠ” 'ê²°ê³¼ì ì¸ ê°€ê²© ë³€í™”'ê°€ ì¤‘ìš”í•˜ë¯€ë¡œ actual_directionì„ ë”°ë¥´ëŠ” ê²ƒì´ ê¸°ë³¸.
                            
                            # ë‹¨, 'UP' ì˜ˆì¸¡ì„ í–ˆëŠ”ë° ì‹¤ì œë¡œëŠ” 'NEUTRAL' ìˆ˜ì¤€ì˜ ì‘ì€ ì´ìµë§Œ ë³´ê³  ëë‚œ ê²½ìš° (ì¡°ê¸° ìµì ˆ),
                            # ì´ë¥¼ 'í‹€ë¦¼'ìœ¼ë¡œ ì²˜ë¦¬í•˜ë©´ ì–µìš¸í•¨. ì´ìµì´ ë‚¬ë‹¤ë©´(price_change > 0) UP ì˜ˆì¸¡ì— ëŒ€í•´ ë¶€ë¶„ ì ìˆ˜ ë¶€ì—¬.
                            
                            direction_reward = 0.0
                            
                            if predicted_direction == actual_direction:
                                # 1. ì™„ì „ ì¼ì¹˜
                                if predicted_direction == 'UP':
                                    direction_reward = 1.0
                                elif predicted_direction == 'DOWN':
                                    direction_reward = 1.0
                                else:
                                    direction_reward = 0.9 # NEUTRAL ë³´ìƒ (Policy Collapse ë°©ì§€)
                            
                            elif predicted_direction == 'UP':
                                # 2. ìƒìŠ¹ ì˜ˆì¸¡í–ˆìœ¼ë‚˜...
                                if actual_direction == 'NEUTRAL' and price_change > 0:
                                    # ì¡°ê¸ˆì´ë¼ë„ ì˜¬ëìœ¼ë©´ ë¶€ë¶„ ì ìˆ˜ (ì¡°ê¸° ìµì ˆ ê°€ëŠ¥ì„±)
                                    # threshold(ë³´í†µ 0.002~0.005)ë³´ë‹¤ ì‘ì§€ë§Œ 0ë³´ë‹¤ëŠ” í° ê²½ìš°
                                    direction_reward = 0.3
                                elif actual_direction == 'DOWN':
                                    # ë°˜ëŒ€ë¡œ ê° -> í˜ë„í‹°
                                    direction_reward = -1.0
                                else:
                                    # ê·¸ ì™¸ (NEUTRALì¸ë° 0 ì´í•˜ì¸ ê²½ìš° ë“±)
                                    direction_reward = -0.3
                                    
                            elif predicted_direction == 'DOWN':
                                # 3. í•˜ë½ ì˜ˆì¸¡í–ˆìœ¼ë‚˜...
                                if actual_direction == 'NEUTRAL' and price_change < 0:
                                    # ì¡°ê¸ˆì´ë¼ë„ ë‚´ë ¸ìœ¼ë©´ ë¶€ë¶„ ì ìˆ˜
                                    direction_reward = 0.3
                                elif actual_direction == 'UP':
                                    # ë°˜ëŒ€ë¡œ ê° -> í˜ë„í‹°
                                    direction_reward = -1.0
                                else:
                                    direction_reward = -0.3
                                    
                            else: # predicted == NEUTRAL
                                # 4. ì¤‘ë¦½ ì˜ˆì¸¡í–ˆìœ¼ë‚˜...
                                # í¬ê²Œ ì›€ì§ì˜€ìœ¼ë©´ í˜ë„í‹° (ê¸°íšŒ ë¹„ìš©)
                                direction_reward = -0.5

                            # ì˜ˆì¸¡ ì‹ ë¢°ë„ ê¸°ë°˜ ë³´ì • (win_rate í™œìš©)
                            confidence_bonus = (win_rate - 0.5) * 0.5  # -0.25 ~ +0.25

                            # ìµœì¢… ë³´ìƒ: ì˜ˆì¸¡ ì •í™•ë„ + ì‹ ë¢°ë„ ë³´ë„ˆìŠ¤
                            reward = direction_reward + confidence_bonus

                            # ğŸ†• Policy Collapse ë°©ì§€: NEUTRAL ì•¡ì…˜ ë³´ë„ˆìŠ¤ ì¶”ê°€
                            # NEUTRAL ì•¡ì…˜ì´ ì ê²Œ ì„ íƒë˜ëŠ” ê²½ìš° ë³´ë„ˆìŠ¤ ì œê³µ
                            if predicted_direction == 'NEUTRAL':
                                reward += self.neutral_action_bonus
                            else:
                                reward += self.direction_action_bonus
                            
                            # ê¸°ë³¸ log_prob
                            log_prob = -1.1
                            
                            # ê¸°ë³¸ value estimate
                            value = reward * 0.9
                            
                            experience = {
                                'episode': episode_num,
                                'agent_id': agent_id,
                                'state': enhanced_state_vec,  # ğŸ”¥ 25ì°¨ì› ë²¡í„°
                                'action': action,
                                'reward': reward,
                                'log_prob': log_prob,
                                'value': value,
                                'done': False,
                                'analysis_score': ensemble_score * 100.0  # ğŸ†• íƒ€ê²Ÿ ë¶„ì„ ì ìˆ˜ (0~100)
                            }
                            experiences.append(experience)
                        
                        # ğŸ”¥ UP/DOWN ì˜ˆì¸¡ tradesê°€ ìˆìœ¼ë©´ ë” ì´ìƒ ì¶”ê°€í•˜ì§€ ì•ŠìŒ (ì˜ˆì¸¡ ë‹¤ì–‘ì„± í™•ë³´) -> ì œê±°: ëª¨ë“  ì„ íƒëœ trades í™œìš©
                        # break  # ëª¨ë“  ì„ íƒëœ íŠ¸ë ˆì´ë“œë¥¼ í™œìš©í•˜ì—¬ ì•¡ì…˜ ë‹¤ì–‘ì„± í™•ë³´

                    
                    # ğŸ”¥ tradesê°€ ì—†ëŠ” ê²½ìš°: total_tradesê°€ ìˆìœ¼ë©´ ê²½í—˜ ìƒì„±
                    elif total_trades > 0:
                        # total_tradesëŠ” ìˆì§€ë§Œ trades ë¦¬ìŠ¤íŠ¸ëŠ” ì—†ëŠ” ê²½ìš°
                        # (ì˜ˆì¸¡ self-playì—ì„œ predictionsë¥¼ tradesë¡œ ë³€í™˜í–ˆì§€ë§Œ ì‹¤ì œ ê±°ë˜ëŠ” ì—†ìŒ)
                        # ğŸ”¥ ì „ëµ ë°©í–¥ì„±ê³¼ ì˜ˆì¸¡ ì‹ ë¢°ë„ë¥¼ í™œìš©í•˜ì—¬ ë‹¤ì–‘í•œ ì•¡ì…˜ ìƒì„±
                        # ğŸ”¥ í•©ì„± ë°ì´í„° ìƒì„±ëŸ‰ ì¦ê°€: ìµœì†Œ 5ê°œ ë³´ì¥, ìµœëŒ€ 20ê°œ (í•™ìŠµ ë°ì´í„° ì¦ê°€)
                        num_experiences = max(5, min(total_trades, 20))

                        # ğŸ”¥ ì „ëµ ë°©í–¥ì„± ê¸°ë°˜ ì•¡ì…˜ ë¶„í¬ ìƒì„± (ë‹¤ì–‘ì„± í™•ë³´ - ê° ë°©í–¥ ìµœì†Œ 1ê°œ ë³´ì¥)
                        # ê° ì•¡ì…˜ì„ ì•ì— ë°°ì¹˜í•˜ì—¬ num_experiencesê°€ ì‘ì•„ë„ ëª¨ë“  ì•¡ì…˜ í¬í•¨ ë³´ì¥
                        if strategy_direction == 'buy':
                            # ë§¤ìˆ˜ ì „ëµ: UP ìš°ì„¸, DOWN/NEUTRAL í¬í•¨ (ë‹¤ì–‘ì„±)
                            # ìµœì†Œ ê° ë°©í–¥ 1ê°œì”© ë¨¼ì € ë³´ì¥ (ì• 3ê°œ), ë‚˜ë¨¸ì§€ UP ìœ„ì£¼ë¡œ ì±„ì›€
                            action_distribution = [0, 1, 2] + [1] * 4 + [2] * 2 + [1]  # NEUTRAL, UP, DOWN ê° 1ê°œ ë¨¼ì €, ë‚˜ë¨¸ì§€ UP ìœ„ì£¼
                        elif strategy_direction == 'sell':
                            # ë§¤ë„ ì „ëµ: DOWN ìš°ì„¸, UP/NEUTRAL í¬í•¨ (ë‹¤ì–‘ì„±)
                            # ìµœì†Œ ê° ë°©í–¥ 1ê°œì”© ë¨¼ì € ë³´ì¥ (ì• 3ê°œ), ë‚˜ë¨¸ì§€ DOWN ìœ„ì£¼ë¡œ ì±„ì›€
                            action_distribution = [0, 1, 2] + [2] * 4 + [1] * 2 + [2]  # NEUTRAL, UP, DOWN ê° 1ê°œ ë¨¼ì €, ë‚˜ë¨¸ì§€ DOWN ìœ„ì£¼
                        else:
                            # ì¤‘ë¦½ ì „ëµ: ê· í˜•ì¡íŒ ë¶„í¬ (ê° ë°©í–¥ ìµœì†Œ 1ê°œ ë³´ì¥)
                            action_distribution = [0, 1, 2] + [0] * 2 + [1] * 2 + [2] * 2 + [0, 1, 2]  # ê· ë“± ë¶„í¬
                        
                        for exp_idx in range(num_experiences):
                            state = {
                                'rsi': 50.0, 'macd': 0.0, 'volume_ratio': 1.0, 'atr': 0.02,
                                'adx': 25.0, 'mfi': 50.0, 'bb_upper': 1.0, 'bb_middle': 1.0,
                                'bb_lower': 1.0, 'macd_signal': 0.0, 'close': 1.0, 'open': 1.0,
                                'high': 1.0, 'low': 1.0, 'volume': 1.0, 'volatility': 0.02,
                                'regime_stage': 3, 'regime_confidence': 0.5,
                                # ğŸš€ í™•ì¥ ì§€í‘œ ì¶”ê°€
                                'wave_progress': 0.5, 'pattern_confidence': predicted_conf,
                                'structure_score': 0.5, 'sentiment': 0.0, 'regime_transition_prob': 0.05
                            }

                            # ğŸš€ ë©”íƒ€ í•™ìŠµ: ë¶„ì„+ì „ëµ íŒŒë¼ë¯¸í„° í¬í•¨ ìƒíƒœ ë²¡í„° ìƒì„± (35ì°¨ì›)
                            enhanced_state_vec = build_state_vector_with_analysis_and_strategy(
                                state,
                                strategy_params,
                                fractal_score=fractal_score,
                                multi_timeframe_score=multi_timeframe_score,
                                indicator_cross_score=indicator_cross_score,
                                ensemble_score=ensemble_score,
                                ensemble_confidence=ensemble_confidence
                            )

                            # ğŸ”¥ ì•¡ì…˜ ë¶„í¬ì—ì„œ ì„ íƒ (ë‹¤ì–‘ì„± í™•ë³´)
                            action = action_distribution[exp_idx % len(action_distribution)]
                            
                            # ğŸ”¥ ì˜ˆì¸¡ ì‹ ë¢°ë„ ê¸°ë°˜ ë³´ìƒ
                            base_reward = win_rate - 0.5  # -0.5 ~ 0.5
                            confidence_bonus = (predicted_conf - 0.5) * 0.3  # -0.15 ~ 0.15
                            
                            # ì•¡ì…˜ê³¼ ì „ëµ ë°©í–¥ ì¼ì¹˜ ì—¬ë¶€ì— ë”°ë¥¸ ë³´ì •
                            if (action == 1 and strategy_direction == 'buy') or \
                               (action == 2 and strategy_direction == 'sell') or \
                               (action == 0 and strategy_direction == 'neutral'):
                                direction_bonus = 0.1  # ë°©í–¥ ì¼ì¹˜ ë³´ë„ˆìŠ¤
                            else:
                                direction_bonus = -0.05  # ë°©í–¥ ë¶ˆì¼ì¹˜ ì‘ì€ í˜ë„í‹°
                            
                            reward = base_reward + confidence_bonus + direction_bonus
                            
                            experience = {
                                'episode': episode_num,
                                'agent_id': agent_id,
                                'state': enhanced_state_vec,  # ğŸ”¥ 25ì°¨ì› ë²¡í„°
                                'action': action,
                                'reward': reward,
                                'log_prob': -1.1,
                                'value': reward * 0.9,
                                'done': True
                            }
                            experiences.append(experience)
                        
                        break
                    
                    else:
                        # ğŸ”¥ ê±°ë˜ê°€ ì—†ëŠ” ê²½ìš°: ì „ëµ ë°©í–¥ì„±ê³¼ ì‹œì¥ ìƒíƒœ ê¸°ë°˜ ì˜ˆì¸¡ ê²½í—˜ ìƒì„±
                        market_info = agent_result.get('market_info', {})
                        regime = agent_result.get('regime', 'neutral')
                        
                        # ğŸ”¥ ì „ëµ ë°©í–¥ì„±ê³¼ ë ˆì§ì„ ì¢…í•©í•˜ì—¬ ì˜ˆì¸¡ ë°©í–¥ ì¶”ì • (ë‹¤ì–‘ì„± í™•ë³´ - ê° ë°©í–¥ ìµœì†Œ 1ê°œ ë³´ì¥)
                        # ê° ì•¡ì…˜ì„ ì•ì— ë°°ì¹˜í•˜ì—¬ num_experiencesê°€ ì‘ì•„ë„ ëª¨ë“  ì•¡ì…˜ í¬í•¨ ë³´ì¥
                        if strategy_direction == 'buy':
                            # ë§¤ìˆ˜ ì „ëµ: UP ìš°ì„¸, DOWN/NEUTRAL í¬í•¨ (ê° ë°©í–¥ ìµœì†Œ 1ê°œ ë³´ì¥)
                            if 'bull' in regime.lower():
                                predicted_actions = [0, 1, 2] + [1] * 4 + [2] * 2 + [1]  # UP ìœ„ì£¼
                            elif 'bear' in regime.lower():
                                predicted_actions = [0, 1, 2] + [1] * 2 + [2] * 3 + [1]  # DOWN í˜¼í•©
                            else:
                                predicted_actions = [0, 1, 2] + [1] * 3 + [2] * 2 + [1, 2]  # ê· í˜•
                        elif strategy_direction == 'sell':
                            # ë§¤ë„ ì „ëµ: DOWN ìš°ì„¸, UP/NEUTRAL í¬í•¨ (ê° ë°©í–¥ ìµœì†Œ 1ê°œ ë³´ì¥)
                            if 'bear' in regime.lower():
                                predicted_actions = [0, 1, 2] + [2] * 4 + [1] * 2 + [2]  # DOWN ìœ„ì£¼
                            elif 'bull' in regime.lower():
                                predicted_actions = [0, 1, 2] + [2] * 2 + [1] * 3 + [2]  # UP í˜¼í•©
                            else:
                                predicted_actions = [0, 1, 2] + [2] * 3 + [1] * 2 + [2, 1]  # ê· í˜•
                        else:
                            # ì¤‘ë¦½ ì „ëµ: ê· í˜•ì¡íŒ ë¶„í¬ (ê° ë°©í–¥ ìµœì†Œ 1ê°œ ë³´ì¥)
                            if 'bull' in regime.lower():
                                predicted_actions = [0, 1, 2] + [1] * 3 + [0, 2] * 2 + [1]  # UP ìœ„ì£¼
                            elif 'bear' in regime.lower():
                                predicted_actions = [0, 1, 2] + [2] * 3 + [0, 1] * 2 + [2]  # DOWN ìœ„ì£¼
                            else:
                                predicted_actions = [0, 1, 2] + [0] * 2 + [1] * 2 + [2] * 2 + [0]  # ê· ë“±
                        
                        # ğŸ”¥ ê° ì˜ˆì¸¡ ë°©í–¥ë³„ë¡œ ê²½í—˜ ìƒì„± (ìµœì†Œ 5ê°œ, ìµœëŒ€ 20ê°œ, í•™ìŠµ ë°ì´í„° ì¦ê°€)
                        num_pred_experiences = max(5, min(len(predicted_actions), 20))
                        for action in predicted_actions[:num_pred_experiences]:
                            state = {
                                'rsi': market_info.get('rsi', 50.0),
                                'macd': market_info.get('macd', 0.0),
                                'volume_ratio': market_info.get('volume_ratio', 1.0),
                                'atr': market_info.get('atr', 0.02),
                                'adx': market_info.get('adx', 25.0),
                                'mfi': market_info.get('mfi', 50.0),
                                'bb_upper': market_info.get('bb_upper', 1.0),
                                'bb_middle': market_info.get('bb_middle', 1.0),
                                'bb_lower': market_info.get('bb_lower', 1.0),
                                'macd_signal': market_info.get('macd_signal', 0.0),
                                'close': market_info.get('close', 1.0),
                                'open': market_info.get('open', 1.0),
                                'high': market_info.get('high', 1.0),
                                'low': market_info.get('low', 1.0),
                                'volume': market_info.get('volume', 1.0),
                                'volatility': market_info.get('volatility', 0.02),
                                'regime_stage': market_info.get('regime_stage', 3),
                                'regime_confidence': market_info.get('regime_confidence', 0.5),
                                # ğŸš€ í™•ì¥ ì§€í‘œ ì¶”ê°€
                                'wave_progress': market_info.get('wave_progress', 0.5),
                                'pattern_confidence': market_info.get('pattern_confidence', predicted_conf),
                                'structure_score': market_info.get('structure_score', 0.5),
                                'sentiment': market_info.get('sentiment', 0.0),
                                'regime_transition_prob': market_info.get('regime_transition_prob', 0.05)
                            }

                            # ğŸš€ ë©”íƒ€ í•™ìŠµ: ë¶„ì„+ì „ëµ íŒŒë¼ë¯¸í„° í¬í•¨ ìƒíƒœ ë²¡í„° ìƒì„± (35ì°¨ì›)
                            enhanced_state_vec = build_state_vector_with_analysis_and_strategy(
                                state,
                                strategy_params,
                                fractal_score=fractal_score,
                                multi_timeframe_score=multi_timeframe_score,
                                indicator_cross_score=indicator_cross_score,
                                ensemble_score=ensemble_score,
                                ensemble_confidence=ensemble_confidence
                            )

                            # ğŸ”¥ ì˜ˆì¸¡ ì „ëµ: ì „ëµ ë°©í–¥ì„±, ë ˆì§, ì˜ˆì¸¡ ì‹ ë¢°ë„ ì¢…í•© ë³´ìƒ
                            base_reward = 0.0
                            
                            # ì „ëµ ë°©í–¥ê³¼ ì•¡ì…˜ ì¼ì¹˜ ì—¬ë¶€
                            if (action == 1 and strategy_direction == 'buy') or \
                               (action == 2 and strategy_direction == 'sell') or \
                               (action == 0 and strategy_direction == 'neutral'):
                                base_reward += 0.1
                            
                            # ë ˆì§ê³¼ ì•¡ì…˜ ì¼ì¹˜ ì—¬ë¶€
                            if (action == 1 and 'bull' in regime.lower()) or \
                               (action == 2 and 'bear' in regime.lower()) or \
                               (action == 0 and ('sideways' in regime.lower() or 'neutral' in regime.lower())):
                                base_reward += 0.1
                            else:
                                base_reward -= 0.05
                            
                            # ì˜ˆì¸¡ ì‹ ë¢°ë„ ë³´ì •
                            confidence_bonus = (predicted_conf - 0.5) * 0.2
                            
                            reward = base_reward + confidence_bonus
                            
                            experience = {
                                'episode': episode_num,
                                'agent_id': agent_id,
                                'state': enhanced_state_vec,  # ğŸ”¥ 25ì°¨ì› ë²¡í„°
                                'action': action,
                                'reward': reward,
                                'log_prob': -1.1,
                                'value': reward * 0.9,
                                'done': True
                            }
                            experiences.append(experience)
                        
                        break
        
        except Exception as e:
            logger.warning(f"âš ï¸ ë¶„ì„ ë°ì´í„° í¬í•¨ ê²½í—˜ ì¶”ì¶œ ì¤‘ ì¼ë¶€ ë°ì´í„° ì†ì‹¤: {e}")
            import traceback
            logger.debug(f"ìƒì„¸ ì—ëŸ¬:\n{traceback.format_exc()}")
        
        # ğŸ”¥ ë°ì´í„° ê²€ì¦ ê°•í™”: ì•¡ì…˜ ë‹¤ì–‘ì„± ì²´í¬
        if experiences:
            actions = [exp.get('action', 0) for exp in experiences]
            unique_actions = set(actions)
            action_counts = {action: actions.count(action) for action in unique_actions}
            
            # ì•¡ì…˜ ë¶„í¬ ë¡œê¹…
            logger.info(f"ğŸ“Š ê²½í—˜ ì¶”ì¶œ ì™„ë£Œ (ë¶„ì„ í¬í•¨): ì´ {len(experiences)}ê°œ, ê³ ìœ  ì•¡ì…˜: {len(unique_actions)}ê°œ")
            logger.info(f"   ì•¡ì…˜ ë¶„í¬: NEUTRAL(0)={action_counts.get(0, 0)}, UP(1)={action_counts.get(1, 0)}, DOWN(2)={action_counts.get(2, 0)}")
            
            # ğŸ”¥ ì•¡ì…˜ ë‹¤ì–‘ì„± ê°•ì œ ë³´ì •ì—ë„ ë¶ˆêµ¬í•˜ê³  ì—¬ì „íˆ 1ê°œë¼ë©´, ë§ˆì§€ë§‰ ì•ˆì „ì¥ì¹˜ë¡œ experiences ë¦¬ìŠ¤íŠ¸ ì§ì ‘ ì¡°ì‘
            if len(unique_actions) < 2:
                logger.warning(f"âš ï¸ ì•¡ì…˜ ë‹¤ì–‘ì„± ë¶€ì¡±: ê³ ìœ  ì•¡ì…˜ {len(unique_actions)}ê°œë§Œ ì¡´ì¬ (ê°•ì œ ì£¼ì… ì‹¤íŒ¨ ê°€ëŠ¥ì„±)")
                
                # ë§ˆì§€ë§‰ ì•ˆì „ì¥ì¹˜: ê¸°ì¡´ experience ì¤‘ ì¼ë¶€ë¥¼ ê°•ì œë¡œ UP/DOWNìœ¼ë¡œ ë³€í™˜
                # (ì´ë¯¸ state ë²¡í„°ëŠ” NEUTRALìš©ì¼ ìˆ˜ ìˆì§€ë§Œ, ë³´ìƒê³¼ ì•¡ì…˜ì„ ê°•ì œë¡œ ë°”ê¿”ì„œë¼ë„ ë‹¤ì–‘ì„± í™•ë³´)
                if len(experiences) >= 2:
                    # 1. UP ê°•ì œ ë³€í™˜
                    experiences[0]['action'] = 1
                    experiences[0]['reward'] = 0.0 # ì¤‘ë¦½ ë³´ìƒ (ì •ë³´ ì—†ìŒ)
                    
                    # 2. DOWN ê°•ì œ ë³€í™˜
                    experiences[1]['action'] = 2
                    experiences[1]['reward'] = 0.0 # ì¤‘ë¦½ ë³´ìƒ (ì •ë³´ ì—†ìŒ)
                    
                    # ë‹¤ì‹œ ê³„ì‚°
                    actions = [exp.get('action', 0) for exp in experiences]
                    unique_actions = set(actions)
                    action_counts = {action: actions.count(action) for action in unique_actions}
                    
                    logger.info(f"ğŸ”§ ìµœí›„ì˜ ì•ˆì „ì¥ì¹˜ ë°œë™: experiences ë¦¬ìŠ¤íŠ¸ ì§ì ‘ ìˆ˜ì •í•˜ì—¬ ë‹¤ì–‘ì„± í™•ë³´")
                    logger.info(f"   ìˆ˜ì • í›„ ë¶„í¬: {action_counts}")

            # ì¬ê²€ì¦
            if len(unique_actions) < 2:
                logger.error(f"âŒ í•™ìŠµ ì „ ê²€ì¦ ì‹¤íŒ¨: ì•¡ì…˜ ë‹¤ì–‘ì„± ë¶€ì¡± (ê³ ìœ  ì•¡ì…˜: {len(unique_actions)}ê°œ)")
                logger.error(f"   ì•¡ì…˜ ë¶„í¬: {action_counts}")
                logger.error(f"   í•™ìŠµì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤. ë°ì´í„° ìˆ˜ì§‘ ë¡œì§ì„ í™•ì¸í•˜ì„¸ìš”.")
                raise ValueError(f"ì•¡ì…˜ ë‹¤ì–‘ì„± ë¶€ì¡±: ê³ ìœ  ì•¡ì…˜ {len(unique_actions)}ê°œë§Œ ì¡´ì¬ (ìµœì†Œ 2ê°œ í•„ìš”)")
            elif len(unique_actions) == 2:
                # ğŸ†• NEUTRALì´ ì—†ëŠ” ê²½ìš°ëŠ” ì œí•œì ì´ë¯€ë¡œ WARNING
                has_neutral = action_counts.get(0, 0) > 0
                if not has_neutral:
                    logger.warning(f"âš ï¸ ì•¡ì…˜ ë‹¤ì–‘ì„± ì œí•œì : ê³ ìœ  ì•¡ì…˜ {len(unique_actions)}ê°œ (NEUTRAL ì—†ìŒ)")
                else:
                    logger.info(f"âœ… ì•¡ì…˜ ë‹¤ì–‘ì„± ì–‘í˜¸: ê³ ìœ  ì•¡ì…˜ {len(unique_actions)}ê°œ")
            else:
                logger.info(f"âœ… ì•¡ì…˜ ë‹¤ì–‘ì„± ìš°ìˆ˜: ê³ ìœ  ì•¡ì…˜ {len(unique_actions)}ê°œ (ëª¨ë“  ë°©í–¥ í¬í•¨)")
        else:
            logger.warning("âš ï¸ ê²½í—˜ ì¶”ì¶œ ê²°ê³¼ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤ (ë¶„ì„ í¬í•¨)")
        
        logger.debug(f"âœ… ì´ {len(experiences)}ê°œ ê²½í—˜ ì¶”ì¶œ ì™„ë£Œ (ë¶„ì„ ë°ì´í„° í¬í•¨)")
        return experiences
    
    def _ensure_neutral_trade_pool(
        self,
        buy_trades: List[Dict[str, Any]],
        sell_trades: List[Dict[str, Any]],
        hold_trades: List[Dict[str, Any]],
        episode_num: Optional[int] = None,
        agent_id: Optional[str] = None
    ) -> List[Dict[str, Any]]:
        """NEUTRAL ìƒ˜í”Œ ëª©í‘œ ë¹„ì¤‘ì„ ë§Œì¡±í•˜ë„ë¡ íŠ¸ë ˆì´ë“œ í’€ì„ ë³´ê°•"""
        hold_trades = list(hold_trades or [])
        directional_count = len(buy_trades) + len(sell_trades)
        total_reference = directional_count + len(hold_trades)
        if total_reference == 0:
            return hold_trades

        target_neutral = max(
            self.min_neutral_samples,
            math.ceil(total_reference * self.neutral_target_ratio)
        )
        target_neutral = min(target_neutral, self.max_neutral_samples)

        deficit = max(0, target_neutral - len(hold_trades))
        if deficit <= 0:
            return hold_trades

        synthetic_trades = self._generate_synthetic_neutral_trades(
            buy_trades + sell_trades,
            deficit
        )
        if synthetic_trades:
            hold_trades.extend(synthetic_trades)
            context = f"episode={episode_num}, agent={agent_id}" if episode_num is not None else "selfplay"
            logger.debug(
                f"ğŸ†• NEUTRAL ìƒ˜í”Œ ë³´ê°•: í•©ì„± {len(synthetic_trades)}ê°œ ì¶”ê°€ ({context})"
            )
        return hold_trades

    def _ensure_directional_trade_pool(
        self,
        buy_trades: List[Dict[str, Any]],
        sell_trades: List[Dict[str, Any]],
        hold_trades: List[Dict[str, Any]],
        episode_num: Optional[int] = None,
        agent_id: Optional[str] = None
    ) -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]]]:
        """BUY/SELL ìƒ˜í”Œì´ ëª¨ë‘ ì¡´ì¬í•˜ë„ë¡ ë¶€ì¡±í•œ ë°©í–¥ì„ í•©ì„±"""
        buy_trades = list(buy_trades or [])
        sell_trades = list(sell_trades or [])
        hold_trades = list(hold_trades or [])

        missing = []
        if not buy_trades:
            missing.append('BUY')
        if not sell_trades:
            missing.append('SELL')

        if not missing:
            return buy_trades, sell_trades

        source_pool = hold_trades + buy_trades + sell_trades
        if not source_pool:
            return buy_trades, sell_trades

        synthetic_summary = {}
        for direction in missing:
            synthetic = self._generate_synthetic_directional_trades(
                source_pool,
                direction,
                target_count=self.min_directional_samples
            )
            if synthetic:
                if direction == 'BUY':
                    buy_trades.extend(synthetic)
                else:
                    sell_trades.extend(synthetic)
                synthetic_summary[direction] = len(synthetic)

        if synthetic_summary:
            context = f"episode={episode_num}, agent={agent_id}" if episode_num is not None else "selfplay"
            logger.debug(
                f"ğŸ†• Directional ìƒ˜í”Œ ë³´ê°•: {synthetic_summary} ({context})"
            )

        return buy_trades, sell_trades

    def _generate_synthetic_directional_trades(
        self,
        source_trades: List[Dict[str, Any]],
        direction: str,
        target_count: int
    ) -> List[Dict[str, Any]]:
        """ì†ŒìŠ¤ íŠ¸ë ˆì´ë“œë¡œë¶€í„° BUY/SELL í•©ì„±"""
        if not source_trades or self.max_synthetic_directional <= 0:
            return []

        limit = min(max(1, target_count), self.max_synthetic_directional)
        threshold = self.neutral_direction_threshold
        ordered = sorted(
            source_trades,
            key=self._compute_trade_magnitude
        )

        synthetic: List[Dict[str, Any]] = []
        polarity = 1.0 if direction == 'BUY' else -1.0
        for trade in ordered:
            if len(synthetic) >= limit:
                break
            cloned = copy.deepcopy(trade)
            cloned['direction'] = 'BUY' if polarity > 0 else 'SELL'
            cloned['synthetic_directional'] = True
            
            # ğŸ”¥ ì¤‘ìš” ìˆ˜ì •: í•©ì„± ë°ì´í„°ëŠ” ì„ê³„ê°’ì„ í™•ì‹¤íˆ ë„˜ë„ë¡ ì„¤ì • (1.5ë°°)
            # thresholdê°€ 0ì´ë©´ ìµœì†Œ 0.02(2%) ë³€ë™í­ ë¶€ì—¬
            base_magnitude = threshold * 1.5 if threshold > 0 else 0.02
            cloned['price_change'] = base_magnitude * polarity
            
            if 'regime' in cloned:
                cloned['regime'] = cloned.get('regime', '').replace('neutral', 'bull' if polarity > 0 else 'bear')
            synthetic.append(cloned)

        return synthetic

    def _generate_synthetic_neutral_trades(
        self,
        source_trades: List[Dict[str, Any]],
        needed: int
    ) -> List[Dict[str, Any]]:
        """BUY/SELL íŠ¸ë ˆì´ë“œì—ì„œ í•©ì„± NEUTRAL íŠ¸ë ˆì´ë“œë¥¼ ìƒì„±"""
        if not source_trades or needed <= 0 or self.max_synthetic_neutral == 0:
            return []

        limit = min(needed, self.max_synthetic_neutral)
        threshold = self.neutral_direction_threshold

        def sort_key(trade):
            return self._compute_trade_magnitude(trade)

        small_moves = [t for t in source_trades if self._compute_trade_magnitude(t) <= threshold]
        large_moves = [t for t in source_trades if self._compute_trade_magnitude(t) > threshold]
        ordered = sorted(small_moves, key=sort_key) + sorted(large_moves, key=sort_key)

        synthetic: List[Dict[str, Any]] = []
        for trade in ordered:
            if len(synthetic) >= limit:
                break
            cloned = copy.deepcopy(trade)
            cloned['direction'] = 'HOLD'
            cloned['synthetic_neutral'] = True
            cloned['price_change'] = 0.0
            # ìƒíƒœ ê´€ë ¨ í•„ë“œê°€ ì—†ì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
            for volatility_key in ['volatility', 'atr']:
                if volatility_key in cloned:
                    cloned[volatility_key] = max(0.0, float(cloned.get(volatility_key, 0.0)))
            # ë¡œê·¸ ì¶”ì ìš© íŒíŠ¸
            original_dir = trade.get('direction', 'UNKNOWN')
            cloned['neutral_source'] = original_dir
            synthetic.append(cloned)

        return synthetic

    @staticmethod
    def _compute_trade_magnitude(trade: Dict[str, Any]) -> float:
        """íŠ¸ë ˆì´ë“œì˜ ë°©í–¥ ê°•ë„ë¥¼ ê·¼ì‚¬í•˜ëŠ” ì§€í‘œ"""
        for key in ('price_change', 'pnl_pct', 'return_pct', 'delta', 'drift'):
            value = trade.get(key)
            if value is not None:
                try:
                    return abs(float(value))
                except (TypeError, ValueError):
                    continue
        return 0.0

    def _select_trades_with_diversity(self, buy_trades, sell_trades, hold_trades):
        """BUY/SELL/NEUTRAL ë¹„ì¤‘ì„ ê· í˜• ìˆê²Œ ì„ íƒ"""
        if not buy_trades and not sell_trades and not hold_trades:
            return []

        max_dir = max(1, self.max_direction_samples)
        selected_buy = buy_trades[:max(min(1, len(buy_trades)), min(len(buy_trades), max_dir))]
        selected_sell = sell_trades[:max(min(1, len(sell_trades)), min(len(sell_trades), max_dir))]

        min_hold = min(1, len(hold_trades))
        base_hold_target = max(min_hold, self.min_neutral_samples)
        initial_hold_target = min(len(hold_trades), min(self.max_neutral_samples, base_hold_target))
        selected_hold = hold_trades[:initial_hold_target]
        hold_used = initial_hold_target

        selected = selected_buy + selected_sell + selected_hold

        if selected:
            current_neutral = sum(1 for t in selected if t.get('direction') not in ['BUY', 'SELL'])
            required_neutral = max(self.min_neutral_samples, math.ceil(len(selected) * self.neutral_target_ratio))
            required_neutral = min(required_neutral, self.max_neutral_samples, len(hold_trades))
            deficit = max(0, required_neutral - current_neutral)
            if deficit > 0 and hold_used < len(hold_trades):
                additional = min(deficit, len(hold_trades) - hold_used)
                if additional > 0:
                    selected.extend(hold_trades[hold_used:hold_used + additional])
                    hold_used += additional

        selected_directions = [t.get('direction') for t in selected]
        if 'BUY' not in selected_directions and buy_trades:
            selected.append(buy_trades[0])
            selected_directions.append('BUY')
        if 'SELL' not in selected_directions and sell_trades:
            selected.append(sell_trades[0])
            selected_directions.append('SELL')
        if not any(d not in ['BUY', 'SELL'] for d in selected_directions) and hold_trades:
            fallback_idx = min(len(hold_trades) - 1, hold_used) if hold_trades else 0
            fallback_trade = hold_trades[max(0, fallback_idx)]
            selected.append(fallback_trade)
            selected_directions.append(fallback_trade.get('direction', 'HOLD'))

        return selected
    
    def _create_batches(self, experiences: List[Dict], batch_size: int) -> List[List[Dict]]:
        """ê²½í—˜ì„ ë°°ì¹˜ë¡œ ë¶„í• """
        batches = []
        
        for i in range(0, len(experiences), batch_size):
            batch = experiences[i:i + batch_size]
            batches.append(batch)
        
        return batches
    
    def _update_policy(self, batch: List[Dict]) -> float:
        """
        PPO ì •ì±… ì—…ë°ì´íŠ¸ (ì‹¤ì œ í•™ìŠµ ì•Œê³ ë¦¬ì¦˜)
        
        Args:
            batch: ê²½í—˜ ë°°ì¹˜ [{state, action, reward, log_prob, value, done}, ...]
        
        Returns:
            í‰ê·  ì†ì‹¤
        """
        if not JAX_AVAILABLE or optax is None:
            logger.error("âŒ JAX ë˜ëŠ” optaxë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return 0.0
        
        try:
            # ë°°ì¹˜ ë°ì´í„° ì¶”ì¶œ ë° ê²€ì¦
            if not batch:
                return 0.0
            
            # í•„ìˆ˜ í•„ë“œ í™•ì¸ ë° ê¸°ë³¸ê°’ ì„¤ì •
            states = []
            actions = []
            rewards = []
            old_log_probs = []
            old_values = []
            analysis_scores = []  # ğŸ†• ë¶„ì„ ì ìˆ˜ ë¦¬ìŠ¤íŠ¸
            
            for exp in batch:
                # State ë²¡í„° ì¶”ì¶œ
                state = exp.get('state')
                if state is None:
                    # agent_idë‚˜ ë‹¤ë¥¸ í•„ë“œì—ì„œ state ì¬êµ¬ì„± ì‹œë„
                    continue
                
                state_vec = build_state_vector(state) if isinstance(state, dict) else np.array(state, dtype=np.float32)
                states.append(state_vec)
                
                # ğŸ”¥ ì˜ˆì¸¡ ì „ëµ: Action (0=NEUTRAL, 1=UP, 2=DOWN)
                action = exp.get('action', exp.get('action_idx', 0))
                actions.append(int(action))
                
                # Reward (ì •ê·œí™” ë° ìŠ¤ì¼€ì¼ë§)
                reward = float(exp.get('reward', 0.0))
                
                # ğŸ”¥ ë³´ìƒ ì •ê·œí™”: ë§¤ìš° ìŒìˆ˜ì¸ ë³´ìƒì„ ì™„í™”
                # ë³´ìƒ ë²”ìœ„ë¥¼ -1.0 ~ 1.0ìœ¼ë¡œ ì •ê·œí™”í•˜ë˜, ì›ë˜ ë¶€í˜¸ ìœ ì§€
                if reward < -1.0:
                    # ê³¼ë„í•œ ìŒìˆ˜ ë³´ìƒì„ -1.0ìœ¼ë¡œ í´ë¦¬í•‘ (í•™ìŠµ ì•ˆì •ì„±)
                    reward = max(-1.0, reward / 10.0)  # -10.0 â†’ -1.0 ìŠ¤ì¼€ì¼ë§
                elif reward > 1.0:
                    # ê³¼ë„í•œ ì–‘ìˆ˜ ë³´ìƒë„ í´ë¦¬í•‘
                    reward = min(1.0, reward)
                
                rewards.append(reward)
                
                # Old log probability (ì—†ìœ¼ë©´ 0ìœ¼ë¡œ ì‹œì‘)
                old_log_prob = float(exp.get('log_prob', exp.get('old_log_prob', -1.1)))  # ê¸°ë³¸ê°’: log(1/3) â‰ˆ -1.1
                old_log_probs.append(old_log_prob)
                
                # Old value estimate
                old_value = float(exp.get('value', exp.get('old_value', 0.0)))
                old_values.append(old_value)

                # ğŸ†• Analysis Score (Target)
                analysis_score = float(exp.get('analysis_score', 50.0))  # ê¸°ë³¸ê°’ 50 (ì¤‘ê°„)
                analysis_scores.append(analysis_score)
            
            if not states:
                logger.warning("âš ï¸ ë°°ì¹˜ì— ìœ íš¨í•œ stateê°€ ì—†ìŠµë‹ˆë‹¤.")
                return 0.0
            
            # ğŸ”¥ ê°œì„ : ìƒíƒœ ë²¡í„° ë°°ì¹˜ ì •ê·œí™” (í•™ìŠµ ì•ˆì •ì„± í–¥ìƒ)
            states_np = np.array(states, dtype=np.float32)
            states_np = np.nan_to_num(states_np, nan=0.0, posinf=1.0, neginf=-1.0)
            
            # ë°°ì¹˜ ë‹¨ìœ„ Z-score ì •ê·œí™” (ê° í”¼ì²˜ë³„ë¡œ)
            states_mean = np.mean(states_np, axis=0, keepdims=True)
            states_std = np.std(states_np, axis=0, keepdims=True) + 1e-8  # 0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€
            states_normalized = (states_np - states_mean) / states_std
            
            # ì •ê·œí™” í›„ í´ë¦¬í•‘ (ì´ìƒì¹˜ ì œê±°)
            states_normalized = np.clip(states_normalized, -3.0, 3.0)  # Â±3 í‘œì¤€í¸ì°¨ ë²”ìœ„
            
            states_jax = jnp.array(states_normalized, dtype=jnp.float32)
            
            actions_jax = jnp.array(actions, dtype=jnp.int32)
            
            rewards_np = np.array(rewards, dtype=np.float32)
            rewards_np = np.nan_to_num(rewards_np, nan=0.0, posinf=1e6, neginf=-1e6)
            rewards_jax = jnp.array(rewards_np, dtype=jnp.float32)
            
            old_log_probs_np = np.array(old_log_probs, dtype=np.float32)
            old_log_probs_np = np.nan_to_num(old_log_probs_np, nan=-1.1, posinf=10.0, neginf=-10.0)
            old_log_probs_jax = jnp.array(old_log_probs_np, dtype=jnp.float32)
            
            old_values_np = np.array(old_values, dtype=np.float32)
            old_values_np = np.nan_to_num(old_values_np, nan=0.0, posinf=1e6, neginf=-1e6)
            old_values_jax = jnp.array(old_values_np, dtype=jnp.float32)

            analysis_scores_np = np.array(analysis_scores, dtype=np.float32)
            analysis_scores_jax = jnp.array(analysis_scores_np, dtype=jnp.float32)
            
            # PPO í•˜ì´í¼íŒŒë¼ë¯¸í„°
            clip_epsilon = self.train_config.get('clip_epsilon', 0.2)
            value_loss_coef = self.train_config.get('value_loss_coef', 0.5)
            
            # ğŸ”¥ í•™ìŠµ ì„±ëŠ¥ ê°œì„ : íƒí—˜(Exploration) ê°•í™” (ì¬í•™ìŠµ ê¶Œì¥ ë°˜ì˜)
            # ğŸ”¥ ëˆ„ì  ì¦ê°€ ë°©ì‹: ë‹¤ì–‘ì„±ì´ ë¶€ì¡±í•˜ë©´ ì ì§„ì ìœ¼ë¡œ ì¦ê°€, ê°œì„ ë˜ë©´ ì²œì²œíˆ ê°ì†Œ

            # ì•¡ì…˜ ë‹¤ì–‘ì„± ì²´í¬ (HOLDë§Œ ìˆëŠ”ì§€ í™•ì¸)
            unique_actions = len(set(actions))
            action_counts = {action: actions.count(action) for action in set(actions)}
            hold_count = action_counts.get(0, 0)
            hold_ratio = hold_count / len(actions) if actions else 0.0

            neutral_target = self.neutral_target_ratio
            severe_shortage = max(0.02, neutral_target * 0.25)

            # ğŸ”¥ ì˜ˆì¸¡ ì „ëµ: NEUTRALë§Œ ìˆëŠ” ê²½ìš° íƒí—˜ ê°•í™” (ëˆ„ì  ì¦ê°€)
            # NEUTRAL(ì¤‘ë¦½ ì˜ˆì¸¡)ë§Œ í•˜ë©´ ì˜ˆì¸¡ ì •ë³´ê°€ ì—†ìœ¼ë¯€ë¡œ UP/DOWN ì˜ˆì¸¡ì„ ìœ ë„
            # ë” ì ê·¹ì ì¸ íƒí—˜ìœ¼ë¡œ ì•¡ì…˜ ë‹¤ì–‘ì„± í™•ë³´
            if unique_actions == 1:
                # ëª¨ë“  ì•¡ì…˜ì´ ë™ì¼í•˜ë©´ íƒí—˜ì„ í¬ê²Œ ì¦ê°€ (ëˆ„ì )
                self.current_entropy_coef = min(self.current_entropy_coef * 2.0, self.base_entropy_coef * 200.0)
                logger.warning(f"ğŸ” ì˜ˆì¸¡ ë‹¤ì–‘ì„± ì‹¬ê° ë¶€ì¡± (ê³ ìœ  ì•¡ì…˜: {unique_actions}), entropy_coef ëˆ„ì  ì¦ê°€: {self.current_entropy_coef:.4f}")
            elif hold_ratio < severe_shortage:
                self.current_entropy_coef = min(self.current_entropy_coef * 1.7, self.base_entropy_coef * 120.0)
                logger.warning(f"ğŸ” NEUTRAL ì•¡ì…˜ ê³ ê°ˆ (ë¹„ìœ¨: {hold_ratio:.1%} < ëª©í‘œ {neutral_target:.0%}), entropy_coef ì¦ê°€: {self.current_entropy_coef:.4f}")
            elif hold_ratio < neutral_target:
                self.current_entropy_coef = min(self.current_entropy_coef * 1.4, self.base_entropy_coef * 60.0)
                logger.info(f"ğŸ” NEUTRAL ë¹„ìœ¨ ë¶€ì¡± (í˜„ì¬ {hold_ratio:.1%}, ëª©í‘œ {neutral_target:.0%}), entropy_coef ëˆ„ì  ì¦ê°€: {self.current_entropy_coef:.4f}")
            elif hold_ratio > 0.95:
                # 95% ì´ìƒì´ NEUTRALì´ë©´ íƒí—˜ì„ í¬ê²Œ ì¦ê°€ (ëˆ„ì )
                self.current_entropy_coef = min(self.current_entropy_coef * 1.8, self.base_entropy_coef * 150.0)
                logger.warning(f"ğŸ” ì˜ˆì¸¡ ë‹¤ì–‘ì„± ì‹¬ê° ë¶€ì¡± (NEUTRAL ë¹„ìœ¨: {hold_ratio:.1%}), entropy_coef ëˆ„ì  ì¦ê°€: {self.current_entropy_coef:.4f}")
            elif hold_ratio > 0.9:
                # 90% ì´ìƒì´ NEUTRALì´ë©´ íƒí—˜ì„ ì¦ê°€ (ëˆ„ì )
                self.current_entropy_coef = min(self.current_entropy_coef * 1.5, self.base_entropy_coef * 75.0)
                logger.info(f"ğŸ” ì˜ˆì¸¡ ë‹¤ì–‘ì„± ë¶€ì¡± ê°ì§€ (NEUTRAL ë¹„ìœ¨: {hold_ratio:.1%}), entropy_coef ëˆ„ì  ì¦ê°€: {self.current_entropy_coef:.4f}")
            elif unique_actions == 2:
                # ğŸ†• 2ì¢…ë¥˜ ì•¡ì…˜ë§Œ ìˆëŠ” ê²½ìš° (NEUTRALì´ ì—†ëŠ” ê²½ìš°) - ë” ê°•ë ¥í•œ íƒí—˜
                if hold_ratio < max(0.1, neutral_target * 0.5):
                    self.current_entropy_coef = min(self.current_entropy_coef * 1.5, self.base_entropy_coef * 100.0)
                    logger.warning(f"ğŸ” ì˜ˆì¸¡ ë‹¤ì–‘ì„± ì‹¬ê° ë¶€ì¡± (ê³ ìœ  ì•¡ì…˜: {unique_actions}, NEUTRAL ë¹„ìœ¨: {hold_ratio:.1%}), entropy_coef ê°•ë ¥ ì¦ê°€: {self.current_entropy_coef:.4f}")
                else:
                    self.current_entropy_coef = min(self.current_entropy_coef * 1.3, self.base_entropy_coef * 50.0)
                    logger.info(f"ğŸ” ì˜ˆì¸¡ ë‹¤ì–‘ì„± ë¶€ì¡± ê°ì§€ (ê³ ìœ  ì•¡ì…˜: {unique_actions}, NEUTRAL ë¹„ìœ¨: {hold_ratio:.1%}), entropy_coef ëˆ„ì  ì¦ê°€: {self.current_entropy_coef:.4f}")
            elif hold_ratio > 0.7:
                # 70% ì´ìƒì´ NEUTRALì´ë©´ ì¤‘ê°„ ì¦ê°€ (ëˆ„ì )
                self.current_entropy_coef = min(self.current_entropy_coef * 1.3, self.base_entropy_coef * 30.0)
                logger.info(f"ğŸ” ì˜ˆì¸¡ ë‹¤ì–‘ì„± ë¶€ì¡± ê°ì§€ (NEUTRAL ë¹„ìœ¨: {hold_ratio:.1%}), entropy_coef ëˆ„ì  ì¦ê°€: {self.current_entropy_coef:.4f}")
            elif unique_actions == 3 and hold_ratio > 0.5:
                # 3ì¢…ë¥˜ ëª¨ë‘ ìˆì§€ë§Œ NEUTRALì´ ì ˆë°˜ ì´ìƒì´ë©´ ì•½ê°„ ì¦ê°€ (ëˆ„ì )
                self.current_entropy_coef = min(self.current_entropy_coef * 1.1, self.base_entropy_coef * 8.0)
                logger.debug(f"ğŸ” ì˜ˆì¸¡ ë‹¤ì–‘ì„± ë³´í†µ (ê³ ìœ  ì•¡ì…˜: {unique_actions}, NEUTRAL ë¹„ìœ¨: {hold_ratio:.1%}), entropy_coef ì•½ê°„ ì¦ê°€: {self.current_entropy_coef:.4f}")
            else:
                # ë‹¤ì–‘í•œ ì•¡ì…˜ì´ ìˆìœ¼ë©´ ì²œì²œíˆ ê°ì†Œ (ê¸°ë³¸ê°’ìœ¼ë¡œ ë³µê·€)
                self.current_entropy_coef = max(self.current_entropy_coef * 0.95, self.base_entropy_coef)
                logger.debug(f"âœ… ì˜ˆì¸¡ ë‹¤ì–‘ì„± ì–‘í˜¸ (ê³ ìœ  ì•¡ì…˜: {unique_actions}, NEUTRAL ë¹„ìœ¨: {hold_ratio:.1%}), entropy_coef: {self.current_entropy_coef:.4f}")

            entropy_coef = self.current_entropy_coef
            
            gamma = self.train_config.get('gamma', 0.99)  # í• ì¸ìœ¨
            gae_lambda = self.train_config.get('gae_lambda', 0.95)  # GAE lambda
            
            # ğŸ”¥ í•™ìŠµ ì„±ëŠ¥ ê°œì„ : ë³´ìƒ ì •ê·œí™” ë° Shaping ê°•í™”
            # ëª¨ë“  ë³´ìƒì´ ìŒìˆ˜ì¸ ê²½ìš° í•™ìŠµì´ ì–´ë ¤ìš¸ ìˆ˜ ìˆìœ¼ë¯€ë¡œ, ë³´ìƒì„ ì ê·¹ì ìœ¼ë¡œ ì •ê·œí™”
            rewards_mean = float(jnp.mean(rewards_jax))
            rewards_std = float(jnp.std(rewards_jax))
            rewards_min = float(jnp.min(rewards_jax))
            rewards_max = float(jnp.max(rewards_jax))
            
            # í‘œì¤€í™”ë¥¼ ìœ„í•œ ìµœì†Œ std ê°’ (0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€)
            min_std = 1e-6
            
            # ğŸ”¥ ë³´ìƒ Shaping: ìŒìˆ˜ ë³´ìƒì— ë” ì ê·¹ì ì¸ ì²˜ë¦¬
            if rewards_mean < -0.1:  # í‰ê·  ë³´ìƒì´ ìŒìˆ˜ì¸ ê²½ìš°
                # 1. ë³´ìƒì„ ì–‘ìˆ˜ ì˜ì—­ìœ¼ë¡œ ì´ë™ (ìƒìˆ˜ ì¶”ê°€)
                # ëª©í‘œ: í‰ê· ì„ 0 ê·¼ì²˜ë¡œ ì´ë™ - ë” ì ê·¹ì ìœ¼ë¡œ
                shift_amount = abs(rewards_mean) * 0.8  # í‰ê· ì˜ 80%ë§Œí¼ ì´ë™ (ë” ê°•ë ¥í•˜ê²Œ)
                rewards_jax = rewards_jax + shift_amount
                rewards_mean = float(jnp.mean(rewards_jax))
                rewards_std = float(jnp.std(rewards_jax))  # std ì¬ê³„ì‚°
                logger.info(f"ğŸ” ë³´ìƒ Shaping: ìŒìˆ˜ ë³´ìƒì— {shift_amount:.4f} ì¶”ê°€ (ìƒˆ í‰ê· : {rewards_mean:.4f})")
            
            # ğŸ”¥ ë³´ìƒ ë‹¤ì–‘ì„± ê°•í™”: ì•¡ì…˜ë³„ë¡œ ë‹¤ë¥¸ ë³´ìƒ ë¶€ì—¬
            if rewards_std < min_std:
                # ë³´ìƒì´ ê±°ì˜ ë™ì¼í•˜ë©´ (ëª¨ë‘ HOLDë¡œ ì¸í•œ ìŒìˆ˜ ë³´ìƒ)
                # ë” ì ê·¹ì ì¸ ë³´ìƒ ë‹¤ì–‘ì„± ë¶€ì—¬
                # 1. ì•¡ì…˜ë³„ë¡œ ë‹¤ë¥¸ ë³´ë„ˆìŠ¤/í˜ë„í‹° ì¶”ê°€ (ê°•í™”ë¨)
                actions_jax = jnp.array(actions, dtype=jnp.int32)
                # ğŸ”¥ ê°œì„ : BUY/SELL ë³´ë„ˆìŠ¤ ì¦ê°€ (0.1 â†’ 0.3), HOLDì— í˜ë„í‹° ì¶”ê°€
                direction_bonus = max(0.0, self.direction_action_bonus * 3.0)
                neutral_penalty = -abs(self.direction_action_bonus) * 0.5
                action_bonuses = jnp.where(
                    (actions_jax == 1) | (actions_jax == 2),
                    direction_bonus,
                    neutral_penalty
                )
                rewards_jax = rewards_jax + action_bonuses
                
                # 2. ì¶”ê°€ ì–‘ìˆ˜ ë³´ë„ˆìŠ¤ (ì „ì²´ì—) - ê°•í™”
                bonus = max(0.2, abs(rewards_mean) * 0.5)  # ğŸ”¥ ê°œì„ : ìµœì†Œ 0.2, í‰ê· ì˜ 50% (ì´ì „: 0.1, 30%)
                rewards_jax = rewards_jax + bonus

                # 3. ëœë¤ ë…¸ì´ì¦ˆ ì¶”ê°€ (ë³´ìƒ ë‹¤ì–‘ì„± ê°•í™”) - ê°•í™”
                rng_key = jax.random.PRNGKey(int(np.random.randint(0, 2**32)))
                noise = jax.random.normal(rng_key, shape=rewards_jax.shape) * 0.1  # ğŸ”¥ ê°œì„ : 10% ë…¸ì´ì¦ˆ (ì´ì „: 5%)
                rewards_jax = rewards_jax + noise

                rewards_std = float(jnp.std(rewards_jax))  # std ì¬ê³„ì‚°
                logger.warning(f"ğŸ” ë³´ìƒ ë‹¤ì–‘ì„± ì‹¬ê° ë¶€ì¡± (std={rewards_std:.6f}), ì•¡ì…˜ë³„ ë³´ë„ˆìŠ¤/í˜ë„í‹° + ì „ì²´ ë³´ë„ˆìŠ¤ {bonus:.4f} + ë…¸ì´ì¦ˆ ì¶”ê°€")
            else:
                # ë³´ìƒ ì •ê·œí™” (Z-score ì •ê·œí™” í›„ tanhë¡œ ìŠ¤ì¼€ì¼ë§)
                rewards_normalized = (rewards_jax - rewards_mean) / (rewards_std + min_std)
                # -3 ~ +3 ë²”ìœ„ë¥¼ -1 ~ +1ë¡œ ìŠ¤ì¼€ì¼ë§ (ë” ë¶€ë“œëŸ¬ìš´ ìŠ¤ì¼€ì¼ë§)
                rewards_jax = jnp.tanh(rewards_normalized * 0.5)
                logger.debug(f"ğŸ” ë³´ìƒ ì •ê·œí™” ì™„ë£Œ (mean={rewards_mean:.4f}, std={rewards_std:.4f})")
            
            # ìµœì¢… í´ë¦¬í•‘ (ì•ˆì „ì¥ì¹˜) - ë” ë„“ì€ ë²”ìœ„ í—ˆìš©
            rewards_jax = jnp.clip(rewards_jax, -2.0, 2.0)  # -1.0 ~ 1.0 â†’ -2.0 ~ 2.0
            
            # ìµœì¢… ë³´ìƒ í†µê³„ ë¡œê¹…
            final_mean = float(jnp.mean(rewards_jax))
            final_std = float(jnp.std(rewards_jax))
            logger.debug(f"ğŸ“Š ìµœì¢… ë³´ìƒ í†µê³„: mean={final_mean:.4f}, std={final_std:.4f}, range=[{rewards_min:.4f}, {rewards_max:.4f}]")
            
            # ğŸ”§ GAE (Generalized Advantage Estimation) êµ¬í˜„
            # 1. Discounted returns ê³„ì‚°
            returns = _compute_returns(rewards_jax, gamma)
            
            # returnsì™€ old_valuesì˜ shape ì¼ì¹˜ í™•ì¸
            if returns.shape != old_values_jax.shape:
                logger.warning(f"âš ï¸ Returnsì™€ Values shape ë¶ˆì¼ì¹˜: returns={returns.shape}, values={old_values_jax.shape}")
                # shape ë§ì¶”ê¸°
                if old_values_jax.ndim == 0:
                    old_values_jax = old_values_jax.reshape(-1)
                if returns.ndim == 0:
                    returns = returns.reshape(-1)
                min_len = min(len(returns), len(old_values_jax))
                returns = returns[:min_len]
                old_values_jax = old_values_jax[:min_len]
            
            # 2. GAE ê³„ì‚° (ë” ì •í™•í•œ advantage ì¶”ì •)
            advantages = _compute_gae(
                rewards=rewards_jax[:min_len] if 'min_len' in locals() else rewards_jax,
                values=old_values_jax,
                gamma=gamma,
                lam=gae_lambda
            )
            
            # advantagesì™€ returnsì˜ shape ì¼ì¹˜ í™•ì¸
            if advantages.shape != returns.shape:
                logger.warning(f"âš ï¸ Advantagesì™€ Returns shape ë¶ˆì¼ì¹˜: advantages={advantages.shape}, returns={returns.shape}")
                min_adv_len = min(len(advantages), len(returns))
                advantages = advantages[:min_adv_len]
                returns = returns[:min_adv_len]
                old_log_probs_jax = old_log_probs_jax[:min_adv_len]
                actions_jax = actions_jax[:min_adv_len]
                states_jax = states_jax[:min_adv_len]
            
            # Advantage ì •ê·œí™” (ì•ˆì „í•œ ë°©ì‹)
            try:
                advantages_mean = jnp.mean(advantages)
                advantages_std = jnp.std(advantages)
                
                # stdê°€ ë„ˆë¬´ ì‘ìœ¼ë©´ ì •ê·œí™” ìŠ¤í‚µ
                if advantages_std > 1e-6:
                    advantages_normalized = (advantages - advantages_mean) / (advantages_std + 1e-8)
                    # í´ë¦¬í•‘ (ê³¼ë„í•œ ê°’ ë°©ì§€)
                    advantages_normalized = jnp.clip(advantages_normalized, -10.0, 10.0)
                else:
                    # stdê°€ ë„ˆë¬´ ì‘ìœ¼ë©´ ì •ê·œí™” ì—†ì´ ì‚¬ìš©
                    advantages_normalized = advantages - advantages_mean
                    advantages_normalized = jnp.clip(advantages_normalized, -10.0, 10.0)
            except Exception as norm_err:
                logger.warning(f"âš ï¸ Advantage ì •ê·œí™” ì‹¤íŒ¨, ì›ë³¸ ì‚¬ìš©: {norm_err}")
                advantages_normalized = jnp.clip(advantages, -10.0, 10.0)
            
            # ğŸ”¥ ë°°ì¹˜ í¬ê¸° ì‚¬ì „ ì¡°ì • (loss_fn ì •ì˜ ì „ì— ìˆ˜í–‰)
            # í´ë¡œì € ë³€ìˆ˜ ìº¡ì²˜ë¥¼ ìœ„í•´ ì¡°ê±´ë¶€ ì¬í• ë‹¹ ì œê±°
            actual_batch_size = states_jax.shape[0] if states_jax.ndim > 0 else 0
            max_safe_batch = 2048  # ğŸ”¥ ì„±ëŠ¥ ê°œì„ : 256 -> 2048ë¡œ ëŒ€í­ ìƒí–¥
            if actual_batch_size > max_safe_batch:
                logger.warning(f"âš ï¸ ë°°ì¹˜ í¬ê¸° ì´ˆê³¼ ({actual_batch_size} > {max_safe_batch}), ì²˜ìŒ {max_safe_batch}ê°œë§Œ ì‚¬ìš©")
                # âœ… ë¬´ì¡°ê±´ ìŠ¬ë¼ì´ì‹±ìœ¼ë¡œ ì¬í• ë‹¹ (í´ë¡œì € ìº¡ì²˜ ë³´ì¥)
                states_jax = states_jax[:max_safe_batch]
                actions_jax = actions_jax[:max_safe_batch]
                old_log_probs_jax = old_log_probs_jax[:max_safe_batch]
                old_values_jax = old_values_jax[:max_safe_batch]
                rewards_jax = rewards_jax[:max_safe_batch]
                analysis_scores_jax = analysis_scores_jax[:max_safe_batch]  # ğŸ†•
                advantages_normalized = advantages_normalized[:max_safe_batch]
                returns = returns[:max_safe_batch]
                actual_batch_size = max_safe_batch
            
            # ì†ì‹¤ í•¨ìˆ˜ ì •ì˜ (í´ë¡œì € ë³€ìˆ˜ë“¤ì„ ëª…ì‹œì ìœ¼ë¡œ ìº¡ì²˜)
            # ëª¨ë“  ì™¸ë¶€ ë³€ìˆ˜ë¥¼ loss_fn ì •ì˜ ì‹œì ì—ì„œ ì•ˆì „í•˜ê²Œ ìº¡ì²˜
            def loss_fn(params):
                try:
                    # í˜„ì¬ ì •ì±…ìœ¼ë¡œ forward pass
                    model = self.model['model_def']
                    
                    # ì…ë ¥ shape í™•ì¸ (batch_size, feature_dim)
                    batch_size = states_jax.shape[0]
                    if batch_size == 0:
                        # ë¹ˆ ë°°ì¹˜ ì²˜ë¦¬ (í•™ìŠµ ì¤‘ë‹¨ ë°©ì§€)
                        safe_loss = jnp.array(0.0)
                        return safe_loss, (safe_loss, safe_loss, safe_loss)
                    
                    # ğŸ”§ ì…ë ¥ ë°ì´í„° ê²€ì¦ ë° í´ë¦¬í•‘
                    states_safe = jnp.clip(states_jax, -10.0, 10.0)
                    states_safe = jnp.nan_to_num(states_safe, nan=0.0, posinf=10.0, neginf=-10.0)
                    
                    # ğŸ”§ ì…ë ¥ shape í™•ì¸ (ìµœì†Œ 2D í•„ìš”: batch_size, feature_dim)
                    if states_safe.ndim == 1:
                        states_safe = states_safe.reshape(1, -1)
                    elif states_safe.ndim == 0:
                        logger.warning(f"âš ï¸ States shape ì´ìƒ: {states_safe.shape}, ë¹ˆ ë°°ì¹˜ ë°˜í™˜")
                        safe_loss = jnp.array(0.0)
                        return safe_loss, (safe_loss, safe_loss, safe_loss)
                    
                    # ğŸ”¥ ì…ë ¥ ë°ì´í„° ì¶”ê°€ ê²€ì¦
                    if not jnp.all(jnp.isfinite(states_safe)):
                        logger.warning(f"âš ï¸ Statesì— NaN/Inf ë°œê²¬, ì œê±° í›„ ì¬ê²€ì¦")
                        states_safe = jnp.nan_to_num(states_safe, nan=0.0, posinf=10.0, neginf=-10.0)
                    
                    # Feature ì°¨ì› ê²€ì¦
                    expected_feature_dim = self.model.get('obs_dim', 25)
                    if states_safe.shape[-1] != expected_feature_dim:
                        logger.error(f"âŒ Feature ì°¨ì› ë¶ˆì¼ì¹˜: {states_safe.shape[-1]} != {expected_feature_dim}")
                        safe_loss = jnp.array(0.0)
                        return safe_loss, (safe_loss, safe_loss, safe_loss)
                    
                    # ğŸ”§ params í˜•ì‹ ê²€ì¦ ë° ì •ê·œí™”
                    # Flax model.init()ì€ {"params": {...}} êµ¬ì¡°ë¥¼ ë°˜í™˜í•¨
                    # self.model['params']ëŠ” ì´ë¯¸ {"params": {...}} êµ¬ì¡°
                    # loss_fnì—ì„œ ë°›ëŠ” paramsë„ ë™ì¼í•œ êµ¬ì¡°ì´ë¯€ë¡œ, ì¤‘ë³µ ë˜í•‘ ë°©ì§€
                    if not isinstance(params, dict):
                        logger.warning(f"âš ï¸ Params í˜•ì‹ ì´ìƒ: {type(params)}, dictë¡œ ë³€í™˜ ì‹œë„")
                        try:
                            # JAX/FrozenDictë¥¼ dictë¡œ ë³€í™˜ ì‹œë„
                            if hasattr(params, '__dict__'):
                                params = dict(params)
                            else:
                                params = {'params': params}
                        except:
                            logger.error(f"âŒ Params ë³€í™˜ ì‹¤íŒ¨: {type(params)}")
                            safe_loss = jnp.array(0.0)
                            return safe_loss, (safe_loss, safe_loss, safe_loss)
                    
                    # ğŸ”§ params êµ¬ì¡° í™•ì¸ ë° ì •ê·œí™”
                    # model.init()ì€ {"params": {...}} ë°˜í™˜
                    # ì²´í¬í¬ì¸íŠ¸ì—ì„œ ë¡œë“œí•œ ê²½ìš°ë„ ë™ì¼ êµ¬ì¡°
                    # ë”°ë¼ì„œ ì´ë¯¸ ì˜¬ë°”ë¥¸ êµ¬ì¡°ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
                    if 'params' in params:
                        # ì´ë¯¸ {"params": {...}} êµ¬ì¡°ì¸ ê²½ìš° ê·¸ëŒ€ë¡œ ì‚¬ìš©
                        variables = params
                    else:
                        # íŒŒë¼ë¯¸í„° ë”•ì…”ë„ˆë¦¬ë§Œ ìˆëŠ” ê²½ìš° {"params": params}ë¡œ ë˜í•‘
                        variables = {'params': params}
                    
                    # ğŸ”¥ ëª¨ë¸ íŒŒë¼ë¯¸í„° ê²€ì¦ (NaN/Inf ì²´í¬)
                    try:
                        # Flax FrozenDictë¥¼ í™•ì¸í•˜ê³  ê²€ì¦
                        # ëª¨ë“ˆ ë ˆë²¨ì˜ jax, jnp ì‚¬ìš© (í•¨ìˆ˜ ë‚´ë¶€ì—ì„œ ë‹¤ì‹œ importí•˜ì§€ ì•ŠìŒ)
                        def check_params_finite(p):
                            """ì¬ê·€ì ìœ¼ë¡œ íŒŒë¼ë¯¸í„° ê²€ì¦"""
                            if isinstance(p, (dict, type(variables))):
                                return all(check_params_finite(v) for v in p.values())
                            elif hasattr(p, 'shape'):
                                # JAX ë°°ì—´ì¸ ê²½ìš°
                                if p.size > 0:
                                    is_finite = jnp.all(jnp.isfinite(p))
                                    if not is_finite:
                                        logger.warning(f"âš ï¸ íŒŒë¼ë¯¸í„°ì— NaN/Inf ë°œê²¬: shape={p.shape}")
                                        return False
                                return True
                            return True
                        
                        if not check_params_finite(variables):
                            logger.warning("âš ï¸ ëª¨ë¸ íŒŒë¼ë¯¸í„°ì— NaN/Inf ë°œê²¬, ì•ˆì „í•œ ê°’ìœ¼ë¡œ ëŒ€ì²´")
                            # íŒŒë¼ë¯¸í„° ì¬ì´ˆê¸°í™” ëŒ€ì‹  ì•ˆì „í•œ ê°’ ì‚¬ìš©
                            safe_loss = jnp.array(0.0)
                            return safe_loss, (safe_loss, safe_loss, safe_loss)
                    except Exception as param_check_err:
                        logger.debug(f"âš ï¸ íŒŒë¼ë¯¸í„° ê²€ì¦ ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œí•˜ê³  ê³„ì†): {param_check_err}")
                    
                    # ğŸ”§ ì•ˆì „í•œ forward pass (Flax ëª¨ë¸ apply ë°©ì‹)
                    try:
                        # variablesëŠ” {"params": {...}} êµ¬ì¡°
                        # ğŸ”¥ Flax ëª¨ë¸ apply í˜¸ì¶œ (mutable íŒŒë¼ë¯¸í„° ì—†ì´, ê¸°ë³¸ê°’ ì‚¬ìš©)
                        # JAX ì»´íŒŒì¼ ì—ëŸ¬ ë°©ì§€ë¥¼ ìœ„í•´ ëª…ì‹œì ìœ¼ë¡œ ë³€ìˆ˜ ê²€ì¦
                        outputs = model.apply(variables, states_safe)
                        
                        # ğŸ†• 5ê°œ ì¶œë ¥ ì²˜ë¦¬ (Multitask Learning)
                        if isinstance(outputs, tuple):
                            if len(outputs) == 5:
                                action_logits, values, price_change_pred, horizon_pred, analysis_score_pred = outputs
                            elif len(outputs) == 4:
                                action_logits, values, price_change_pred, horizon_pred = outputs
                                analysis_score_pred = jnp.full((batch_size, 1), 50.0)
                            elif len(outputs) == 2:
                                # ì´ì „ ëª¨ë¸ í˜¸í™˜ì„± (2ê°œ ì¶œë ¥)
                                action_logits, values = outputs
                                price_change_pred = jnp.zeros((states_safe.shape[0], 1))
                                horizon_pred = jnp.ones((states_safe.shape[0], 1)) * 10
                                analysis_score_pred = jnp.full((states_safe.shape[0], 1), 50.0)
                            else:
                                logger.warning(f"âš ï¸ Model ì¶œë ¥ ê°œìˆ˜ ì˜ˆìƒê³¼ ë‹¤ë¦„: {len(outputs)}")
                                safe_loss = jnp.array(0.0)
                                return safe_loss, (safe_loss, safe_loss, safe_loss)
                        else:
                            logger.warning(f"âš ï¸ Model ì¶œë ¥ í˜•íƒœ ì˜ˆìƒê³¼ ë‹¤ë¦„: {type(outputs)}")
                            safe_loss = jnp.array(0.0)
                            return safe_loss, (safe_loss, safe_loss, safe_loss)
                    except Exception as apply_err:
                        logger.warning(f"âš ï¸ Model.apply ì‹¤íŒ¨: {apply_err}")
                        import traceback
                        logger.error(f"Model.apply ìƒì„¸ ì—ëŸ¬:\n{traceback.format_exc()}")
                        # paramsì™€ states_safe í˜•ì‹ ë¡œê¹…
                        logger.error(f"Params type: {type(params)}, States shape: {states_safe.shape}, dtype: {states_safe.dtype}")
                        
                        # ğŸ”¥ ë°°ì¹˜ê°€ ë„ˆë¬´ í¬ë©´ ìë™ìœ¼ë¡œ ë” ì‘ì€ ë°°ì¹˜ë¡œ ë¶„í• í•˜ì—¬ ì¬ì‹œë„
                        # JAX ì»´íŒŒì¼ ì—ëŸ¬ëŠ” ì¢…ì¢… í° ë°°ì¹˜ì—ì„œ ë°œìƒ
                        # XLA ì»´íŒŒì¼ ë¬¸ì œë¡œ ë” ì‘ì€ ì²­í¬ í¬ê¸° ì‚¬ìš©
                        if states_safe.shape[0] > 128:
                            logger.info(f"ğŸ”„ Model.apply ì‹¤íŒ¨, ë°°ì¹˜ ë¶„í•  ì‹œë„: {states_safe.shape[0]} â†’ 128ì”©")
                            # ì‘ì€ ë°°ì¹˜ë¡œ ë‚˜ëˆ„ì–´ ì²˜ë¦¬ (512 â†’ 128ë¡œ ì¶•ì†Œ)
                            chunk_size = 128
                            
                            # ğŸ”¥ ë” ì‘ì€ ì²­í¬ë„ ì‹œë„
                            # 128ë„ ì‹¤íŒ¨í•˜ë©´ 64ë¡œ, 64ë„ ì‹¤íŒ¨í•˜ë©´ 32ë¡œ ì¬ì‹œë„
                            chunk_sizes = [128, 64, 32]
                            success = False
                            
                            for try_chunk_size in chunk_sizes:
                                if states_safe.shape[0] <= try_chunk_size:
                                    continue  # ë°°ì¹˜ê°€ ì´ë¯¸ ì²­í¬ í¬ê¸°ë³´ë‹¤ ì‘ìœ¼ë©´ ìŠ¤í‚µ
                                
                                try:
                                    logger.info(f"  ğŸ”„ {try_chunk_size} í¬ê¸°ë¡œ ì¬ì‹œë„...")
                                    all_action_logits = []
                                    all_values = []
                                    all_pc = []
                                    all_h = []
                                    all_as = []
                                    
                                    for chunk_start in range(0, states_safe.shape[0], try_chunk_size):
                                        chunk_end = min(chunk_start + try_chunk_size, states_safe.shape[0])
                                        states_chunk = states_safe[chunk_start:chunk_end]
                                        
                                        # ê° ì²­í¬ì— ëŒ€í•´ forward pass
                                        outputs_chunk = model.apply(variables, states_chunk)

                                        # ğŸ†• 5ê°œ ì¶œë ¥ ì²˜ë¦¬
                                        if isinstance(outputs_chunk, tuple):
                                            if len(outputs_chunk) == 5:
                                                a, v, p, h, s = outputs_chunk
                                                all_as.append(s)
                                                all_pc.append(p)
                                                all_h.append(h)
                                            elif len(outputs_chunk) == 4:
                                                a, v, p, h = outputs_chunk
                                                all_as.append(jnp.full((a.shape[0], 1), 50.0))
                                                all_pc.append(p)
                                                all_h.append(h)
                                            elif len(outputs_chunk) == 2:
                                                a, v = outputs_chunk
                                                all_as.append(jnp.full((a.shape[0], 1), 50.0))
                                                all_pc.append(jnp.zeros((a.shape[0], 1)))
                                                all_h.append(jnp.ones((a.shape[0], 1)) * 10)
                                            else:
                                                raise ValueError(f"ì²­í¬ ì¶œë ¥ ê°œìˆ˜ ì˜¤ë¥˜: {len(outputs_chunk)}")
                                            all_action_logits.append(a)
                                            all_values.append(v)
                                        else:
                                            raise ValueError(f"ì²­í¬ ì¶œë ¥ í˜•ì‹ ì˜¤ë¥˜: {type(outputs_chunk)}")
                                    
                                    # ëª¨ë“  ì²­í¬ ê²°ê³¼ í•©ì¹˜ê¸°
                                    if all_action_logits and all_values:
                                        action_logits = jnp.concatenate(all_action_logits, axis=0)
                                        values = jnp.concatenate(all_values, axis=0)
                                        price_change_pred = jnp.concatenate(all_pc, axis=0)
                                        horizon_pred = jnp.concatenate(all_h, axis=0)
                                        analysis_score_pred = jnp.concatenate(all_as, axis=0)
                                        logger.info(f"âœ… ë¶„í•  ë°°ì¹˜ ì²˜ë¦¬ ì„±ê³µ: {states_safe.shape[0]} â†’ {len(all_action_logits)}ê°œ ì²­í¬ (ê° {try_chunk_size} í¬ê¸°)")
                                        success = True
                                        break  # ì„±ê³µí•˜ë©´ ë£¨í”„ ì¢…ë£Œ
                                except Exception as chunk_try_err:
                                    logger.debug(f"  âš ï¸ {try_chunk_size} í¬ê¸°ë¡œë„ ì‹¤íŒ¨, ë‹¤ìŒ í¬ê¸° ì‹œë„: {chunk_try_err}")
                                    continue
                            
                            if not success:
                                # ëª¨ë“  ì²­í¬ í¬ê¸° ì‹¤íŒ¨
                                logger.warning(f"âš ï¸ ëª¨ë“  ë°°ì¹˜ ë¶„í•  ì‹œë„ ì‹¤íŒ¨")
                                dummy_loss = jnp.array(0.0)
                                return dummy_loss, (dummy_loss, dummy_loss, dummy_loss)
                            else:
                                # ì„±ê³µí–ˆìœ¼ë¯€ë¡œ ê³„ì† ì§„í–‰
                                pass
                        else:
                            # ì‘ì€ ë°°ì¹˜ë„ ì‹¤íŒ¨í•œ ê²½ìš°, ë” ì‘ì€ ë‹¨ìœ„ë¡œ ì¬ì‹œë„
                            if states_safe.shape[0] > 32:
                                logger.info(f"ğŸ”„ ì‘ì€ ë°°ì¹˜ë„ ì‹¤íŒ¨, ë” ì‘ê²Œ ë¶„í•  ì‹œë„: {states_safe.shape[0]} â†’ 32ì”©")
                                chunk_sizes = [32, 16, 8]
                                success = False
                                
                                for try_chunk_size in chunk_sizes:
                                    if states_safe.shape[0] <= try_chunk_size:
                                        # ë°°ì¹˜ê°€ ì²­í¬ í¬ê¸° ì´í•˜ë©´ ì§ì ‘ ì‹œë„
                                        try:
                                            logger.info(f"  ğŸ”„ {states_safe.shape[0]} í¬ê¸°ë¡œ ì§ì ‘ ì¬ì‹œë„...")
                                            outputs = model.apply(variables, states_safe)
                                            # ğŸ†• 5ê°œ ì¶œë ¥ ì²˜ë¦¬
                                            if isinstance(outputs, tuple):
                                                if len(outputs) == 5:
                                                    action_logits, values, price_change_pred, horizon_pred, analysis_score_pred = outputs
                                                elif len(outputs) == 4:
                                                    action_logits, values, price_change_pred, horizon_pred = outputs
                                                    analysis_score_pred = jnp.full((states_safe.shape[0], 1), 50.0)
                                                elif len(outputs) == 2:
                                                    action_logits, values = outputs
                                                    price_change_pred = jnp.zeros((states_safe.shape[0], 1))
                                                    horizon_pred = jnp.ones((states_safe.shape[0], 1)) * 10
                                                    analysis_score_pred = jnp.full((states_safe.shape[0], 1), 50.0)
                                                logger.info(f"âœ… ì§ì ‘ ì¬ì‹œë„ ì„±ê³µ: {states_safe.shape[0]}")
                                                success = True
                                                break
                                        except:
                                            continue
                                    
                                    try:
                                        logger.info(f"  ğŸ”„ {try_chunk_size} í¬ê¸°ë¡œ ë¶„í•  ì¬ì‹œë„...")
                                        all_action_logits = []
                                        all_values = []
                                        all_pc = []
                                        all_h = []
                                        all_as = []
                                        
                                        for chunk_start in range(0, states_safe.shape[0], try_chunk_size):
                                            chunk_end = min(chunk_start + try_chunk_size, states_safe.shape[0])
                                            states_chunk = states_safe[chunk_start:chunk_end]
                                            
                                            outputs_chunk = model.apply(variables, states_chunk)

                                            # ğŸ†• 5ê°œ ì¶œë ¥ ì²˜ë¦¬
                                            if isinstance(outputs_chunk, tuple):
                                                if len(outputs_chunk) == 5:
                                                    a, v, p, h, s = outputs_chunk
                                                    all_as.append(s)
                                                    all_pc.append(p)
                                                    all_h.append(h)
                                                elif len(outputs_chunk) == 4:
                                                    a, v, p, h = outputs_chunk
                                                    all_as.append(jnp.full((a.shape[0], 1), 50.0))
                                                    all_pc.append(p)
                                                    all_h.append(h)
                                                elif len(outputs_chunk) == 2:
                                                    a, v = outputs_chunk
                                                    all_as.append(jnp.full((a.shape[0], 1), 50.0))
                                                    all_pc.append(jnp.zeros((a.shape[0], 1)))
                                                    all_h.append(jnp.ones((a.shape[0], 1)) * 10)
                                                else:
                                                    raise ValueError(f"ì²­í¬ ì¶œë ¥ ê°œìˆ˜ ì˜¤ë¥˜: {len(outputs_chunk)}")
                                                all_action_logits.append(a)
                                                all_values.append(v)
                                            else:
                                                raise ValueError(f"ì²­í¬ ì¶œë ¥ í˜•ì‹ ì˜¤ë¥˜: {type(outputs_chunk)}")
                                        
                                        if all_action_logits and all_values:
                                            action_logits = jnp.concatenate(all_action_logits, axis=0)
                                            values = jnp.concatenate(all_values, axis=0)
                                            price_change_pred = jnp.concatenate(all_pc, axis=0)
                                            horizon_pred = jnp.concatenate(all_h, axis=0)
                                            analysis_score_pred = jnp.concatenate(all_as, axis=0)
                                            logger.info(f"âœ… ë¶„í•  ì¬ì‹œë„ ì„±ê³µ: {states_safe.shape[0]} â†’ {len(all_action_logits)}ê°œ ì²­í¬ (ê° {try_chunk_size})")
                                            success = True
                                            break
                                    except Exception as small_chunk_err:
                                        logger.debug(f"  âš ï¸ {try_chunk_size} í¬ê¸°ë¡œë„ ì‹¤íŒ¨: {small_chunk_err}")
                                        continue
                                
                                if not success:
                                    logger.error(f"âŒ ëª¨ë“  ë°°ì¹˜ í¬ê¸° ì‹œë„ ì‹¤íŒ¨ (ìµœì†Œ í¬ê¸°: {states_safe.shape[0]})")
                                    dummy_loss = jnp.array(0.0)
                                    return dummy_loss, (dummy_loss, dummy_loss, dummy_loss)
                            else:
                                # ì´ë¯¸ ë§¤ìš° ì‘ì€ ë°°ì¹˜ (<=32)ë„ ì‹¤íŒ¨
                                logger.error(f"âŒ ë§¤ìš° ì‘ì€ ë°°ì¹˜ ({states_safe.shape[0]})ë„ ì‹¤íŒ¨ - ëª¨ë¸ ë˜ëŠ” íŒŒë¼ë¯¸í„° ë¬¸ì œ ê°€ëŠ¥")
                                dummy_loss = jnp.array(0.0)
                                return dummy_loss, (dummy_loss, dummy_loss, dummy_loss)
                    
                    # ì¶œë ¥ shape í™•ì¸ ë° ê²€ì¦
                    # valuesëŠ” (batch_size, 1) í˜•íƒœì¼ ìˆ˜ ìˆìŒ
                    values_batch_dim = values.shape[0] if values.ndim > 0 else 0
                    if action_logits.shape[0] != batch_size:
                        logger.warning(f"âš ï¸ Action logits shape ë¶ˆì¼ì¹˜: {action_logits.shape}, batch_size={batch_size}")
                        dummy_loss = jnp.array(0.0)
                        return dummy_loss, (dummy_loss, dummy_loss, dummy_loss)
                    
                    # values shape ì •ê·œí™” (2D -> 1D)
                    if values.ndim == 2 and values.shape[1] == 1:
                        # (batch_size, 1) -> (batch_size,)
                        values = values.squeeze(axis=1)
                    elif values.ndim == 2 and values.shape[1] > 1:
                        # (batch_size, value_dim) -> ì²« ë²ˆì§¸ ì°¨ì›ë§Œ ì‚¬ìš©
                        values = values[:, 0]
                    elif values.ndim == 0:
                        # ìŠ¤ì¹¼ë¼ -> ë°°ì¹˜ë¡œ í™•ì¥
                        values = jnp.broadcast_to(values, (batch_size,))
                    
                    if values.shape[0] != batch_size:
                        logger.warning(f"âš ï¸ Values shape ë¶ˆì¼ì¹˜: {values.shape}, batch_size={batch_size}")
                        dummy_loss = jnp.array(0.0)
                        return dummy_loss, (dummy_loss, dummy_loss, dummy_loss)
                    
                    # í˜„ì¬ ì •ì±…ì˜ log probability ê³„ì‚°
                    action_probs = jax.nn.softmax(action_logits)
                    log_probs = jax.nn.log_softmax(action_logits)
                    
                    # ì„ íƒëœ actionì˜ log_prob
                    action_one_hot = jax.nn.one_hot(actions_jax, num_classes=3)
                    new_log_probs = jnp.sum(log_probs * action_one_hot, axis=1)
                    
                    # Ratio ê³„ì‚° (í˜„ì¬ ì •ì±… / ì´ì „ ì •ì±…)
                    ratio = jnp.exp(new_log_probs - old_log_probs_jax)
                    
                    # ğŸ”¥ í•™ìŠµ ì„±ëŠ¥ ê°œì„ : Loss ê³„ì‚° ê°œì„ 
                    # Clipped surrogate objective (ë” ì•ˆì „í•œ í´ë¦¬í•‘)
                    ratio_clipped = jnp.clip(ratio, 1.0 - clip_epsilon, 1.0 + clip_epsilon)
                    surr1 = ratio * advantages_normalized
                    surr2 = ratio_clipped * advantages_normalized
                    policy_loss = -jnp.mean(jnp.minimum(surr1, surr2))
                    
                    # ğŸ”¥ Policy loss ì •ê·œí™” (ë„ˆë¬´ í° loss ë°©ì§€)
                    policy_loss = jnp.clip(policy_loss, -10.0, 10.0)
                    
                    # Value loss (MSE) - valuesëŠ” ì´ë¯¸ 1Dë¡œ ì •ê·œí™”ë¨
                    # returnsì™€ valuesì˜ shape ì¼ì¹˜ í™•ì¸
                    if values.shape != returns.shape:
                        min_val_len = min(len(values), len(returns))
                        values_aligned = values[:min_val_len]
                        returns_aligned = returns[:min_val_len]
                    else:
                        values_aligned = values
                        returns_aligned = returns
                    
                    # ğŸ”¥ Value lossë„ ì •ê·œí™” (Huber loss ê³ ë ¤)
                    value_error = values_aligned - returns_aligned
                    # MSE ëŒ€ì‹  Huber loss ì‚¬ìš© (ì´ìƒì¹˜ì— ëœ ë¯¼ê°)
                    delta = 1.0
                    huber_loss = jnp.where(
                        jnp.abs(value_error) < delta,
                        0.5 * value_error ** 2,
                        delta * (jnp.abs(value_error) - 0.5 * delta)
                    )
                    value_loss = jnp.mean(huber_loss)
                    
                    # ğŸ”¥ Value loss ì •ê·œí™”
                    value_loss = jnp.clip(value_loss, 0.0, 10.0)
                    
                    # Entropy bonus (íƒí—˜ ì¥ë ¤)
                    entropy = -jnp.mean(jnp.sum(action_probs * log_probs, axis=1))

                    # ğŸ”¥ Entropy ì •ê·œí™”
                    entropy = jnp.clip(entropy, 0.0, 10.0)

                    # ğŸ†• Price change loss (MSE) - íšŒê·€ ì˜ˆì¸¡
                    # price_change_pred shape: (batch_size, 1) or (batch_size,)
                    # ì‹¤ì œ ë ˆì´ë¸”ì€ í–¥í›„ orchestratorì—ì„œ ì¶”ê°€ë¨ (í˜„ì¬ëŠ” 0ìœ¼ë¡œ ì´ˆê¸°í™”)
                    # TODO: orchestratorì—ì„œ ì‹¤ì œ price_change ë ˆì´ë¸” ì œê³µ ì‹œ ì‚¬ìš©
                    price_change_target = jnp.zeros_like(price_change_pred)  # ì„ì‹œ íƒ€ê²Ÿ (0%)
                    if price_change_pred.ndim == 2 and price_change_pred.shape[1] == 1:
                        price_change_pred_flat = price_change_pred.squeeze(axis=1)
                    else:
                        price_change_pred_flat = price_change_pred
                    price_change_loss = jnp.mean((price_change_pred_flat - price_change_target.squeeze()) ** 2)
                    price_change_loss = jnp.clip(price_change_loss, 0.0, 1.0)  # í´ë¦¬í•‘ (0~1 ë²”ìœ„)

                    # ğŸ†• Horizon loss (MSE) - íšŒê·€ ì˜ˆì¸¡
                    # horizon_pred shape: (batch_size, 1) or (batch_size,)
                    # ì‹¤ì œ ë ˆì´ë¸”ì€ í–¥í›„ orchestratorì—ì„œ ì¶”ê°€ë¨ (í˜„ì¬ëŠ” 10ìœ¼ë¡œ ì´ˆê¸°í™”)
                    # TODO: orchestratorì—ì„œ ì‹¤ì œ horizon ë ˆì´ë¸” ì œê³µ ì‹œ ì‚¬ìš©
                    horizon_target = jnp.ones_like(horizon_pred) * 10.0  # ì„ì‹œ íƒ€ê²Ÿ (10 ìº”ë“¤)
                    if horizon_pred.ndim == 2 and horizon_pred.shape[1] == 1:
                        horizon_pred_flat = horizon_pred.squeeze(axis=1)
                    else:
                        horizon_pred_flat = horizon_pred
                    horizon_loss = jnp.mean((horizon_pred_flat - horizon_target.squeeze()) ** 2)
                    horizon_loss = jnp.clip(horizon_loss, 0.0, 100.0)  # í´ë¦¬í•‘ (0~100 ë²”ìœ„)

                    # ğŸ†• Analysis Score Loss (MSE)
                    if analysis_score_pred.ndim == 2 and analysis_score_pred.shape[1] == 1:
                        analysis_score_pred_flat = analysis_score_pred.squeeze(axis=1)
                    else:
                        analysis_score_pred_flat = analysis_score_pred
                    
                    # shape ë§ì¶”ê¸° (ì•ˆì „ì¥ì¹˜)
                    if analysis_score_pred_flat.shape[0] != analysis_scores_jax.shape[0]:
                        min_len = min(analysis_score_pred_flat.shape[0], analysis_scores_jax.shape[0])
                        analysis_score_pred_flat = analysis_score_pred_flat[:min_len]
                        analysis_targets = analysis_scores_jax[:min_len]
                    else:
                        analysis_targets = analysis_scores_jax

                    analysis_loss = jnp.mean((analysis_score_pred_flat - analysis_targets) ** 2)
                    analysis_loss = jnp.clip(analysis_loss, 0.0, 10000.0)

                    # ì´ ì†ì‹¤ (Loss êµ¬ì„± ìš”ì†Œë³„ ê°€ì¤‘ì¹˜ ì¡°ì •)
                    # ğŸ†• íšŒê·€ ì†ì‹¤ ì¶”ê°€ (ì‘ì€ ê°€ì¤‘ì¹˜ë¡œ ì‹œì‘)
                    regression_loss_coef = 0.1  # íšŒê·€ ì†ì‹¤ ê°€ì¤‘ì¹˜ (í–¥í›„ ì¡°ì • ê°€ëŠ¥)
                    total_loss = (
                        policy_loss +
                        value_loss_coef * value_loss -
                        entropy_coef * entropy +
                        regression_loss_coef * price_change_loss +
                        regression_loss_coef * horizon_loss +
                        regression_loss_coef * (analysis_loss / 100.0)  # ìŠ¤ì¼€ì¼ ì¡°ì •
                    )
                    
                    # ğŸ”¥ Loss ì •ê·œí™” (ê³¼ë„í•œ loss ë°©ì§€)
                    total_loss = jnp.clip(total_loss, -20.0, 20.0)
                    
                    # NaN/Inf ì²´í¬
                    total_loss = jnp.nan_to_num(total_loss, nan=0.0, posinf=1e6, neginf=-1e6)
                    policy_loss = jnp.nan_to_num(policy_loss, nan=0.0, posinf=1e6, neginf=-1e6)
                    value_loss = jnp.nan_to_num(value_loss, nan=0.0, posinf=1e6, neginf=-1e6)
                    entropy = jnp.nan_to_num(entropy, nan=0.0, posinf=1e6, neginf=-1e6)
                    
                    return total_loss, (policy_loss, value_loss, entropy)
                except Exception as loss_err:
                    logger.warning(f"âš ï¸ Loss ê³„ì‚° ì‹¤íŒ¨, 0 ë°˜í™˜: {loss_err}")
                    dummy_loss = jnp.array(0.0)
                    return dummy_loss, (dummy_loss, dummy_loss, dummy_loss)
            
            # Gradient ê³„ì‚° (ì•ˆì „í•œ ë°©ì‹)
            try:
                # ğŸ”§ self.model['params']ê°€ ì´ë¯¸ ì˜¬ë°”ë¥¸ í˜•ì‹ì¸ì§€ í™•ì¸
                model_params = self.model['params']
                # Flax paramsëŠ” FrozenDict ë˜ëŠ” dict í˜•íƒœ
                (loss_value, (policy_loss_val, value_loss_val, entropy_val)), grads = jax.value_and_grad(loss_fn, has_aux=True)(model_params)
                
                # Gradient í´ë¦¬í•‘ (ê³¼ë„í•œ gradient ë°©ì§€)
                # ğŸ”§ JAX ë²„ì „ í˜¸í™˜ì„±: jax.tree_map â†’ jax.tree.map ë˜ëŠ” jax.tree_util.tree_map
                if USE_JAX_TREE and hasattr(jax, 'tree'):
                    # JAX v0.4.25+ ë˜ëŠ” ìµœì‹  ë²„ì „
                    grads = jax.tree.map(lambda g: jnp.clip(g, -1.0, 1.0), grads)
                elif JAX_TREE_UTIL is not None:
                    # êµ¬ë²„ì „ í˜¸í™˜ì„±
                    grads = JAX_TREE_UTIL.tree_map(lambda g: jnp.clip(g, -1.0, 1.0), grads)
                else:
                    # ìµœí›„ì˜ ìˆ˜ë‹¨: ìˆ˜ë™ìœ¼ë¡œ ì²˜ë¦¬
                    grads_clipped = {}
                    for k, v in grads.items():
                        if hasattr(v, '__iter__') and not isinstance(v, (str, bytes)):
                            grads_clipped[k] = jnp.clip(v, -1.0, 1.0)
                        else:
                            grads_clipped[k] = v
                    grads = grads_clipped
                
                # NaN/Inf ì²´í¬
                loss_value = jnp.nan_to_num(loss_value, nan=0.0, posinf=1e6, neginf=-1e6)
                
                # ì˜µí‹°ë§ˆì´ì € ì—…ë°ì´íŠ¸
                updates, self.opt_state = self.optimizer.update(grads, self.opt_state, self.model['params'])
                
                # Updates í´ë¦¬í•‘
                if USE_JAX_TREE and hasattr(jax, 'tree'):
                    # JAX v0.4.25+ ë˜ëŠ” ìµœì‹  ë²„ì „
                    updates = jax.tree.map(lambda u: jnp.clip(u, -0.1, 0.1), updates)
                elif JAX_TREE_UTIL is not None:
                    # êµ¬ë²„ì „ í˜¸í™˜ì„±
                    updates = JAX_TREE_UTIL.tree_map(lambda u: jnp.clip(u, -0.1, 0.1), updates)
                else:
                    # ìˆ˜ë™ ì²˜ë¦¬
                    updates_clipped = {}
                    for k, v in updates.items():
                        if hasattr(v, '__iter__') and not isinstance(v, (str, bytes)):
                            updates_clipped[k] = jnp.clip(v, -0.1, 0.1)
                        else:
                            updates_clipped[k] = v
                    updates = updates_clipped
                
                self.model['params'] = optax.apply_updates(self.model['params'], updates)
                
            except Exception as grad_err:
                logger.error(f"âŒ Gradient ê³„ì‚° ì‹¤íŒ¨: {grad_err}")
                import traceback
                logger.debug(f"Gradient ê³„ì‚° ìƒì„¸ ì—ëŸ¬:\n{traceback.format_exc()}")
                return 0.0
            
            # ì†ì‹¤ê°’ ë°˜í™˜
            loss_float = float(loss_value)
            policy_loss_float = float(policy_loss_val)
            value_loss_float = float(value_loss_val)
            entropy_float = float(entropy_val)

            # í•™ìŠµ ì§„í–‰ ìƒí™© ë¡œê¹… (ë” ìì£¼)
            if np.random.random() < 0.2:  # 20% í™•ë¥ ë¡œ ë¡œê·¸
                logger.debug(f"ğŸ“Š Loss: total={loss_float:.4f}, policy={policy_loss_float:.4f}, "
                          f"value={value_loss_float:.4f}, entropy={entropy_float:.4f}")

            # ğŸ”¥ ë””ë²„ê±° ë¡œê¹…: ë°°ì¹˜ í•™ìŠµ ìƒì„¸ ì •ë³´
            if self.debug:
                try:
                    # action_probs ê³„ì‚° (forward passì—ì„œ ì´ë¯¸ ê³„ì‚°ë¨)
                    # loss_fn ë‚´ì—ì„œ ê³„ì‚°í–ˆìœ¼ë¯€ë¡œ ë‹¤ì‹œ ê³„ì‚°í•´ì•¼ í•¨
                    if JAX_AVAILABLE:
                        model = self.model['model_def']
                        variables = self.model['params']
                        outputs = model.apply(variables, states_jax)
                        if isinstance(outputs, tuple):
                            action_logits, _ = outputs
                            action_probs = jax.nn.softmax(action_logits)
                        else:
                            action_probs = None
                    else:
                        action_probs = None

                    # KL divergence ê³„ì‚° (old vs new policy)
                    if action_probs is not None and JAX_AVAILABLE:
                        log_probs_new = jax.nn.log_softmax(action_logits)
                        # one-hot ì¸ì½”ë”©
                        action_one_hot = jax.nn.one_hot(actions_jax, num_classes=3)
                        new_log_probs = jnp.sum(log_probs_new * action_one_hot, axis=1)
                        # KL divergence: E[log(new) - log(old)]
                        kl_div = float(jnp.mean(new_log_probs - old_log_probs_jax))
                    else:
                        kl_div = 0.0

                    # í˜„ì¬ ë°°ì¹˜ ì¸ë±ìŠ¤ ì¶”ì  (ì—†ìœ¼ë©´ 0)
                    if not hasattr(self, '_debug_batch_idx'):
                        self._debug_batch_idx = 0
                    self._debug_batch_idx += 1

                    # í˜„ì¬ epoch ì¶”ì  (ì—†ìœ¼ë©´ 1)
                    current_epoch = getattr(self, '_debug_current_epoch', 1)

                    self.debug.log_batch_training(
                        epoch=current_epoch,
                        batch_idx=self._debug_batch_idx,
                        total_batches=getattr(self, '_debug_total_batches', 1),
                        loss=loss_float,
                        policy_loss=policy_loss_float,
                        value_loss=value_loss_float,
                        entropy_loss=entropy_float,
                        actions=actions,  # ì›ë³¸ actions ë¦¬ìŠ¤íŠ¸
                        action_probs=np.array(action_probs) if action_probs is not None else None,
                        entropy_coef=entropy_coef,
                        clip_ratio=clip_epsilon,
                        kl_divergence=kl_div
                    )

                    # ê·¸ë˜ë””ì–¸íŠ¸ í†µê³„ ë¡œê¹… (ì²« ë°°ì¹˜ë§Œ)
                    if self._debug_batch_idx == 1:
                        # gradsëŠ” FrozenDict í˜•íƒœ
                        grad_dict = {}
                        if 'params' in grads:
                            for layer_name, layer_params in grads['params'].items():
                                if hasattr(layer_params, 'items'):
                                    for param_name, param_grad in layer_params.items():
                                        full_name = f"{layer_name}_{param_name}"
                                        grad_dict[full_name] = np.array(param_grad) if hasattr(param_grad, 'shape') else param_grad

                        # Gradient norm ê³„ì‚°
                        grad_norm = 0.0
                        for grad_arr in grad_dict.values():
                            if hasattr(grad_arr, 'flatten'):
                                grad_norm += float(np.sum(grad_arr.flatten() ** 2))
                        grad_norm = np.sqrt(grad_norm)

                        self.debug.log_gradient_update(
                            epoch=current_epoch,
                            batch_idx=self._debug_batch_idx,
                            gradients=grad_dict,
                            learning_rate=self.current_learning_rate,
                            grad_norm=grad_norm,
                            clipped=grad_norm > 0.5  # clip_by_global_norm(0.5) ì‚¬ìš©
                        )
                except Exception as debug_err:
                    logger.debug(f"âš ï¸ ë°°ì¹˜ ë””ë²„ê·¸ ë¡œê¹… ì‹¤íŒ¨ (ë¬´ì‹œ): {debug_err}")

            # í•™ìŠµ ë°ì´í„° ê²€ì¦ ë¡œê¹… (ì²« ë°°ì¹˜ë§Œ) - ë””ë²„ê±°ë¡œ ëŒ€ì²´
            if not hasattr(self, '_first_batch_logged'):
                actual_batch_size = states_jax.shape[0] if len(states_jax.shape) > 0 else 0

                # ğŸ”¥ ë””ë²„ê±°ë¡œ í•™ìŠµ ë°ì´í„° í†µê³„ ë¡œê¹…
                if self.debug:
                    try:
                        self.debug.log_training_data_stats(
                            states=np.array(states_jax),
                            actions=actions,
                            rewards=rewards,
                            advantages=np.array(advantages)
                        )
                    except Exception as stats_err:
                        logger.debug(f"âš ï¸ í•™ìŠµ ë°ì´í„° í†µê³„ ë¡œê¹… ì‹¤íŒ¨ (ë¬´ì‹œ): {stats_err}")

                # ê¸°ì¡´ ë¡œê¹… ìœ ì§€
                logger.info(f"ğŸ” í•™ìŠµ ë°ì´í„° ê²€ì¦ (ë°°ì¹˜ í¬ê¸°: {actual_batch_size}):")
                logger.info(f"   - States shape: {states_jax.shape}, dtype: {states_jax.dtype}")
                logger.info(f"   - Actions: {dict(zip(*np.unique(actions, return_counts=True)))}")
                logger.info(f"   - Rewards ë²”ìœ„: [{np.min(rewards):.4f}, {np.max(rewards):.4f}], í‰ê· : {np.mean(rewards):.4f}")
                logger.info(f"   - Returns ë²”ìœ„: [{jnp.min(returns):.4f}, {jnp.max(returns):.4f}], í‰ê· : {jnp.mean(returns):.4f}")
                logger.info(f"   - Advantages ë²”ìœ„: [{jnp.min(advantages):.4f}, {jnp.max(advantages):.4f}], í‰ê· : {jnp.mean(advantages):.4f}")
                self._first_batch_logged = True

            return loss_float
        
        except Exception as e:
            logger.error(f"âŒ ì •ì±… ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
            import traceback
            logger.debug(f"ìƒì„¸ ì—ëŸ¬:\n{traceback.format_exc()}")
            return 0.0
    
    def _evaluate_model(self, epoch: int, sample_experiences: List[Dict]) -> Dict[str, Any]:
        """
        ëª¨ë¸ í‰ê°€
        
        Args:
            epoch: í˜„ì¬ ì—í­
            sample_experiences: ìƒ˜í”Œ ê²½í—˜ ë°ì´í„°
        
        Returns:
            í‰ê°€ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        try:
            rewards = [exp.get('reward', 0.0) for exp in sample_experiences]
            
            return {
                'epoch': epoch,
                'avg_reward': np.mean(rewards) if rewards else 0.0,
                'std_reward': np.std(rewards) if rewards else 0.0,
                'sample_size': len(sample_experiences)
            }
        
        except Exception as e:
            logger.warning(f"âš ï¸ ëª¨ë¸ í‰ê°€ ì‹¤íŒ¨: {e}")
            return {'epoch': epoch, 'avg_reward': 0.0}
    
    def _save_model(self, db_path: Optional[str] = None) -> str:
        """
        ëª¨ë¸ ì €ì¥
        
        Args:
            db_path: DB ê²½ë¡œ (Noneì´ë©´ ì„¤ì • íŒŒì¼ì—ì„œ ê°€ì ¸ì˜´)
        
        Returns:
            model_id: ì €ì¥ëœ ëª¨ë¸ ID
        """
        try:
            import uuid
            from datetime import datetime
            
            # ëª¨ë¸ ID ìƒì„±
            model_id = f"ppo_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{uuid.uuid4().hex[:8]}"
            
            # ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ
            checkpoint_dir = self.config.get('paths', {}).get('checkpoints', '/workspace/rl_pipeline/artifacts/checkpoints')
            ckpt_path = os.path.join(checkpoint_dir, f"{model_id}.ckpt")
            
            # ì²´í¬í¬ì¸íŠ¸ ì €ì¥
            save_ckpt(self.model, ckpt_path)
            
            # DBì— ê¸°ë¡
            if db_path:
                self._save_to_db(model_id, ckpt_path, db_path)
            
            logger.info(f"âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {model_id}")
            return model_id
            
        except Exception as e:
            logger.error(f"âŒ ëª¨ë¸ ì €ì¥ ì‹¤íŒ¨: {e}")
            raise
    
    def _save_to_db(self, model_id: str, ckpt_path: str, db_path: str):
        """
        ëª¨ë¸ ì •ë³´ë¥¼ DBì— ì €ì¥
        
        Args:
            model_id: ëª¨ë¸ ID
            ckpt_path: ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ
            db_path: DB ê²½ë¡œ
        """
        try:
            from rl_pipeline.db.writes import write_batch
            from rl_pipeline.hybrid.features import FEATURES_VERSION
            
            model_record = {
                'model_id': model_id,
                'algo': 'PPO',
                'features_ver': FEATURES_VERSION,
                'created_at': datetime.now().isoformat(),
                'ckpt_path': ckpt_path,
                'notes': json.dumps({
                    'hidden_dim': self.model['hidden_dim'],
                    'obs_dim': self.model['obs_dim'],
                    'action_dim': self.model['action_dim']
                })
            }
            
            write_batch([model_record], 'policy_models', db_path=db_path)
            
        except Exception as e:
            logger.warning(f"âš ï¸ DB ì €ì¥ ì‹¤íŒ¨ (ê³„ì† ì§„í–‰): {e}")


def _compute_returns(rewards: jnp.ndarray, gamma: float) -> jnp.ndarray:
    """
    Discounted returns ê³„ì‚° (ì—­ìˆœìœ¼ë¡œ ëˆ„ì )
    
    Args:
        rewards: ë³´ìƒ ë°°ì—´
        gamma: í• ì¸ìœ¨
    
    Returns:
        Discounted returns ë°°ì—´
    """
    if not JAX_AVAILABLE:
        return np.zeros_like(rewards)
    
    try:
        # ğŸ”§ JAX ì•ˆì „ ë°©ì‹: NumPyë¡œ ë¨¼ì € ê³„ì‚° í›„ ë³€í™˜
        rewards_np = np.array(rewards, dtype=np.float32)
        returns_np = np.zeros_like(rewards_np, dtype=np.float32)
        running_return = 0.0
        
        # ì—­ìˆœìœ¼ë¡œ ê³„ì‚°
        for i in range(len(rewards_np) - 1, -1, -1):
            running_return = float(rewards_np[i]) + gamma * running_return
            returns_np[i] = running_return
        
        # NaN/Inf ì²´í¬ ë° ì²˜ë¦¬
        returns_np = np.nan_to_num(returns_np, nan=0.0, posinf=1e6, neginf=-1e6)
        
        # JAX ë°°ì—´ë¡œ ë³€í™˜
        returns_jax = jnp.array(returns_np, dtype=jnp.float32)
        
        return returns_jax
    except Exception as e:
        logger.warning(f"âš ï¸ Returns ê³„ì‚° ì‹¤íŒ¨, 0 ë°˜í™˜: {e}")
        return jnp.zeros_like(rewards)


def _compute_gae(
    rewards: jnp.ndarray,
    values: jnp.ndarray,
    gamma: float,
    lam: float
) -> jnp.ndarray:
    """
    GAE (Generalized Advantage Estimation) ê³„ì‚°
    
    Args:
        rewards: ë³´ìƒ ë°°ì—´
        values: ê°€ì¹˜ ì¶”ì • ë°°ì—´
        gamma: í• ì¸ìœ¨
        lam: GAE lambda (0~1)
    
    Returns:
        GAE advantages ë°°ì—´
    """
    if not JAX_AVAILABLE:
        return np.zeros_like(rewards)
    
    try:
        # ğŸ”§ JAX ì•ˆì „ ë°©ì‹: NumPyë¡œ ë¨¼ì € ê³„ì‚° í›„ ë³€í™˜
        rewards_np = np.array(rewards, dtype=np.float32)
        values_np = np.array(values, dtype=np.float32)
        batch_size = len(rewards_np)
        
        advantages_np = np.zeros_like(rewards_np, dtype=np.float32)
        
        # ë§ˆì§€ë§‰ valueëŠ” 0ìœ¼ë¡œ ê°€ì • (ì—í”¼ì†Œë“œ ì¢…ë£Œ)
        last_gae = 0.0
        
        # ì—­ìˆœìœ¼ë¡œ GAE ê³„ì‚°
        for i in range(batch_size - 1, -1, -1):
            if i == batch_size - 1:
                next_value = 0.0  # ë§ˆì§€ë§‰ ìŠ¤í…
            else:
                next_value = float(values_np[i + 1])
            
            # TD residual: Î´_t = r_t + Î³ * V(s_{t+1}) - V(s_t)
            reward_val = float(rewards_np[i])
            value_val = float(values_np[i])
            
            delta = reward_val + gamma * next_value - value_val
            
            # GAE: A_t = Î´_t + (Î³Î») * A_{t+1}
            last_gae = delta + gamma * lam * last_gae
            advantages_np[i] = last_gae
        
        # NaN/Inf ì²´í¬ ë° ì²˜ë¦¬
        advantages_np = np.nan_to_num(advantages_np, nan=0.0, posinf=1e6, neginf=-1e6)
        
        # ê°’ ë²”ìœ„ í´ë¦¬í•‘ (ê³¼ë„í•œ ê°’ ë°©ì§€)
        advantages_np = np.clip(advantages_np, -100.0, 100.0)
        
        # JAX ë°°ì—´ë¡œ ë³€í™˜
        advantages_jax = jnp.array(advantages_np, dtype=jnp.float32)
        
        return advantages_jax
    except Exception as e:
        logger.warning(f"âš ï¸ GAE ê³„ì‚° ì‹¤íŒ¨, 0 ë°˜í™˜: {e}")
        return jnp.zeros_like(rewards)


def train(config_path: str, db_path: Optional[str] = None) -> str:
    """
    í•™ìŠµ ì‹¤í–‰ í•¨ìˆ˜
    
    Args:
        config_path: ì„¤ì • íŒŒì¼ ê²½ë¡œ
        db_path: DB ê²½ë¡œ (ì„ íƒì )
    
    Returns:
        model_id: í•™ìŠµëœ ëª¨ë¸ ID
    """
    try:
        # ì„¤ì • íŒŒì¼ ë¡œë“œ
        with open(config_path, 'r') as f:
            config = json.load(f)
        
        # DB ê²½ë¡œ ì„¤ì •
        if db_path is None:
            db_path = config.get('paths', {}).get('db')
        
        # Trainer ì´ˆê¸°í™”
        trainer = PPOTrainer(config)
        
        # Self-play ë°ì´í„°ëŠ” íŒŒë¼ë¯¸í„°ë¡œ ë°›ì•„ì•¼ í•¨
        # ì´ í•¨ìˆ˜ëŠ” ì§ì ‘ í˜¸ì¶œë˜ì§€ ì•Šê³  train_from_selfplay_dataë¥¼ í†µí•´ í˜¸ì¶œë¨
        raise NotImplementedError(
            "train() í•¨ìˆ˜ëŠ” ì§ì ‘ í˜¸ì¶œí•˜ì§€ ë§ˆì„¸ìš”. "
            "ëŒ€ì‹  PPOTrainer.train_from_selfplay_data()ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”. "
            "ë˜ëŠ” auto_train_from_selfplay() ë˜ëŠ” auto_train_from_integrated_analysis()ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”."
        )
        
    except Exception as e:
        logger.error(f"âŒ í•™ìŠµ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        raise

