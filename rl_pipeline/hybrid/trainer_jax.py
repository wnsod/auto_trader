"""
PPO í•™ìŠµê¸° (JAX ê¸°ë°˜)
Self-play ë°ì´í„°ë¥¼ í™œìš©í•œ ê°•í™”í•™ìŠµ
"""

import logging
import os
import json
import uuid
from typing import Dict, List, Any, Optional
from datetime import datetime
import numpy as np
import pandas as pd

logger = logging.getLogger(__name__)

# JAX ê°€ìš©ì„± í™•ì¸
try:
    import jax
    import jax.numpy as jnp
    # JAX tree ìœ í‹¸ë¦¬í‹° í™•ì¸ (ë²„ì „ë³„ í˜¸í™˜ì„±)
    try:
        # ìµœì‹  ë²„ì „ (v0.4.25+): jax.tree
        _ = jax.tree
        USE_JAX_TREE = True
    except AttributeError:
        # êµ¬ë²„ì „: jax.tree_util
        try:
            from jax import tree_util
            USE_JAX_TREE = False
            JAX_TREE_UTIL = tree_util
        except ImportError:
            USE_JAX_TREE = False
            JAX_TREE_UTIL = None
    
    JAX_AVAILABLE = True
    logger.debug("âœ… trainer_jax: JAX/Flax ì„í¬íŠ¸ ì„±ê³µ")
except ImportError as e:
    JAX_AVAILABLE = False
    logger.warning(f"âš ï¸ trainer_jax: JAXê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: {e}")
    jax = None
    jnp = None
    USE_JAX_TREE = False
    JAX_TREE_UTIL = None

# neural_policy_jax ëª¨ë“ˆ ì„í¬íŠ¸ (JAX_AVAILABLE ì²´í¬ í¬í•¨)
try:
    from rl_pipeline.hybrid.neural_policy_jax import init_model, apply, save_ckpt, PolicyNetwork, JAX_AVAILABLE as NEURAL_JAX_AVAILABLE
    # neural_policy_jaxì˜ JAX_AVAILABLEë„ í™•ì¸
    if not NEURAL_JAX_AVAILABLE:
        logger.warning("âš ï¸ trainer_jax: neural_policy_jaxì—ì„œ JAX ì‚¬ìš© ë¶ˆê°€")
        if JAX_AVAILABLE:
            # ë¡œì»¬ì—ì„œëŠ” JAXê°€ ìˆì§€ë§Œ neural_policy_jaxì—ì„œëŠ” ì—†ëŠ” ê²½ìš°
            logger.warning("âš ï¸ trainer_jax: ë¡œì»¬ JAXëŠ” ìˆì§€ë§Œ neural_policy_jax ëª¨ë“ˆì—ì„œ ì‚¬ìš© ë¶ˆê°€")
        JAX_AVAILABLE = False
except ImportError as e:
    logger.warning(f"âš ï¸ trainer_jax: neural_policy_jax ì„í¬íŠ¸ ì‹¤íŒ¨: {e}")
    JAX_AVAILABLE = False
    init_model = None
    apply = None
    save_ckpt = None
    PolicyNetwork = None

from rl_pipeline.hybrid.features import (
    build_state_vector,
    build_state_vector_with_analysis,
    build_state_vector_with_strategy,  # ğŸš€ ë©”íƒ€ í•™ìŠµ
    build_state_vector_with_analysis_and_strategy,  # ğŸš€ ë©”íƒ€ í•™ìŠµ
    FEATURE_DIM,
    FEATURE_DIM_WITH_ANALYSIS,
    FEATURE_DIM_WITH_STRATEGY,  # ğŸš€ ë©”íƒ€ í•™ìŠµ: 30ì°¨ì›
    FEATURE_DIM_WITH_ANALYSIS_AND_STRATEGY  # ğŸš€ ë©”íƒ€ í•™ìŠµ: 35ì°¨ì›
)
from rl_pipeline.engine.reward_engine import RewardEngine
from rl_pipeline.db.connection_pool import get_strategy_db_pool

# ğŸ”¥ ë””ë²„ê·¸ ë¡œê¹… ì‹œìŠ¤í…œ
try:
    from rl_pipeline.monitoring import TrainingDebugger
    DEBUG_AVAILABLE = True
except ImportError:
    logger.warning("âš ï¸ ë””ë²„ê·¸ ë¡œê¹… ì‹œìŠ¤í…œì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
    DEBUG_AVAILABLE = False
    TrainingDebugger = None

# optaxëŠ” Flax 0.10+ ë²„ì „ì—ì„œ í•„ìˆ˜ (optimizer ì—­í• )
if JAX_AVAILABLE:
    try:
        import optax
        logger.debug("âœ… trainer_jax: optax ì„í¬íŠ¸ ì„±ê³µ")
    except ImportError as e:
        logger.error(f"âŒ trainer_jax: optax ì„í¬íŠ¸ ì‹¤íŒ¨: {e}")
        logger.error("âŒ optaxëŠ” í•„ìˆ˜ì…ë‹ˆë‹¤. pip install optax")
        JAX_AVAILABLE = False
        optax = None
else:
    optax = None


class PPOTrainer:
    """
    PPO (Proximal Policy Optimization) í•™ìŠµê¸° - ì˜ˆì¸¡ ì „ëµ
    
    Self-play ë°ì´í„°ë¥¼ í™œìš©í•˜ì—¬ ë°©í–¥ ì˜ˆì¸¡ ì •ì±… í•™ìŠµ
    - ì•¡ì…˜ ê³µê°„: NEUTRAL(0), UP(1), DOWN(2) - ì˜ˆì¸¡ ë°©í–¥
    - ë³´ìƒ: ì˜ˆì¸¡ ì •í™•ë„ ê¸°ë°˜ (ë°©í–¥ ë§ì¶¤/í‹€ë¦¼)
    """
    
    def __init__(self, config: Dict[str, Any], session_id: Optional[str] = None):
        """
        ì´ˆê¸°í™”

        Args:
            config: í•™ìŠµ ì„¤ì • ë”•ì…”ë„ˆë¦¬
            session_id: ë””ë²„ê·¸ ì„¸ì…˜ ID (ì„ íƒì )
        """
        if not JAX_AVAILABLE:
            error_msg = "JAXê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install jax flax"
            logger.error(f"âŒ {error_msg}")
            raise ImportError(error_msg)

        # neural_policy_jax í•¨ìˆ˜ë“¤ì´ ì‚¬ìš© ê°€ëŠ¥í•œì§€ í™•ì¸
        if init_model is None or apply is None or save_ckpt is None:
            error_msg = "neural_policy_jax ëª¨ë“ˆì—ì„œ í•„ìš”í•œ í•¨ìˆ˜ë¥¼ ì„í¬íŠ¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            logger.error(f"âŒ {error_msg}")
            raise ImportError(error_msg)

        self.config = config
        self.train_config = config.get('train', {})
        self.reward_engine = RewardEngine()

        # ğŸ”¥ ë””ë²„ê±° ì´ˆê¸°í™”
        self.debug = None
        if DEBUG_AVAILABLE and session_id:
            try:
                self.debug = TrainingDebugger(session_id=session_id)
                logger.debug(f"âœ… Training ë””ë²„ê±° ì´ˆê¸°í™” ì™„ë£Œ (session: {session_id})")
            except Exception as e:
                logger.warning(f"âš ï¸ Training ë””ë²„ê±° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        
        # ëª¨ë¸ ì´ˆê¸°í™” (ë¶„ì„ ì ìˆ˜ í¬í•¨ ì°¨ì›ìœ¼ë¡œ ê¸°ë³¸ ì„¤ì •)
        # ğŸ”¥ jaxëŠ” ëª¨ë“ˆ ë ˆë²¨ì—ì„œ ì´ë¯¸ importë˜ì–´ ìˆìŒ - global ì„ ì–¸ìœ¼ë¡œ ëª…ì‹œ
        try:
            # global ì„ ì–¸ìœ¼ë¡œ ëª¨ë“ˆ ë ˆë²¨ì˜ jaxë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì‚¬ìš© (ìŠ¤ì½”í”„ ë¬¸ì œ ì™„ì „ í•´ê²°)
            global jax, jnp
            # JAX_AVAILABLEì´ Trueì´ë¯€ë¡œ jaxëŠ” Noneì´ ì•„ë‹ˆì–´ì•¼ í•¨
            if jax is None:
                raise ImportError("JAXê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. JAX_AVAILABLE ì²´í¬ë¥¼ í†µê³¼í–ˆì§€ë§Œ jaxê°€ Noneì…ë‹ˆë‹¤.")
            
            # ğŸ”¥ JAX ë°±ì—”ë“œ í™•ì¸ ë° CPU í´ë°± ì²˜ë¦¬ (RTX 5090 í˜¸í™˜)
            try:
                # JAX í”Œë«í¼ í™•ì¸ (ì—ëŸ¬ ë°œìƒ ì‹œ CPUë¡œ í´ë°±)
                devices = jax.devices()
                logger.debug(f"ğŸ” JAX ë””ë°”ì´ìŠ¤: {devices}")
                rng_key = jax.random.PRNGKey(config.get('seed', 42))
            except RuntimeError as backend_err:
                # CUDA ë°±ì—”ë“œ ì´ˆê¸°í™” ì‹¤íŒ¨ ì‹œ CPUë¡œ í´ë°±
                if 'cuda' in str(backend_err).lower() or 'backend' in str(backend_err).lower():
                    logger.warning(f"âš ï¸ JAX CUDA ë°±ì—”ë“œ ì‚¬ìš© ë¶ˆê°€: {backend_err}")
                    logger.info("ğŸ’» JAX CPU ëª¨ë“œë¡œ ì „í™˜ ì¤‘...")
                    try:
                        jax.config.update('jax_platform_name', 'cpu')
                        # JAX ì¬ì´ˆê¸°í™” (CPU ëª¨ë“œ)
                        devices = jax.devices()
                        logger.info(f"âœ… JAX CPU ëª¨ë“œë¡œ ì „í™˜ ì™„ë£Œ: {devices}")
                        rng_key = jax.random.PRNGKey(config.get('seed', 42))
                    except Exception as cpu_fallback_err:
                        logger.error(f"âŒ JAX CPU ëª¨ë“œ ì „í™˜ë„ ì‹¤íŒ¨: {cpu_fallback_err}")
                        raise
                else:
                    raise
            # ğŸ”¥ ì˜ˆì¸¡ ì „ëµ: ì•¡ì…˜ ê³µê°„ì„ ë°©í–¥ ì˜ˆì¸¡ìœ¼ë¡œ ì •ì˜
            # action_dim=3: 0=NEUTRAL(ì¤‘ë¦½), 1=UP(ìƒìŠ¹ ì˜ˆì¸¡), 2=DOWN(í•˜ë½ ì˜ˆì¸¡)
            # ê±°ë˜ ì•¡ì…˜(BUY/SELL/HOLD)ì´ ì•„ë‹Œ ì˜ˆì¸¡ ë°©í–¥ìœ¼ë¡œ í•™ìŠµ
            # ğŸš€ ë©”íƒ€ í•™ìŠµ: Stateì— ì „ëµ íŒŒë¼ë¯¸í„° í¬í•¨ (35ì°¨ì›)
            self.model = init_model(
                rng_key,
                obs_dim=FEATURE_DIM_WITH_ANALYSIS_AND_STRATEGY,  # ğŸš€ 35ì°¨ì› (ë¶„ì„+ì „ëµ íŒŒë¼ë¯¸í„°)
                action_dim=3,  # ğŸ”¥ ì˜ˆì¸¡ ë°©í–¥: NEUTRAL(0), UP(1), DOWN(2)
                hidden_dim=self.train_config.get('hidden_dim', 128)
            )
            
            # ğŸ”¥ ëª¨ë¸ íŒŒë¼ë¯¸í„° ì´ˆê¸° ê²€ì¦ (NaN/Inf ì²´í¬)
            try:
                # ëª¨ë“ˆ ë ˆë²¨ì˜ jax, jnp ì‚¬ìš© (í•¨ìˆ˜ ë‚´ë¶€ì—ì„œ ë‹¤ì‹œ importí•˜ì§€ ì•ŠìŒ)
                def validate_params(p):
                    """ì¬ê·€ì ìœ¼ë¡œ íŒŒë¼ë¯¸í„° ê²€ì¦"""
                    if isinstance(p, dict):
                        return all(validate_params(v) for v in p.values())
                    elif hasattr(p, 'shape') and hasattr(p, 'size'):
                        if p.size > 0:
                            if not jnp.all(jnp.isfinite(p)):
                                logger.warning(f"âš ï¸ ì´ˆê¸°í™” ì‹œ íŒŒë¼ë¯¸í„°ì— NaN/Inf ë°œê²¬: shape={p.shape}")
                                return False
                        return True
                    return True
                
                if not validate_params(self.model.get('params', {})):
                    logger.error("âŒ ëª¨ë¸ ì´ˆê¸°í™” í›„ íŒŒë¼ë¯¸í„° ê²€ì¦ ì‹¤íŒ¨")
                    raise ValueError("ëª¨ë¸ íŒŒë¼ë¯¸í„°ì— NaN/Inf í¬í•¨")
            except Exception as param_check_err:
                logger.warning(f"âš ï¸ íŒŒë¼ë¯¸í„° ê²€ì¦ ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œ): {param_check_err}")
                
        except Exception as e:
            logger.error(f"âŒ ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            import traceback
            logger.error(f"ì´ˆê¸°í™” ì‹¤íŒ¨ ìƒì„¸:\n{traceback.format_exc()}")
            raise
        
        # ì˜µí‹°ë§ˆì´ì € ì´ˆê¸°í™” (optax ì‚¬ìš© - Flax 0.10+ ë²„ì „)
        if optax is None:
            raise ImportError("optaxê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install optax")
        
        # ğŸ”¥ í•™ìŠµ ì„±ëŠ¥ ê°œì„ : í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ë§ ë° ì¡°ì •
        base_lr = self.train_config.get('lr', 0.0003)
        # í•™ìŠµë¥  ì¦ê°€: Neural networkê°€ í•™ìŠµí•˜ì§€ ëª»í•˜ëŠ” ë¬¸ì œ í•´ê²°
        # ì´ì „ 0.000075 (7.5e-5)ëŠ” ë„ˆë¬´ ë‚®ì•„ì„œ ì¡°ê¸° ì¢…ë£Œë¨
        learning_rate = base_lr  # 0.0003 (3e-4) - ì ì ˆí•œ í•™ìŠµì„ ìœ„í•´ ì¦ê°€
        
        # ğŸ”¥ í•™ìŠµë¥  ìë™ ì¡°ì •ì„ ìœ„í•œ ê¸°ë³¸ê°’ ì €ì¥
        self.base_learning_rate = learning_rate
        self.current_learning_rate = learning_rate

        # ğŸ”¥ Entropy coefficient ìë™ ì¡°ì •ì„ ìœ„í•œ ê¸°ë³¸ê°’ ì €ì¥
        base_entropy_coef = self.train_config.get('entropy_coef', 0.15)
        self.base_entropy_coef = base_entropy_coef
        self.current_entropy_coef = base_entropy_coef

        # ğŸ”¥ Gradient clipping ê°•í™” + í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ë§
        # optax.adamì€ learning_rateë¥¼ ìœ„ì¹˜ ì¸ìë¡œ ë°›ìŒ
        # ë” ì•ˆì •ì ì¸ í•™ìŠµì„ ìœ„í•´ gradient clipping ê°•í™”
        self.optimizer = optax.chain(
            optax.clip_by_global_norm(0.5),  # Gradient clipping ê°•í™” (ê¸°ì¡´ë³´ë‹¤ ë” ì‘ê²Œ)
            optax.scale_by_adam(),
            optax.scale(-learning_rate)  # í•™ìŠµë¥  ì ìš©
        )
        
        self.opt_state = self.optimizer.init(self.model['params'])
        self._step_count = 0  # ìŠ¤í… ì¹´ìš´í„°
        
        # í•™ìŠµ í†µê³„
        self.training_history = []
        
        logger.info(f"âœ… PPO Trainer ì´ˆê¸°í™” ì™„ë£Œ (lr={learning_rate})")
    
    def train_from_selfplay_data(
        self,
        episodes_data: List[Dict[str, Any]],
        db_path: Optional[str] = None,
        analysis_data: Optional[Dict[str, Any]] = None  # ğŸ”¥ ì¶”ê°€: í†µí•© ë¶„ì„ ê²°ê³¼
    ) -> str:
        """
        Self-play ë°ì´í„°ë¡œ í•™ìŠµ (ë¶„ì„ ë°ì´í„° í¬í•¨ ê°€ëŠ¥)
        
        Args:
            episodes_data: Self-play ê²°ê³¼ ë°ì´í„°
            db_path: DB ê²½ë¡œ (ëª¨ë¸ ì €ì¥ìš©, Noneì´ë©´ configì—ì„œ ê°€ì ¸ì˜´)
            analysis_data: í†µí•© ë¶„ì„ ê²°ê³¼ (ì„ íƒì )
                {
                    'fractal_score': float,
                    'multi_timeframe_score': float,
                    'indicator_cross_score': float,
                    'ensemble_score': float,
                    'ensemble_confidence': float
                }
        
        Returns:
            model_id: í•™ìŠµëœ ëª¨ë¸ ID
        """
        try:
            logger.info(f"ğŸš€ PPO í•™ìŠµ ì‹œì‘: {len(episodes_data)}ê°œ ì—í”¼ì†Œë“œ" +
                       (", ë¶„ì„ ë°ì´í„° í¬í•¨" if analysis_data else ""))

            # ğŸ”¥ ë””ë²„ê±° ë¡œê¹…: í•™ìŠµ ì‹œì‘
            if self.debug:
                try:
                    self.debug.log_training_start({
                        "learning_rate": self.current_learning_rate,
                        "epochs": self.train_config.get('epochs', 30),
                        "batch_size": self.train_config.get('batch_size', 4096),
                        "num_episodes": len(episodes_data),
                        "has_analysis_data": analysis_data is not None,
                        "clip_epsilon": self.train_config.get('clip_epsilon', 0.2),
                        "entropy_coef": self.train_config.get('entropy_coef', 0.05),
                        "value_loss_coef": self.train_config.get('value_loss_coef', 0.5),
                        "gamma": self.train_config.get('gamma', 0.99),
                        "gae_lambda": self.train_config.get('gae_lambda', 0.95)
                    })
                except Exception as debug_err:
                    logger.debug(f"âš ï¸ í•™ìŠµ ì‹œì‘ ë””ë²„ê·¸ ë¡œê¹… ì‹¤íŒ¨ (ë¬´ì‹œ): {debug_err}")

            # 1. Self-play ë°ì´í„°ì—ì„œ ê²½í—˜ ì¶”ì¶œ (ë¶„ì„ ë°ì´í„° í¬í•¨)
            experiences = self._extract_experiences(episodes_data, analysis_data)
            # ğŸš€ ë©”íƒ€ í•™ìŠµ: ì „ëµ íŒŒë¼ë¯¸í„° í¬í•¨ (35ì°¨ì› or 30ì°¨ì›)
            feature_dim = FEATURE_DIM_WITH_ANALYSIS_AND_STRATEGY if analysis_data else FEATURE_DIM_WITH_STRATEGY
            logger.info(f"ğŸ“Š ì¶”ì¶œëœ ê²½í—˜: {len(experiences)}ê°œ (ì°¨ì›: {feature_dim})")

            if analysis_data:
                logger.info(f"ğŸš€ 35ì°¨ì› ë©”íƒ€ í•™ìŠµ í™œì„±í™” - í™•ì¥ ì§€í‘œ + ë¶„ì„ ì ìˆ˜ + ì „ëµ íŒŒë¼ë¯¸í„° í¬í•¨")
                logger.info(f"   í”„ë™íƒˆ: {analysis_data.get('fractal_score', 0.5):.3f}, "
                          f"ë©€í‹°TF: {analysis_data.get('multi_timeframe_score', 0.5):.3f}, "
                          f"ì§€í‘œêµì°¨: {analysis_data.get('indicator_cross_score', 0.5):.3f}")
            else:
                logger.info(f"ğŸš€ 30ì°¨ì› ë©”íƒ€ í•™ìŠµ (í™•ì¥ ì§€í‘œ + ì „ëµ íŒŒë¼ë¯¸í„° í¬í•¨, ë¶„ì„ ì ìˆ˜ ì—†ìŒ)")
            
            # ğŸ”¥ í•™ìŠµ ì „ ë°ì´í„° ê²€ì¦ ê°•í™” (ê°œì„ : ìµœì†Œ ìš”êµ¬ëŸ‰ ì™„í™”)
            if len(experiences) < 50:  # ğŸ”¥ ê°œì„ : 100 â†’ 50 (ë” ë¹ ë¥¸ í•™ìŠµ ì‹œì‘)
                logger.warning(f"âš ï¸ ê²½í—˜ ë°ì´í„°ê°€ ë§¤ìš° ë¶€ì¡±í•©ë‹ˆë‹¤ ({len(experiences)}ê°œ). ìµœì†Œ 50ê°œ ê¶Œì¥")
            elif len(experiences) < 100:
                logger.info(f"â„¹ï¸ ê²½í—˜ ë°ì´í„°: {len(experiences)}ê°œ (ê¶Œì¥: 100ê°œ ì´ìƒ)")
            
            # ì•¡ì…˜ ë‹¤ì–‘ì„± ìµœì¢… ê²€ì¦
            if experiences:
                actions = [exp.get('action', 0) for exp in experiences]
                unique_actions = set(actions)
                action_counts = {action: actions.count(action) for action in unique_actions}
                
                if len(unique_actions) < 2:
                    logger.error(f"âŒ í•™ìŠµ ì „ ê²€ì¦ ì‹¤íŒ¨: ì•¡ì…˜ ë‹¤ì–‘ì„± ë¶€ì¡± (ê³ ìœ  ì•¡ì…˜: {len(unique_actions)}ê°œ)")
                    logger.error(f"   ì•¡ì…˜ ë¶„í¬: {action_counts}")
                    logger.error(f"   í•™ìŠµì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤. ë°ì´í„° ìˆ˜ì§‘ ë¡œì§ì„ í™•ì¸í•˜ì„¸ìš”.")
                    raise ValueError(f"ì•¡ì…˜ ë‹¤ì–‘ì„± ë¶€ì¡±: ê³ ìœ  ì•¡ì…˜ {len(unique_actions)}ê°œë§Œ ì¡´ì¬ (ìµœì†Œ 2ê°œ í•„ìš”)")
                elif len(unique_actions) == 2:
                    logger.warning(f"âš ï¸ í•™ìŠµ ì „ ê²€ì¦: ì•¡ì…˜ ë‹¤ì–‘ì„± ì œí•œì  (ê³ ìœ  ì•¡ì…˜: {len(unique_actions)}ê°œ)")
                    logger.warning(f"   ì•¡ì…˜ ë¶„í¬: {action_counts}")
                    logger.warning(f"   í•™ìŠµì€ ê³„ì† ì§„í–‰ë˜ì§€ë§Œ, entropy_coefê°€ ìë™ìœ¼ë¡œ ì¦ê°€í•©ë‹ˆë‹¤.")
                else:
                    logger.info(f"âœ… í•™ìŠµ ì „ ê²€ì¦ í†µê³¼: ì•¡ì…˜ ë‹¤ì–‘ì„± ìš°ìˆ˜ (ê³ ìœ  ì•¡ì…˜: {len(unique_actions)}ê°œ)")
            else:
                logger.error("âŒ í•™ìŠµ ì „ ê²€ì¦ ì‹¤íŒ¨: ê²½í—˜ ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")
                raise ValueError("ê²½í—˜ ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")
            
            # 2. PPO ì—…ë°ì´íŠ¸
            epochs = self.train_config.get('epochs', 30)
            batch_size = self.train_config.get('batch_size', 4096)
            
            # ğŸ”¥ ì ì‘í˜• ë°°ì¹˜ í¬ê¸°: ë°ì´í„° í¬ê¸°ì— ë”°ë¼ ë™ì  ì¡°ì •
            # ë„ˆë¬´ ì‘ìœ¼ë©´ í•™ìŠµì´ ë¶ˆì•ˆì •í•˜ê³ , ë„ˆë¬´ í¬ë©´ ë©”ëª¨ë¦¬/ì»´íŒŒì¼ ë¬¸ì œ ë°œìƒ
            data_size = len(experiences)

            if data_size < 1000:
                optimal_batch_size = 64
            elif data_size < 5000:
                optimal_batch_size = 128
            elif data_size < 10000:
                optimal_batch_size = 256
            else:
                optimal_batch_size = 512

            # ì„¤ì •ëœ batch_sizeì™€ optimal_batch_size ì¤‘ ì‘ì€ ê°’ ì‚¬ìš©
            if batch_size > optimal_batch_size:
                logger.info(f"ğŸ“Š ë°°ì¹˜ í¬ê¸° ìµœì í™”: {batch_size} â†’ {optimal_batch_size} (ë°ì´í„° í¬ê¸°: {data_size})")
                batch_size = optimal_batch_size
            
            # ê²½í—˜ ìˆ˜ì— ë”°ë¼ ë™ì  ì¡°ì •
            if len(experiences) < batch_size:
                # ë°ì´í„°ê°€ ì ìœ¼ë©´ ë°°ì¹˜ í¬ê¸° ì¤„ì´ê¸°
                batch_size = min(batch_size, len(experiences))
                if batch_size == 0:
                    logger.warning("âš ï¸ ê²½í—˜ ë°ì´í„° ì—†ìŒ, í•™ìŠµ ê±´ë„ˆëœ€")
                    return None
                logger.info(f"ğŸ“Š ë°°ì¹˜ í¬ê¸° ì¡°ì •: {batch_size} (ê²½í—˜ ìˆ˜: {len(experiences)})")
            
            eval_every = self.train_config.get('eval_every_epochs', 5)
            
            # ğŸ”¥ ì¡°ê¸° ì¢…ë£Œ ì„¤ì • (ì˜ˆì¸¡ ì „ëµ: ì•¡ì…˜ ë‹¤ì–‘ì„± ë¶€ì¡± ì‹œ ë” ë§ì€ epoch í—ˆìš©)
            # ê¸°ë³¸ê°’: 10 epoch (ì´ì „ 5ì—ì„œ ì¦ê°€) - ì˜ˆì¸¡ ì „ëµ í•™ìŠµì— ë” ë§ì€ ì‹œê°„ í•„ìš”
            early_stop_patience = self.train_config.get('early_stop_patience', 10)  # 10 epoch ë™ì•ˆ ê°œì„  ì—†ìœ¼ë©´ ì¢…ë£Œ
            early_stop_min_delta = self.train_config.get('early_stop_min_delta', 0.0005)  # ìµœì†Œ ê°œì„  ì„ê³„ê°’ (0.001 â†’ 0.0005ë¡œ ì™„í™”)
            best_loss = float('inf')
            no_improvement_count = 0
            
            # ğŸ”¥ ì´ˆê¸° ì†ì‹¤ ê¸°ë¡ (í•™ìŠµ ì „) - ê°œì„ : ë” ë‚˜ì€ ì´ˆê¸°í™”
            if experiences:
                # ë” ë§ì€ ìƒ˜í”Œë¡œ ì´ˆê¸° ì†ì‹¤ ì¸¡ì • (ì•ˆì •ì„± í–¥ìƒ)
                sample_size = min(200, len(experiences))
                sample_batch = experiences[:sample_size]
                
                # ì´ˆê¸° ì†ì‹¤ ì¸¡ì • (í•™ìŠµ ì „)
                if sample_batch:
                    try:
                        initial_loss = self._update_policy(sample_batch)
                        # ì´ˆê¸° ì†ì‹¤ì´ ë¹„ì •ìƒì ìœ¼ë¡œ í¬ë©´ ê²½ê³ 
                        if initial_loss > 10.0 or initial_loss < -10.0:
                            logger.warning(f"âš ï¸ ì´ˆê¸° ì†ì‹¤ì´ ë¹„ì •ìƒì : {initial_loss:.4f}, ì •ê·œí™” ì‹œë„")
                            # ì†ì‹¤ ì •ê·œí™” (í° ê°’ ì œí•œ)
                            initial_loss = max(-5.0, min(5.0, initial_loss))
                    except Exception as init_err:
                        logger.warning(f"âš ï¸ ì´ˆê¸° ì†ì‹¤ ì¸¡ì • ì‹¤íŒ¨: {init_err}, ê¸°ë³¸ê°’ ì‚¬ìš©")
                        initial_loss = 0.0
                else:
                    initial_loss = 0.0
                
                logger.info(f"ğŸ“Š ì´ˆê¸° ì†ì‹¤ (í•™ìŠµ ì „): {initial_loss:.4f} (ìƒ˜í”Œ: {sample_size}ê°œ)")
                best_loss = initial_loss
            else:
                logger.warning("âš ï¸ í•™ìŠµ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤ (experiencesê°€ ë¹„ì–´ìˆìŒ)")
                best_loss = 0.0
            
            for epoch in range(epochs):
                # ğŸ”¥ ë””ë²„ê±°: í˜„ì¬ epoch ì„¤ì • (ë°°ì¹˜ ë¡œê¹…ì—ì„œ ì‚¬ìš©)
                if self.debug:
                    self._debug_current_epoch = epoch + 1
                    self._debug_batch_idx = 0  # ë°°ì¹˜ ì¸ë±ìŠ¤ ì´ˆê¸°í™”

                # ë°°ì¹˜ ìƒì„±
                batches = self._create_batches(experiences, batch_size)

                # ğŸ”¥ ë””ë²„ê±°: ì´ ë°°ì¹˜ ìˆ˜ ì„¤ì •
                if self.debug:
                    self._debug_total_batches = len(batches)

                # ğŸ”¥ ë””ë²„ê±° ë¡œê¹…: Epoch ì‹œì‘
                if self.debug:
                    try:
                        self.debug.log_epoch_start(
                            epoch=epoch + 1,
                            total_epochs=epochs,
                            learning_rate=self.current_learning_rate
                        )
                    except Exception as debug_err:
                        logger.debug(f"âš ï¸ Epoch ì‹œì‘ ë””ë²„ê·¸ ë¡œê¹… ì‹¤íŒ¨ (ë¬´ì‹œ): {debug_err}")

                epoch_loss = 0.0
                successful_updates = 0
                # ê° ë°°ì¹˜ë¡œ ì—…ë°ì´íŠ¸
                for batch_idx, batch in enumerate(batches):
                    try:
                        loss = self._update_policy(batch)
                        if loss is not None and not (isinstance(loss, float) and (loss != loss or loss == float('inf'))):  # NaN/Inf ì²´í¬
                            epoch_loss += loss
                            successful_updates += 1
                        else:
                            logger.debug(f"âš ï¸ Epoch {epoch+1}, Batch {batch_idx+1}: Loss ê°’ ì´ìƒ (NaN/Inf), ìŠ¤í‚µ")
                    except Exception as batch_err:
                        logger.warning(f"âš ï¸ Epoch {epoch+1}, Batch {batch_idx+1} ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {batch_err}")
                        # ê³„ì† ì§„í–‰ (ë‹¤ìŒ ë°°ì¹˜ ì²˜ë¦¬)
                        continue
                
                # ì„±ê³µí•œ ì—…ë°ì´íŠ¸ê°€ ì—†ìœ¼ë©´ ê²½ê³ 
                if successful_updates == 0:
                    logger.warning(f"âš ï¸ Epoch {epoch+1}: ëª¨ë“  ë°°ì¹˜ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨")
                
                avg_loss = epoch_loss / successful_updates if successful_updates > 0 else 0.0
                
                # ì†ì‹¤ ë³€í™” ì¶”ì 
                if epoch == 0:
                    self._initial_epoch_loss = avg_loss

                # ğŸ”¥ ì¡°ê¸° ì¢…ë£Œ ì²´í¬
                improved = False
                if avg_loss < best_loss - early_stop_min_delta:
                    # ê°œì„ ë¨
                    best_loss = avg_loss
                    no_improvement_count = 0
                    improvement_msg = "âœ… ê°œì„ "
                    improved = True
                else:
                    # ê°œì„  ì—†ìŒ
                    no_improvement_count += 1
                    improvement_msg = f"âš ï¸ ê°œì„  ì—†ìŒ ({no_improvement_count}/{early_stop_patience})"

                # ğŸ”¥ ë””ë²„ê±° ë¡œê¹…: Epoch ì¢…ë£Œ
                if self.debug:
                    try:
                        self.debug.log_epoch_end(
                            epoch=epoch + 1,
                            avg_loss=avg_loss,
                            best_loss=best_loss,
                            improved=improved,
                            no_improvement_count=no_improvement_count,
                            learning_rate=self.current_learning_rate
                        )
                    except Exception as debug_err:
                        logger.debug(f"âš ï¸ Epoch ì¢…ë£Œ ë””ë²„ê·¸ ë¡œê¹… ì‹¤íŒ¨ (ë¬´ì‹œ): {debug_err}")

                if epoch == epochs - 1:
                    loss_change = avg_loss - self._initial_epoch_loss
                    loss_change_pct = (loss_change / self._initial_epoch_loss * 100) if self._initial_epoch_loss > 0 else 0.0
                    logger.info(f"ğŸ“ˆ Epoch {epoch+1}/{epochs}: í‰ê·  Loss = {avg_loss:.4f} "
                              f"(ë³€í™”: {loss_change:+.4f}, {loss_change_pct:+.2f}%) {improvement_msg}")
                else:
                    logger.info(f"ğŸ“ˆ Epoch {epoch+1}/{epochs}: í‰ê·  Loss = {avg_loss:.4f} {improvement_msg}")

                # ğŸ”¥ í•™ìŠµë¥  ìë™ ì¡°ì • (ê°œì„  ì—†ì„ ë•Œ)
                if no_improvement_count >= 3 and no_improvement_count % 2 == 1:  # 3, 5, 7... epochë§ˆë‹¤
                    # í•™ìŠµë¥  10% ê°ì†Œ
                    self.current_learning_rate *= 0.9
                    # ì˜µí‹°ë§ˆì´ì € ì¬ìƒì„± (í•™ìŠµë¥  ë³€ê²½)
                    self.optimizer = optax.chain(
                        optax.clip_by_global_norm(0.5),
                        optax.scale_by_adam(),
                        optax.scale(-self.current_learning_rate)
                    )
                    logger.info(f"ğŸ“‰ í•™ìŠµë¥  ìë™ ì¡°ì •: {self.current_learning_rate:.6f} (ê°œì„  ì—†ìŒ {no_improvement_count}íšŒ)")

                # ğŸ”¥ ì¡°ê¸° ì¢…ë£Œ ì²´í¬
                if no_improvement_count >= early_stop_patience:
                    logger.warning(f"ğŸ›‘ ì¡°ê¸° ì¢…ë£Œ: {early_stop_patience} epoch ë™ì•ˆ ê°œì„  ì—†ìŒ (ìµœê³  Loss: {best_loss:.4f})")
                    break

                # í‰ê°€ (ì£¼ê¸°ì ìœ¼ë¡œ)
                if (epoch + 1) % eval_every == 0:
                    eval_result = self._evaluate_model(epoch, experiences[:100])
                    logger.info(f"ğŸ“Š Epoch {epoch+1} í‰ê°€: í‰ê·  ë³´ìƒ = {eval_result.get('avg_reward', 0.0):.4f}")

                # í•™ìŠµ íˆìŠ¤í† ë¦¬ ì €ì¥
                self.training_history.append({
                    'epoch': epoch + 1,
                    'loss': avg_loss,
                    'experiences_count': len(experiences)
                })
            
            # ğŸ”¥ ë””ë²„ê±° ë¡œê¹…: í•™ìŠµ ì¢…ë£Œ
            if self.debug:
                try:
                    # ìˆ˜ë ´ ì—¬ë¶€ íŒë‹¨ (ì¡°ê¸° ì¢…ë£Œ ë˜ëŠ” ìµœì¢… lossê°€ ì¶©ë¶„íˆ ë‚®ìŒ)
                    converged = (no_improvement_count >= early_stop_patience) or (avg_loss < 0.01)

                    self.debug.log_training_end(
                        total_epochs=len(self.training_history),
                        best_loss=best_loss,
                        final_loss=avg_loss if 'avg_loss' in locals() else best_loss,
                        converged=converged
                    )
                except Exception as debug_err:
                    logger.debug(f"âš ï¸ í•™ìŠµ ì¢…ë£Œ ë””ë²„ê·¸ ë¡œê¹… ì‹¤íŒ¨ (ë¬´ì‹œ): {debug_err}")

            # 3. ëª¨ë¸ ì €ì¥
            if db_path is None:
                db_path = self.config.get('paths', {}).get('db', None)

            model_id = self._save_model(db_path)
            logger.info(f"âœ… í•™ìŠµ ì™„ë£Œ: model_id={model_id}")

            return model_id
            
        except Exception as e:
            logger.error(f"âŒ í•™ìŠµ ì‹¤íŒ¨: {e}")
            raise
    
    def _extract_experiences(
        self, 
        episodes_data: List[Dict[str, Any]],
        analysis_data: Optional[Dict[str, Any]] = None
    ) -> List[Dict[str, Any]]:
        """
        Self-play ê²°ê³¼ì—ì„œ ê²½í—˜ ì¶”ì¶œ (ë¶„ì„ ë°ì´í„° í¬í•¨ ë²„ì „)
        
        Args:
            episodes_data: Self-play ì—í”¼ì†Œë“œ ë°ì´í„°
            analysis_data: í†µí•© ë¶„ì„ ê²°ê³¼ (ì„ íƒì )
                {
                    'fractal_score': float,
                    'multi_timeframe_score': float,
                    'indicator_cross_score': float,
                    'ensemble_score': float,
                    'ensemble_confidence': float
                }
        
        Returns:
            ê²½í—˜ ë¦¬ìŠ¤íŠ¸ [{state, action, reward, log_prob, value, done}, ...]
        """
        # ë¶„ì„ ë°ì´í„°ê°€ ìˆìœ¼ë©´ í™•ì¥ ë²„ì „ ì‚¬ìš©
        if analysis_data:
            return self._extract_experiences_with_analysis(episodes_data, analysis_data)
        
        # ê¸°ë³¸ ë²„ì „ (í•˜ìœ„ í˜¸í™˜ì„±)
        return self._extract_experiences_basic(episodes_data)
    
    def _extract_experiences_basic(self, episodes_data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        Self-play ê²°ê³¼ì—ì„œ ê²½í—˜ ì¶”ì¶œ (ê¸°ë³¸ ë²„ì „ - 20ì°¨ì›: ê¸°ë³¸ 15ê°œ + í™•ì¥ ì§€í‘œ 5ê°œ)
        """
        experiences = []
        
        try:
            for episode in episodes_data:
                results = episode.get('results', {})
                episode_num = episode.get('episode', 0)

                for agent_id, agent_result in results.items():
                    # ì—ì´ì „íŠ¸ë³„ ì„±ê³¼ì—ì„œ ê²½í—˜ ì¶”ì¶œ
                    trades = agent_result.get('trades', [])
                    total_pnl = agent_result.get('total_pnl', 0.0)
                    win_rate = agent_result.get('win_rate', 0.0)
                    total_trades = agent_result.get('total_trades', 0)  # ğŸ”¥ total_trades ì¶”ì¶œ
                    profit_factor = agent_result.get('profit_factor', 0.0)
                    strategy_direction = agent_result.get('strategy_direction', 'neutral')  # ğŸ”¥ ì „ëµ ë°©í–¥ ('buy', 'sell', 'neutral')
                    predicted_conf = agent_result.get('predicted_conf', 0.5)  # ğŸ”¥ ì˜ˆì¸¡ ì‹ ë¢°ë„
                    strategy_params = agent_result.get('strategy_params', {})  # ğŸš€ ë©”íƒ€ í•™ìŠµ: ì „ëµ íŒŒë¼ë¯¸í„°

                    # ğŸ”¥ ì˜ˆì¸¡ ì „ëµ: ê±°ë˜ê°€ ì—†ì–´ë„ í•™ìŠµ ë°ì´í„°ë¡œ í™œìš©
                    # ì˜ˆì¸¡ ì „ëµì€ ê±°ë˜ ê²°ê³¼ë³´ë‹¤ ì˜ˆì¸¡ ì •í™•ë„ê°€ ì¤‘ìš”í•˜ë¯€ë¡œ í•„í„° ì™„í™”
                    # ê±°ë˜ê°€ ì—†ëŠ” ê²½ìš°ì—ë„ ì‹œì¥ ìƒíƒœ ê¸°ë°˜ ì˜ˆì¸¡ ê²½í—˜ ìƒì„±
                    
                    # í’ˆì§ˆ í•„í„°ë§ ì™„í™”: ê±°ë˜ê°€ ì—†ì–´ë„ í•™ìŠµì— í¬í•¨
                    # (ì˜ˆì¸¡ ì „ëµì€ ê±°ë˜ ì‹¤í–‰ ì—¬ë¶€ë³´ë‹¤ ì˜ˆì¸¡ ì •í™•ë„ê°€ ì¤‘ìš”)
                    quality_check_passed = True  # ëª¨ë“  ì—í”¼ì†Œë“œ í¬í•¨
                    
                    # ğŸ”¥ í•™ìŠµ ì„±ëŠ¥ ê°œì„ : íŠ¸ë ˆì´ë“œë³„ ê²½í—˜ ìƒì„± (ì•¡ì…˜ ë‹¤ì–‘ì„± ìš°ì„ )
                    if trades:
                        # ğŸ”¥ ì˜ˆì¸¡ ì „ëµ: UP/DOWN ì˜ˆì¸¡ì„ ìš°ì„ ì ìœ¼ë¡œ ìˆ˜ì§‘ (NEUTRALì€ ì œí•œ)
                        # ê±°ë˜ ë°ì´í„°ì˜ BUY/SELL/HOLDë¥¼ ì˜ˆì¸¡ ë°©í–¥ìœ¼ë¡œ ë³€í™˜
                        buy_trades = [t for t in trades if t.get('direction') == 'BUY']  # â†’ UP ì˜ˆì¸¡
                        sell_trades = [t for t in trades if t.get('direction') == 'SELL']  # â†’ DOWN ì˜ˆì¸¡
                        hold_trades = [t for t in trades if t.get('direction') != 'BUY' and t.get('direction') != 'SELL']  # â†’ NEUTRAL ì˜ˆì¸¡
                        
                        # ğŸ”¥ UP/DOWN ì˜ˆì¸¡ì„ ìµœëŒ€í•œ í¬í•¨, NEUTRALì€ ì œí•œì ìœ¼ë¡œ í¬í•¨
                        # ì•¡ì…˜ ë‹¤ì–‘ì„± í™•ë³´: ìµœì†Œí•œ ê° ë°©í–¥ 1ê°œì”©ì€ ë³´ì¥
                        min_buy = min(1, len(buy_trades))  # ìµœì†Œ 1ê°œ ë³´ì¥
                        min_sell = min(1, len(sell_trades))  # ìµœì†Œ 1ê°œ ë³´ì¥
                        min_hold = min(1, len(hold_trades))  # ìµœì†Œ 1ê°œ ë³´ì¥
                        
                        selected_trades = (
                            buy_trades[:max(10, min_buy)] +  # UP ì˜ˆì¸¡ ìµœëŒ€ 10ê°œ (ìµœì†Œ 1ê°œ ë³´ì¥)
                            sell_trades[:max(10, min_sell)] +  # DOWN ì˜ˆì¸¡ ìµœëŒ€ 10ê°œ (ìµœì†Œ 1ê°œ ë³´ì¥)
                            hold_trades[:max(5, min_hold)]  # NEUTRAL ì˜ˆì¸¡ì€ ìµœëŒ€ 5ê°œë§Œ (ìµœì†Œ 1ê°œ ë³´ì¥)
                        )
                        
                        # ğŸ”¥ ì•¡ì…˜ ë‹¤ì–‘ì„± ê°•ì œ: ê° ë°©í–¥ì´ ìµœì†Œ 1ê°œì”© ìˆëŠ”ì§€ í™•ì¸
                        selected_directions = [t.get('direction') for t in selected_trades]
                        has_buy = 'BUY' in selected_directions
                        has_sell = 'SELL' in selected_directions
                        has_hold = any(d not in ['BUY', 'SELL'] for d in selected_directions)
                        
                        # ë¶€ì¡±í•œ ë°©í–¥ì´ ìˆìœ¼ë©´ ì¶”ê°€ ìƒì„±
                        if not has_buy and buy_trades:
                            selected_trades.append(buy_trades[0])
                        if not has_sell and sell_trades:
                            selected_trades.append(sell_trades[0])
                        if not has_hold and hold_trades:
                            selected_trades.append(hold_trades[0])
                        
                        # ê° íŠ¸ë ˆì´ë“œì—ì„œ ê²½í—˜ ì¶”ì¶œ
                        for trade in selected_trades:
                            # Market state ì¬êµ¬ì„± (í™•ì¥ ì§€í‘œ í¬í•¨)
                            # ì‹¤ì œë¡œëŠ” tradeì— state ì •ë³´ê°€ í¬í•¨ë˜ì–´ì•¼ í•¨
                            state = {
                                'rsi': trade.get('rsi', 50.0),
                                'macd': trade.get('macd', 0.0),
                                'volume_ratio': trade.get('volume_ratio', 1.0),
                                'atr': trade.get('atr', 0.02),
                                'adx': trade.get('adx', 25.0),
                                'mfi': trade.get('mfi', 50.0),
                                'bb_upper': trade.get('bb_upper', 1.0),
                                'bb_middle': trade.get('bb_middle', 1.0),
                                'bb_lower': trade.get('bb_lower', 1.0),
                                'macd_signal': trade.get('macd_signal', 0.0),
                                'close': trade.get('close', 1.0),
                                'open': trade.get('open', 1.0),
                                'high': trade.get('high', 1.0),
                                'low': trade.get('low', 1.0),
                                'volume': trade.get('volume', 1.0),
                                'volatility': trade.get('volatility', 0.02),
                                'regime_stage': trade.get('regime_stage', 3),
                                'regime_confidence': trade.get('regime_confidence', 0.5),
                                # ğŸš€ í™•ì¥ ì§€í‘œ ì¶”ê°€ (1ë‹¨ê³„ í™•ì¥)
                                'wave_progress': trade.get('wave_progress', 0.5),
                                'pattern_confidence': trade.get('pattern_confidence', 0.5),
                                'structure_score': trade.get('structure_score', 0.5),
                                'sentiment': trade.get('sentiment', 0.0),
                                'regime_transition_prob': trade.get('regime_transition_prob', 0.05)
                            }
                            
                            # ğŸ”¥ ì˜ˆì¸¡ ì „ëµ: ì•¡ì…˜ì„ ë°©í–¥ ì˜ˆì¸¡ìœ¼ë¡œ ë³€í™˜
                            # BUY â†’ UP(1): ìƒìŠ¹ ì˜ˆì¸¡, SELL â†’ DOWN(2): í•˜ë½ ì˜ˆì¸¡, HOLD â†’ NEUTRAL(0): ì¤‘ë¦½ ì˜ˆì¸¡
                            trade_direction = trade.get('direction', 'HOLD')
                            if trade_direction == 'BUY':
                                action = 1  # UP: ìƒìŠ¹ ì˜ˆì¸¡
                                predicted_direction = 'UP'
                            elif trade_direction == 'SELL':
                                action = 2  # DOWN: í•˜ë½ ì˜ˆì¸¡
                                predicted_direction = 'DOWN'
                            else:
                                action = 0  # NEUTRAL: ì¤‘ë¦½ ì˜ˆì¸¡
                                predicted_direction = 'NEUTRAL'
                            
                            # ğŸ”¥ ì˜ˆì¸¡ ì •í™•ë„ ê¸°ë°˜ ë³´ìƒ ì‹œìŠ¤í…œ
                            # ì‹¤ì œ ê°€ê²© ë³€í™”ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì˜ˆì¸¡ ì •í™•ë„ í‰ê°€
                            price_change = trade.get('price_change', 0.0)  # ì‹¤ì œ ê°€ê²© ë³€í™”ìœ¨
                            actual_direction = 'UP' if price_change > 0.005 else ('DOWN' if price_change < -0.005 else 'NEUTRAL')

                            # ë°©í–¥ ì˜ˆì¸¡ ì •í™•ë„ ë³´ìƒ (ì˜ˆì¸¡ ì „ëµì˜ í•µì‹¬)
                            if predicted_direction == actual_direction:
                                # ì˜ˆì¸¡ ì •í™•ë„: ë°©í–¥ ë§ì¶¤
                                if predicted_direction == 'UP':
                                    direction_reward = 1.0  # ìƒìŠ¹ ì˜ˆì¸¡ ë§ì¶¤
                                elif predicted_direction == 'DOWN':
                                    direction_reward = 1.0  # í•˜ë½ ì˜ˆì¸¡ ë§ì¶¤
                                else:  # NEUTRAL
                                    direction_reward = 0.7  # ì¤‘ë¦½ ì˜ˆì¸¡ ë§ì¶¤ (ë³´ìˆ˜ì  ë³´ìƒ)
                            elif (predicted_direction == 'UP' and actual_direction == 'DOWN') or \
                                 (predicted_direction == 'DOWN' and actual_direction == 'UP'):
                                # ì˜ˆì¸¡ ì •ë°˜ëŒ€: í° í˜ë„í‹°
                                direction_reward = -1.0
                            else:
                                # ì˜ˆì¸¡ ë¶€ë¶„ ì˜¤ë¥˜ (UP/DOWN â†” NEUTRAL)
                                direction_reward = -0.3

                            # ì˜ˆì¸¡ ì‹ ë¢°ë„ ê¸°ë°˜ ë³´ì • (win_rate í™œìš©)
                            confidence_bonus = (win_rate - 0.5) * 0.5  # -0.25 ~ +0.25

                            # ìµœì¢… ë³´ìƒ: ì˜ˆì¸¡ ì •í™•ë„ + ì‹ ë¢°ë„ ë³´ë„ˆìŠ¤
                            reward = direction_reward + confidence_bonus

                            # ğŸ”¥ ì˜ˆì¸¡ í™œì„±í™” ë³´ë„ˆìŠ¤ (NEUTRALë§Œ í•˜ì§€ ì•Šë„ë¡)
                            # UP/DOWN ì˜ˆì¸¡ì€ ë” ë§ì€ ì •ë³´ë¥¼ ì œê³µí•˜ë¯€ë¡œ ë³´ë„ˆìŠ¤
                            if predicted_direction != 'NEUTRAL':
                                reward += 0.1  # ë°©í–¥ ì˜ˆì¸¡ ì‹œë„ì— ì‘ì€ ë³´ë„ˆìŠ¤
                            
                            # ê¸°ë³¸ log_prob (ê· ë“± ë¶„í¬ ê°€ì •: log(1/3) â‰ˆ -1.1)
                            log_prob = -1.1
                            
                            # ê¸°ë³¸ value estimate (ë³´ìƒ ê¸°ë°˜)
                            value = reward * 0.9  # ê°„ë‹¨í•œ ì¶”ì •

                            # ğŸš€ ë©”íƒ€ í•™ìŠµ: ìƒíƒœ ë²¡í„° ìƒì„± (30ì°¨ì›: 20 base + 10 strategy params)
                            state_vec = build_state_vector_with_strategy(state, strategy_params)
                            
                            experience = {
                                'episode': episode_num,
                                'agent_id': agent_id,
                                'state': state_vec,  # ğŸ”¥ numpy arrayë¡œ ë³€í™˜
                                'action': action,
                                'reward': reward,
                                'log_prob': log_prob,
                                'value': value,
                                'done': False  # ë‹¨ì¼ íŠ¸ë ˆì´ë“œëŠ” ì™„ë£Œë¡œ ê°„ì£¼
                            }
                            experiences.append(experience)
                        
                        # ğŸ”¥ UP/DOWN ì˜ˆì¸¡ tradesê°€ ìˆìœ¼ë©´ ë” ì´ìƒ ì¶”ê°€í•˜ì§€ ì•ŠìŒ (ì˜ˆì¸¡ ë‹¤ì–‘ì„± í™•ë³´)
                        # ëª¨ë“  tradesë¥¼ ë‹¤ ì‚¬ìš©í•˜ë©´ ë„ˆë¬´ ë§ì•„ì§€ë¯€ë¡œ, ì„ íƒëœ tradesë§Œ ì‚¬ìš©
                        break  # ì´ ì—ì´ì „íŠ¸ëŠ” tradesê°€ ìˆìœ¼ë¯€ë¡œ breakí•˜ì—¬ ë‹¤ìŒ ì—ì´ì „íŠ¸ë¡œ
                    
                    # ğŸ”¥ tradesê°€ ì—†ëŠ” ê²½ìš°: total_tradesê°€ ìˆìœ¼ë©´ ê²½í—˜ ìƒì„±
                    elif total_trades > 0:
                        # total_tradesëŠ” ìˆì§€ë§Œ trades ë¦¬ìŠ¤íŠ¸ëŠ” ì—†ëŠ” ê²½ìš°
                        # (ì˜ˆì¸¡ self-playì—ì„œ predictionsë¥¼ tradesë¡œ ë³€í™˜í–ˆì§€ë§Œ ì‹¤ì œ ê±°ë˜ëŠ” ì—†ìŒ)
                        # ğŸ”¥ ì „ëµ ë°©í–¥ì„±ê³¼ ì˜ˆì¸¡ ì‹ ë¢°ë„ë¥¼ í™œìš©í•˜ì—¬ ë‹¤ì–‘í•œ ì•¡ì…˜ ìƒì„±
                        # ğŸ”¥ í•©ì„± ë°ì´í„° ìƒì„±ëŸ‰ ì¦ê°€: ìµœì†Œ 5ê°œ ë³´ì¥, ìµœëŒ€ 20ê°œ (í•™ìŠµ ë°ì´í„° ì¦ê°€)
                        num_experiences = max(5, min(total_trades, 20))

                        # ğŸ”¥ ì „ëµ ë°©í–¥ì„± ê¸°ë°˜ ì•¡ì…˜ ë¶„í¬ ìƒì„± (ë‹¤ì–‘ì„± í™•ë³´ - ê° ë°©í–¥ ìµœì†Œ 1ê°œ ë³´ì¥)
                        # ê° ì•¡ì…˜ì„ ì•ì— ë°°ì¹˜í•˜ì—¬ num_experiencesê°€ ì‘ì•„ë„ ëª¨ë“  ì•¡ì…˜ í¬í•¨ ë³´ì¥
                        if strategy_direction == 'buy':
                            # ë§¤ìˆ˜ ì „ëµ: UP ìš°ì„¸, DOWN/NEUTRAL í¬í•¨ (ë‹¤ì–‘ì„±)
                            # ìµœì†Œ ê° ë°©í–¥ 1ê°œì”© ë¨¼ì € ë³´ì¥ (ì• 3ê°œ), ë‚˜ë¨¸ì§€ UP ìœ„ì£¼ë¡œ ì±„ì›€
                            action_distribution = [0, 1, 2] + [1] * 4 + [2] * 2 + [1]  # NEUTRAL, UP, DOWN ê° 1ê°œ ë¨¼ì €, ë‚˜ë¨¸ì§€ UP ìœ„ì£¼
                        elif strategy_direction == 'sell':
                            # ë§¤ë„ ì „ëµ: DOWN ìš°ì„¸, UP/NEUTRAL í¬í•¨ (ë‹¤ì–‘ì„±)
                            # ìµœì†Œ ê° ë°©í–¥ 1ê°œì”© ë¨¼ì € ë³´ì¥ (ì• 3ê°œ), ë‚˜ë¨¸ì§€ DOWN ìœ„ì£¼ë¡œ ì±„ì›€
                            action_distribution = [0, 1, 2] + [2] * 4 + [1] * 2 + [2]  # NEUTRAL, UP, DOWN ê° 1ê°œ ë¨¼ì €, ë‚˜ë¨¸ì§€ DOWN ìœ„ì£¼
                        else:
                            # ì¤‘ë¦½ ì „ëµ: ê· í˜•ì¡íŒ ë¶„í¬ (ê° ë°©í–¥ ìµœì†Œ 1ê°œ ë³´ì¥)
                            action_distribution = [0, 1, 2] + [0] * 2 + [1] * 2 + [2] * 2 + [0, 1, 2]  # ê· ë“± ë¶„í¬
                        
                        for exp_idx in range(num_experiences):
                            # ê°„ë‹¨í•œ ìƒíƒœ ë²¡í„° (ì„±ê³¼ ê¸°ë°˜ ì¶”ì •)
                            state = {
                                'rsi': 50.0, 'macd': 0.0, 'volume_ratio': 1.0, 'atr': 0.02,
                                'adx': 25.0, 'mfi': 50.0, 'bb_upper': 1.0, 'bb_middle': 1.0,
                                'bb_lower': 1.0, 'macd_signal': 0.0, 'close': 1.0, 'open': 1.0,
                                'high': 1.0, 'low': 1.0, 'volume': 1.0, 'volatility': 0.02,
                                'regime_stage': 3, 'regime_confidence': 0.5,
                                # ğŸš€ í™•ì¥ ì§€í‘œ ì¶”ê°€
                                'wave_progress': 0.5, 'pattern_confidence': predicted_conf,
                                'structure_score': 0.5, 'sentiment': 0.0, 'regime_transition_prob': 0.05
                            }
                            
                            # ğŸ”¥ ì•¡ì…˜ ë¶„í¬ì—ì„œ ì„ íƒ (ë‹¤ì–‘ì„± í™•ë³´)
                            action = action_distribution[exp_idx % len(action_distribution)]

                            # ğŸš€ ë©”íƒ€ í•™ìŠµ: ìƒíƒœ ë²¡í„° ìƒì„± (30ì°¨ì›)
                            state_vec = build_state_vector_with_strategy(state, strategy_params)

                            # ğŸ”¥ ì˜ˆì¸¡ ì‹ ë¢°ë„ ê¸°ë°˜ ë³´ìƒ (win_rateì™€ predicted_conf í™œìš©)
                            base_reward = win_rate - 0.5  # -0.5 ~ 0.5
                            confidence_bonus = (predicted_conf - 0.5) * 0.3  # -0.15 ~ 0.15
                            
                            # ì•¡ì…˜ê³¼ ì „ëµ ë°©í–¥ ì¼ì¹˜ ì—¬ë¶€ì— ë”°ë¥¸ ë³´ì •
                            if (action == 1 and strategy_direction == 'buy') or \
                               (action == 2 and strategy_direction == 'sell') or \
                               (action == 0 and strategy_direction == 'neutral'):
                                direction_bonus = 0.1  # ë°©í–¥ ì¼ì¹˜ ë³´ë„ˆìŠ¤
                            else:
                                direction_bonus = -0.05  # ë°©í–¥ ë¶ˆì¼ì¹˜ ì‘ì€ í˜ë„í‹°
                            
                            reward = base_reward + confidence_bonus + direction_bonus
                            
                            experience = {
                                'episode': episode_num,
                                'agent_id': agent_id,
                                'state': state_vec,
                                'action': action,
                                'reward': reward,
                                'log_prob': -1.1,
                                'value': reward * 0.9,
                                'done': True
                            }
                            experiences.append(experience)
                        
                        # í•œ ì—ì´ì „íŠ¸ë‹¹ ìµœëŒ€ 10ê°œ ìƒì„±í–ˆìœ¼ë¯€ë¡œ breakí•˜ì—¬ ë‹¤ìŒ ì—ì´ì „íŠ¸ë¡œ
                        break
                    
                    else:
                        # ğŸ”¥ ê±°ë˜ê°€ ì—†ëŠ” ê²½ìš°: ì „ëµ ë°©í–¥ì„±ê³¼ ì‹œì¥ ìƒíƒœ ê¸°ë°˜ ì˜ˆì¸¡ ê²½í—˜ ìƒì„±
                        # ì˜ˆì¸¡ ì „ëµì€ ê±°ë˜ê°€ ì—†ì–´ë„ ì‹œì¥ ìƒíƒœë¥¼ í•™ìŠµí•  ìˆ˜ ìˆì–´ì•¼ í•¨
                        # ë‹¤ì–‘í•œ ì˜ˆì¸¡ ë°©í–¥(UP/DOWN/NEUTRAL)ì„ ê· í˜•ìˆê²Œ ìƒì„±
                        
                        # ì—ì´ì „íŠ¸ ê²°ê³¼ì—ì„œ ì‹œì¥ ì •ë³´ ì¶”ì¶œ (ê°€ëŠ¥í•œ ê²½ìš°)
                        market_info = agent_result.get('market_info', {})
                        regime = agent_result.get('regime', 'neutral')
                        
                        # ğŸ”¥ ì „ëµ ë°©í–¥ì„±ê³¼ ë ˆì§ì„ ì¢…í•©í•˜ì—¬ ì˜ˆì¸¡ ë°©í–¥ ì¶”ì • (ë‹¤ì–‘ì„± í™•ë³´ - ê° ë°©í–¥ ìµœì†Œ 1ê°œ ë³´ì¥)
                        # ê° ì•¡ì…˜ì„ ì•ì— ë°°ì¹˜í•˜ì—¬ num_experiencesê°€ ì‘ì•„ë„ ëª¨ë“  ì•¡ì…˜ í¬í•¨ ë³´ì¥
                        if strategy_direction == 'buy':
                            # ë§¤ìˆ˜ ì „ëµ: UP ìš°ì„¸, DOWN/NEUTRAL í¬í•¨ (ê° ë°©í–¥ ìµœì†Œ 1ê°œ ë³´ì¥)
                            if 'bull' in regime.lower():
                                predicted_actions = [0, 1, 2] + [1] * 4 + [2] * 2 + [1]  # UP ìœ„ì£¼
                            elif 'bear' in regime.lower():
                                predicted_actions = [0, 1, 2] + [1] * 2 + [2] * 3 + [1]  # DOWN í˜¼í•©
                            else:
                                predicted_actions = [0, 1, 2] + [1] * 3 + [2] * 2 + [1, 2]  # ê· í˜•
                        elif strategy_direction == 'sell':
                            # ë§¤ë„ ì „ëµ: DOWN ìš°ì„¸, UP/NEUTRAL í¬í•¨ (ê° ë°©í–¥ ìµœì†Œ 1ê°œ ë³´ì¥)
                            if 'bear' in regime.lower():
                                predicted_actions = [0, 1, 2] + [2] * 4 + [1] * 2 + [2]  # DOWN ìœ„ì£¼
                            elif 'bull' in regime.lower():
                                predicted_actions = [0, 1, 2] + [2] * 2 + [1] * 3 + [2]  # UP í˜¼í•©
                            else:
                                predicted_actions = [0, 1, 2] + [2] * 3 + [1] * 2 + [2, 1]  # ê· í˜•
                        else:
                            # ì¤‘ë¦½ ì „ëµ: ê· í˜•ì¡íŒ ë¶„í¬ (ê° ë°©í–¥ ìµœì†Œ 1ê°œ ë³´ì¥)
                            if 'bull' in regime.lower():
                                predicted_actions = [0, 1, 2] + [1] * 3 + [0, 2] * 2 + [1]  # UP ìœ„ì£¼
                            elif 'bear' in regime.lower():
                                predicted_actions = [0, 1, 2] + [2] * 3 + [0, 1] * 2 + [2]  # DOWN ìœ„ì£¼
                            else:
                                predicted_actions = [0, 1, 2] + [0] * 2 + [1] * 2 + [2] * 2 + [0]  # ê· ë“±
                        
                        # ğŸ”¥ ê° ì˜ˆì¸¡ ë°©í–¥ë³„ë¡œ ê²½í—˜ ìƒì„± (ìµœì†Œ 5ê°œ, ìµœëŒ€ 20ê°œ, í•™ìŠµ ë°ì´í„° ì¦ê°€)
                        num_pred_experiences = max(5, min(len(predicted_actions), 20))
                        for action in predicted_actions[:num_pred_experiences]:
                            # ì‹œì¥ ìƒíƒœ ì¶”ì • (ì—ì´ì „íŠ¸ ê²°ê³¼ ê¸°ë°˜, í™•ì¥ ì§€í‘œ í¬í•¨)
                            state = {
                                'rsi': market_info.get('rsi', 50.0),
                                'macd': market_info.get('macd', 0.0),
                                'volume_ratio': market_info.get('volume_ratio', 1.0),
                                'atr': market_info.get('atr', 0.02),
                                'adx': market_info.get('adx', 25.0),
                                'mfi': market_info.get('mfi', 50.0),
                                'bb_upper': market_info.get('bb_upper', 1.0),
                                'bb_middle': market_info.get('bb_middle', 1.0),
                                'bb_lower': market_info.get('bb_lower', 1.0),
                                'macd_signal': market_info.get('macd_signal', 0.0),
                                'close': market_info.get('close', 1.0),
                                'open': market_info.get('open', 1.0),
                                'high': market_info.get('high', 1.0),
                                'low': market_info.get('low', 1.0),
                                'volume': market_info.get('volume', 1.0),
                                'volatility': market_info.get('volatility', 0.02),
                                'regime_stage': market_info.get('regime_stage', 3),
                                'regime_confidence': market_info.get('regime_confidence', 0.5),
                                # ğŸš€ í™•ì¥ ì§€í‘œ ì¶”ê°€
                                'wave_progress': market_info.get('wave_progress', 0.5),
                                'pattern_confidence': market_info.get('pattern_confidence', predicted_conf),
                                'structure_score': market_info.get('structure_score', 0.5),
                                'sentiment': market_info.get('sentiment', 0.0),
                                'regime_transition_prob': market_info.get('regime_transition_prob', 0.05)
                            }

                            # ğŸš€ ë©”íƒ€ í•™ìŠµ: ìƒíƒœ ë²¡í„° ìƒì„± (30ì°¨ì›)
                            state_vec = build_state_vector_with_strategy(state, strategy_params)

                            # ğŸ”¥ ì˜ˆì¸¡ ì „ëµ: ì „ëµ ë°©í–¥ì„±, ë ˆì§, ì˜ˆì¸¡ ì‹ ë¢°ë„ ì¢…í•© ë³´ìƒ
                            base_reward = 0.0
                            
                            # ì „ëµ ë°©í–¥ê³¼ ì•¡ì…˜ ì¼ì¹˜ ì—¬ë¶€
                            if (action == 1 and strategy_direction == 'buy') or \
                               (action == 2 and strategy_direction == 'sell') or \
                               (action == 0 and strategy_direction == 'neutral'):
                                base_reward += 0.1  # ë°©í–¥ ì¼ì¹˜ ë³´ë„ˆìŠ¤
                            
                            # ë ˆì§ê³¼ ì•¡ì…˜ ì¼ì¹˜ ì—¬ë¶€
                            if (action == 1 and 'bull' in regime.lower()) or \
                               (action == 2 and 'bear' in regime.lower()) or \
                               (action == 0 and ('sideways' in regime.lower() or 'neutral' in regime.lower())):
                                base_reward += 0.1  # ë ˆì§ ì¼ì¹˜ ë³´ë„ˆìŠ¤
                            else:
                                base_reward -= 0.05  # ë ˆì§ ë¶ˆì¼ì¹˜ ì‘ì€ í˜ë„í‹°
                            
                            # ì˜ˆì¸¡ ì‹ ë¢°ë„ ë³´ì •
                            confidence_bonus = (predicted_conf - 0.5) * 0.2  # -0.1 ~ 0.1
                            
                            reward = base_reward + confidence_bonus
                            
                            experience = {
                                'episode': episode_num,
                                'agent_id': agent_id,
                                'state': state_vec,
                                'action': action,  # ğŸ”¥ ë‹¤ì–‘í•œ ì•¡ì…˜ (UP/DOWN/NEUTRAL)
                                'reward': reward,
                                'log_prob': -1.1,
                                'value': reward * 0.9,
                                'done': True
                            }
                            experiences.append(experience)
                        
                        # ğŸ”¥ ì²« ë²ˆì§¸ ì—ì´ì „íŠ¸ë§Œ ì‚¬ìš© (ì¤‘ë³µ ë°©ì§€)
                        break
        
        except Exception as e:
            logger.warning(f"âš ï¸ ê²½í—˜ ì¶”ì¶œ ì¤‘ ì¼ë¶€ ë°ì´í„° ì†ì‹¤: {e}")
            import traceback
            logger.debug(f"ê²½í—˜ ì¶”ì¶œ ìƒì„¸ ì—ëŸ¬:\n{traceback.format_exc()}")
        
        # ğŸ”¥ ë°ì´í„° ê²€ì¦ ê°•í™”: ì•¡ì…˜ ë‹¤ì–‘ì„± ì²´í¬
        if experiences:
            actions = [exp.get('action', 0) for exp in experiences]
            unique_actions = set(actions)
            action_counts = {action: actions.count(action) for action in unique_actions}
            
            # ì•¡ì…˜ ë¶„í¬ ë¡œê¹…
            logger.info(f"ğŸ“Š ê²½í—˜ ì¶”ì¶œ ì™„ë£Œ: ì´ {len(experiences)}ê°œ, ê³ ìœ  ì•¡ì…˜: {len(unique_actions)}ê°œ")
            logger.info(f"   ì•¡ì…˜ ë¶„í¬: NEUTRAL(0)={action_counts.get(0, 0)}, UP(1)={action_counts.get(1, 0)}, DOWN(2)={action_counts.get(2, 0)}")
            
            # ğŸ”¥ ì•¡ì…˜ ë‹¤ì–‘ì„± ê²€ì¦
            if len(unique_actions) < 2:
                logger.warning(f"âš ï¸ ì•¡ì…˜ ë‹¤ì–‘ì„± ë¶€ì¡±: ê³ ìœ  ì•¡ì…˜ {len(unique_actions)}ê°œë§Œ ì¡´ì¬")
                logger.warning(f"   ì•¡ì…˜ ë¶„í¬: {action_counts}")
                
                # ì•¡ì…˜ ë‹¤ì–‘ì„± ë¶€ì¡± ì‹œ ê²½ê³ ë§Œ ì¶œë ¥ (í•™ìŠµì€ ê³„ì† ì§„í–‰)
                # í•™ìŠµ ê³¼ì •ì—ì„œ entropy_coefê°€ ìë™ìœ¼ë¡œ ì¦ê°€í•˜ì—¬ íƒí—˜ì„ ê°•í™”í•¨
            elif len(unique_actions) == 2:
                logger.info(f"âœ… ì•¡ì…˜ ë‹¤ì–‘ì„± ì–‘í˜¸: ê³ ìœ  ì•¡ì…˜ {len(unique_actions)}ê°œ")
            else:
                logger.info(f"âœ… ì•¡ì…˜ ë‹¤ì–‘ì„± ìš°ìˆ˜: ê³ ìœ  ì•¡ì…˜ {len(unique_actions)}ê°œ (ëª¨ë“  ë°©í–¥ í¬í•¨)")
        else:
            logger.warning("âš ï¸ ê²½í—˜ ì¶”ì¶œ ê²°ê³¼ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")
        
        logger.debug(f"âœ… ì´ {len(experiences)}ê°œ ê²½í—˜ ì¶”ì¶œ ì™„ë£Œ")
        return experiences
    
    def _extract_experiences_with_analysis(
        self,
        episodes_data: List[Dict[str, Any]],
        analysis_data: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """
        Self-play ê²°ê³¼ + ë¶„ì„ ë°ì´í„°ì—ì„œ ê²½í—˜ ì¶”ì¶œ (25ì°¨ì›)
        
        Args:
            episodes_data: Self-play ì—í”¼ì†Œë“œ ë°ì´í„°
            analysis_data: í†µí•© ë¶„ì„ ê²°ê³¼
                {
                    'fractal_score': float,
                    'multi_timeframe_score': float,
                    'indicator_cross_score': float,
                    'ensemble_score': float,
                    'ensemble_confidence': float
                }
        
        Returns:
            ê²½í—˜ ë¦¬ìŠ¤íŠ¸ (stateëŠ” 35ì°¨ì› ë²¡í„°: 20 base + 5 analysis + 10 strategy params)
        """
        experiences = []
        
        try:
            # ë¶„ì„ ì ìˆ˜ ì¶”ì¶œ (ê¸°ë³¸ê°’ 0.5)
            fractal_score = analysis_data.get('fractal_score', 0.5)
            multi_timeframe_score = analysis_data.get('multi_timeframe_score', 0.5)
            indicator_cross_score = analysis_data.get('indicator_cross_score', 0.5)
            ensemble_score = analysis_data.get('ensemble_score', 0.5)
            ensemble_confidence = analysis_data.get('ensemble_confidence', 0.5)
            
            for episode in episodes_data:
                results = episode.get('results', {})
                episode_num = episode.get('episode', 0)
                
                for agent_id, agent_result in results.items():
                    # ì—ì´ì „íŠ¸ë³„ ì„±ê³¼ì—ì„œ ê²½í—˜ ì¶”ì¶œ
                    trades = agent_result.get('trades', [])
                    total_pnl = agent_result.get('total_pnl', 0.0)
                    win_rate = agent_result.get('win_rate', 0.0)
                    total_trades = agent_result.get('total_trades', 0)
                    profit_factor = agent_result.get('profit_factor', 0.0)
                    strategy_direction = agent_result.get('strategy_direction', 'neutral')  # ğŸ”¥ ì „ëµ ë°©í–¥
                    predicted_conf = agent_result.get('predicted_conf', 0.5)  # ğŸ”¥ ì˜ˆì¸¡ ì‹ ë¢°ë„
                    strategy_params = agent_result.get('strategy_params', {})  # ğŸš€ ë©”íƒ€ í•™ìŠµ: ì „ëµ íŒŒë¼ë¯¸í„°

                    # ğŸ”¥ ì˜ˆì¸¡ ì „ëµ: ê±°ë˜ê°€ ì—†ì–´ë„ í•™ìŠµ ë°ì´í„°ë¡œ í™œìš©
                    # ì˜ˆì¸¡ ì „ëµì€ ê±°ë˜ ê²°ê³¼ë³´ë‹¤ ì˜ˆì¸¡ ì •í™•ë„ê°€ ì¤‘ìš”í•˜ë¯€ë¡œ í•„í„° ì™„í™”
                    quality_check_passed = True  # ëª¨ë“  ì—í”¼ì†Œë“œ í¬í•¨

                    # ğŸ”¥ í•™ìŠµ ì„±ëŠ¥ ê°œì„ : íŠ¸ë ˆì´ë“œë³„ ê²½í—˜ ìƒì„± (ì•¡ì…˜ ë‹¤ì–‘ì„± ìš°ì„ )
                    if trades:
                        # ğŸ”¥ ì˜ˆì¸¡ ì „ëµ: UP/DOWN ì˜ˆì¸¡ì„ ìš°ì„ ì ìœ¼ë¡œ ìˆ˜ì§‘ (NEUTRALì€ ì œí•œ)
                        # ê±°ë˜ ë°ì´í„°ì˜ BUY/SELL/HOLDë¥¼ ì˜ˆì¸¡ ë°©í–¥ìœ¼ë¡œ ë³€í™˜
                        buy_trades = [t for t in trades if t.get('direction') == 'BUY']  # â†’ UP ì˜ˆì¸¡
                        sell_trades = [t for t in trades if t.get('direction') == 'SELL']  # â†’ DOWN ì˜ˆì¸¡
                        hold_trades = [t for t in trades if t.get('direction') != 'BUY' and t.get('direction') != 'SELL']  # â†’ NEUTRAL ì˜ˆì¸¡
                        
                        # ğŸ”¥ UP/DOWN ì˜ˆì¸¡ì„ ìµœëŒ€í•œ í¬í•¨, NEUTRALì€ ì œí•œì ìœ¼ë¡œ í¬í•¨
                        # ì•¡ì…˜ ë‹¤ì–‘ì„± í™•ë³´: ìµœì†Œí•œ ê° ë°©í–¥ 1ê°œì”©ì€ ë³´ì¥
                        min_buy = min(1, len(buy_trades))  # ìµœì†Œ 1ê°œ ë³´ì¥
                        min_sell = min(1, len(sell_trades))  # ìµœì†Œ 1ê°œ ë³´ì¥
                        min_hold = min(1, len(hold_trades))  # ìµœì†Œ 1ê°œ ë³´ì¥
                        
                        selected_trades = (
                            buy_trades[:max(10, min_buy)] +  # UP ì˜ˆì¸¡ ìµœëŒ€ 10ê°œ (ìµœì†Œ 1ê°œ ë³´ì¥)
                            sell_trades[:max(10, min_sell)] +  # DOWN ì˜ˆì¸¡ ìµœëŒ€ 10ê°œ (ìµœì†Œ 1ê°œ ë³´ì¥)
                            hold_trades[:max(5, min_hold)]  # NEUTRAL ì˜ˆì¸¡ì€ ìµœëŒ€ 5ê°œë§Œ (ìµœì†Œ 1ê°œ ë³´ì¥)
                        )
                        
                        # ğŸ”¥ ì•¡ì…˜ ë‹¤ì–‘ì„± ê°•ì œ: ê° ë°©í–¥ì´ ìµœì†Œ 1ê°œì”© ìˆëŠ”ì§€ í™•ì¸
                        selected_directions = [t.get('direction') for t in selected_trades]
                        has_buy = 'BUY' in selected_directions
                        has_sell = 'SELL' in selected_directions
                        has_hold = any(d not in ['BUY', 'SELL'] for d in selected_directions)
                        
                        # ë¶€ì¡±í•œ ë°©í–¥ì´ ìˆìœ¼ë©´ ì¶”ê°€ ìƒì„±
                        if not has_buy and buy_trades:
                            selected_trades.append(buy_trades[0])
                        if not has_sell and sell_trades:
                            selected_trades.append(sell_trades[0])
                        if not has_hold and hold_trades:
                            selected_trades.append(hold_trades[0])
                        
                        for trade in selected_trades:
                            # Market state ì¬êµ¬ì„± (í™•ì¥ ì§€í‘œ í¬í•¨)
                            state = {
                                'rsi': trade.get('rsi', 50.0),
                                'macd': trade.get('macd', 0.0),
                                'volume_ratio': trade.get('volume_ratio', 1.0),
                                'atr': trade.get('atr', 0.02),
                                'adx': trade.get('adx', 25.0),
                                'mfi': trade.get('mfi', 50.0),
                                'bb_upper': trade.get('bb_upper', 1.0),
                                'bb_middle': trade.get('bb_middle', 1.0),
                                'bb_lower': trade.get('bb_lower', 1.0),
                                'macd_signal': trade.get('macd_signal', 0.0),
                                'close': trade.get('close', 1.0),
                                'open': trade.get('open', 1.0),
                                'high': trade.get('high', 1.0),
                                'low': trade.get('low', 1.0),
                                'volume': trade.get('volume', 1.0),
                                'volatility': trade.get('volatility', 0.02),
                                'regime_stage': trade.get('regime_stage', 3),
                                'regime_confidence': trade.get('regime_confidence', 0.5),
                                # ğŸš€ í™•ì¥ ì§€í‘œ ì¶”ê°€ (1ë‹¨ê³„ í™•ì¥)
                                'wave_progress': trade.get('wave_progress', 0.5),
                                'pattern_confidence': trade.get('pattern_confidence', 0.5),
                                'structure_score': trade.get('structure_score', 0.5),
                                'sentiment': trade.get('sentiment', 0.0),
                                'regime_transition_prob': trade.get('regime_transition_prob', 0.05)
                            }
                            
                            # ğŸš€ ë©”íƒ€ í•™ìŠµ: ë¶„ì„+ì „ëµ íŒŒë¼ë¯¸í„° í¬í•¨ ìƒíƒœ ë²¡í„° ìƒì„± (35ì°¨ì›)
                            # í”„ë™íƒˆ/ë©€í‹°íƒ€ì„í”„ë ˆì„/ì§€í‘œêµì°¨ ì ìˆ˜ + ì „ëµ íŒŒë¼ë¯¸í„° í¬í•¨í•˜ì—¬ ë” ê°•ë ¥í•œ í•™ìŠµ
                            enhanced_state_vec = build_state_vector_with_analysis_and_strategy(
                                state,
                                strategy_params,
                                fractal_score=fractal_score,
                                multi_timeframe_score=multi_timeframe_score,
                                indicator_cross_score=indicator_cross_score,
                                ensemble_score=ensemble_score,
                                ensemble_confidence=ensemble_confidence
                            )
                            
                            # ğŸ”¥ ì˜ˆì¸¡ ì „ëµ: ì•¡ì…˜ì„ ë°©í–¥ ì˜ˆì¸¡ìœ¼ë¡œ ë³€í™˜
                            # BUY â†’ UP(1): ìƒìŠ¹ ì˜ˆì¸¡, SELL â†’ DOWN(2): í•˜ë½ ì˜ˆì¸¡, HOLD â†’ NEUTRAL(0): ì¤‘ë¦½ ì˜ˆì¸¡
                            trade_direction = trade.get('direction', 'HOLD')
                            if trade_direction == 'BUY':
                                action = 1  # UP: ìƒìŠ¹ ì˜ˆì¸¡
                                predicted_direction = 'UP'
                            elif trade_direction == 'SELL':
                                action = 2  # DOWN: í•˜ë½ ì˜ˆì¸¡
                                predicted_direction = 'DOWN'
                            else:
                                action = 0  # NEUTRAL: ì¤‘ë¦½ ì˜ˆì¸¡
                                predicted_direction = 'NEUTRAL'
                            
                            # ğŸ”¥ ì˜ˆì¸¡ ì •í™•ë„ ê¸°ë°˜ ë³´ìƒ ì‹œìŠ¤í…œ
                            # ì‹¤ì œ ê°€ê²© ë³€í™”ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì˜ˆì¸¡ ì •í™•ë„ í‰ê°€
                            price_change = trade.get('price_change', 0.0)  # ì‹¤ì œ ê°€ê²© ë³€í™”ìœ¨
                            actual_direction = 'UP' if price_change > 0.005 else ('DOWN' if price_change < -0.005 else 'NEUTRAL')

                            # ë°©í–¥ ì˜ˆì¸¡ ì •í™•ë„ ë³´ìƒ (ì˜ˆì¸¡ ì „ëµì˜ í•µì‹¬)
                            if predicted_direction == actual_direction:
                                # ì˜ˆì¸¡ ì •í™•ë„: ë°©í–¥ ë§ì¶¤
                                if predicted_direction == 'UP':
                                    direction_reward = 1.0  # ìƒìŠ¹ ì˜ˆì¸¡ ë§ì¶¤
                                elif predicted_direction == 'DOWN':
                                    direction_reward = 1.0  # í•˜ë½ ì˜ˆì¸¡ ë§ì¶¤
                                else:  # NEUTRAL
                                    direction_reward = 0.7  # ì¤‘ë¦½ ì˜ˆì¸¡ ë§ì¶¤ (ë³´ìˆ˜ì  ë³´ìƒ)
                            elif (predicted_direction == 'UP' and actual_direction == 'DOWN') or \
                                 (predicted_direction == 'DOWN' and actual_direction == 'UP'):
                                # ì˜ˆì¸¡ ì •ë°˜ëŒ€: í° í˜ë„í‹°
                                direction_reward = -1.0
                            else:
                                # ì˜ˆì¸¡ ë¶€ë¶„ ì˜¤ë¥˜ (UP/DOWN â†” NEUTRAL)
                                direction_reward = -0.3

                            # ì˜ˆì¸¡ ì‹ ë¢°ë„ ê¸°ë°˜ ë³´ì • (win_rate í™œìš©)
                            confidence_bonus = (win_rate - 0.5) * 0.5  # -0.25 ~ +0.25

                            # ìµœì¢… ë³´ìƒ: ì˜ˆì¸¡ ì •í™•ë„ + ì‹ ë¢°ë„ ë³´ë„ˆìŠ¤
                            reward = direction_reward + confidence_bonus

                            # ğŸ”¥ ì˜ˆì¸¡ í™œì„±í™” ë³´ë„ˆìŠ¤ (NEUTRALë§Œ í•˜ì§€ ì•Šë„ë¡)
                            # UP/DOWN ì˜ˆì¸¡ì€ ë” ë§ì€ ì •ë³´ë¥¼ ì œê³µí•˜ë¯€ë¡œ ë³´ë„ˆìŠ¤
                            if predicted_direction != 'NEUTRAL':
                                reward += 0.1  # ë°©í–¥ ì˜ˆì¸¡ ì‹œë„ì— ì‘ì€ ë³´ë„ˆìŠ¤
                            
                            # ê¸°ë³¸ log_prob
                            log_prob = -1.1
                            
                            # ê¸°ë³¸ value estimate
                            value = reward * 0.9
                            
                        experience = {
                            'episode': episode_num,
                            'agent_id': agent_id,
                            'state': enhanced_state_vec,  # ğŸ”¥ 25ì°¨ì› ë²¡í„°
                            'action': action,
                            'reward': reward,
                            'log_prob': log_prob,
                            'value': value,
                            'done': False
                        }
                        experiences.append(experience)
                        
                        # ğŸ”¥ UP/DOWN ì˜ˆì¸¡ tradesê°€ ìˆìœ¼ë©´ ë” ì´ìƒ ì¶”ê°€í•˜ì§€ ì•ŠìŒ (ì˜ˆì¸¡ ë‹¤ì–‘ì„± í™•ë³´)
                        break
                    
                    # ğŸ”¥ tradesê°€ ì—†ëŠ” ê²½ìš°: total_tradesê°€ ìˆìœ¼ë©´ ê²½í—˜ ìƒì„±
                    elif total_trades > 0:
                        # total_tradesëŠ” ìˆì§€ë§Œ trades ë¦¬ìŠ¤íŠ¸ëŠ” ì—†ëŠ” ê²½ìš°
                        # (ì˜ˆì¸¡ self-playì—ì„œ predictionsë¥¼ tradesë¡œ ë³€í™˜í–ˆì§€ë§Œ ì‹¤ì œ ê±°ë˜ëŠ” ì—†ìŒ)
                        # ğŸ”¥ ì „ëµ ë°©í–¥ì„±ê³¼ ì˜ˆì¸¡ ì‹ ë¢°ë„ë¥¼ í™œìš©í•˜ì—¬ ë‹¤ì–‘í•œ ì•¡ì…˜ ìƒì„±
                        # ğŸ”¥ í•©ì„± ë°ì´í„° ìƒì„±ëŸ‰ ì¦ê°€: ìµœì†Œ 5ê°œ ë³´ì¥, ìµœëŒ€ 20ê°œ (í•™ìŠµ ë°ì´í„° ì¦ê°€)
                        num_experiences = max(5, min(total_trades, 20))

                        # ğŸ”¥ ì „ëµ ë°©í–¥ì„± ê¸°ë°˜ ì•¡ì…˜ ë¶„í¬ ìƒì„± (ë‹¤ì–‘ì„± í™•ë³´ - ê° ë°©í–¥ ìµœì†Œ 1ê°œ ë³´ì¥)
                        # ê° ì•¡ì…˜ì„ ì•ì— ë°°ì¹˜í•˜ì—¬ num_experiencesê°€ ì‘ì•„ë„ ëª¨ë“  ì•¡ì…˜ í¬í•¨ ë³´ì¥
                        if strategy_direction == 'buy':
                            # ë§¤ìˆ˜ ì „ëµ: UP ìš°ì„¸, DOWN/NEUTRAL í¬í•¨ (ë‹¤ì–‘ì„±)
                            # ìµœì†Œ ê° ë°©í–¥ 1ê°œì”© ë¨¼ì € ë³´ì¥ (ì• 3ê°œ), ë‚˜ë¨¸ì§€ UP ìœ„ì£¼ë¡œ ì±„ì›€
                            action_distribution = [0, 1, 2] + [1] * 4 + [2] * 2 + [1]  # NEUTRAL, UP, DOWN ê° 1ê°œ ë¨¼ì €, ë‚˜ë¨¸ì§€ UP ìœ„ì£¼
                        elif strategy_direction == 'sell':
                            # ë§¤ë„ ì „ëµ: DOWN ìš°ì„¸, UP/NEUTRAL í¬í•¨ (ë‹¤ì–‘ì„±)
                            # ìµœì†Œ ê° ë°©í–¥ 1ê°œì”© ë¨¼ì € ë³´ì¥ (ì• 3ê°œ), ë‚˜ë¨¸ì§€ DOWN ìœ„ì£¼ë¡œ ì±„ì›€
                            action_distribution = [0, 1, 2] + [2] * 4 + [1] * 2 + [2]  # NEUTRAL, UP, DOWN ê° 1ê°œ ë¨¼ì €, ë‚˜ë¨¸ì§€ DOWN ìœ„ì£¼
                        else:
                            # ì¤‘ë¦½ ì „ëµ: ê· í˜•ì¡íŒ ë¶„í¬ (ê° ë°©í–¥ ìµœì†Œ 1ê°œ ë³´ì¥)
                            action_distribution = [0, 1, 2] + [0] * 2 + [1] * 2 + [2] * 2 + [0, 1, 2]  # ê· ë“± ë¶„í¬
                        
                        for exp_idx in range(num_experiences):
                            state = {
                                'rsi': 50.0, 'macd': 0.0, 'volume_ratio': 1.0, 'atr': 0.02,
                                'adx': 25.0, 'mfi': 50.0, 'bb_upper': 1.0, 'bb_middle': 1.0,
                                'bb_lower': 1.0, 'macd_signal': 0.0, 'close': 1.0, 'open': 1.0,
                                'high': 1.0, 'low': 1.0, 'volume': 1.0, 'volatility': 0.02,
                                'regime_stage': 3, 'regime_confidence': 0.5,
                                # ğŸš€ í™•ì¥ ì§€í‘œ ì¶”ê°€
                                'wave_progress': 0.5, 'pattern_confidence': predicted_conf,
                                'structure_score': 0.5, 'sentiment': 0.0, 'regime_transition_prob': 0.05
                            }

                            # ğŸš€ ë©”íƒ€ í•™ìŠµ: ë¶„ì„+ì „ëµ íŒŒë¼ë¯¸í„° í¬í•¨ ìƒíƒœ ë²¡í„° ìƒì„± (35ì°¨ì›)
                            enhanced_state_vec = build_state_vector_with_analysis_and_strategy(
                                state,
                                strategy_params,
                                fractal_score=fractal_score,
                                multi_timeframe_score=multi_timeframe_score,
                                indicator_cross_score=indicator_cross_score,
                                ensemble_score=ensemble_score,
                                ensemble_confidence=ensemble_confidence
                            )

                            # ğŸ”¥ ì•¡ì…˜ ë¶„í¬ì—ì„œ ì„ íƒ (ë‹¤ì–‘ì„± í™•ë³´)
                            action = action_distribution[exp_idx % len(action_distribution)]
                            
                            # ğŸ”¥ ì˜ˆì¸¡ ì‹ ë¢°ë„ ê¸°ë°˜ ë³´ìƒ
                            base_reward = win_rate - 0.5  # -0.5 ~ 0.5
                            confidence_bonus = (predicted_conf - 0.5) * 0.3  # -0.15 ~ 0.15
                            
                            # ì•¡ì…˜ê³¼ ì „ëµ ë°©í–¥ ì¼ì¹˜ ì—¬ë¶€ì— ë”°ë¥¸ ë³´ì •
                            if (action == 1 and strategy_direction == 'buy') or \
                               (action == 2 and strategy_direction == 'sell') or \
                               (action == 0 and strategy_direction == 'neutral'):
                                direction_bonus = 0.1  # ë°©í–¥ ì¼ì¹˜ ë³´ë„ˆìŠ¤
                            else:
                                direction_bonus = -0.05  # ë°©í–¥ ë¶ˆì¼ì¹˜ ì‘ì€ í˜ë„í‹°
                            
                            reward = base_reward + confidence_bonus + direction_bonus
                            
                            experience = {
                                'episode': episode_num,
                                'agent_id': agent_id,
                                'state': enhanced_state_vec,  # ğŸ”¥ 25ì°¨ì› ë²¡í„°
                                'action': action,
                                'reward': reward,
                                'log_prob': -1.1,
                                'value': reward * 0.9,
                                'done': True
                            }
                            experiences.append(experience)
                        
                        break
                    
                    else:
                        # ğŸ”¥ ê±°ë˜ê°€ ì—†ëŠ” ê²½ìš°: ì „ëµ ë°©í–¥ì„±ê³¼ ì‹œì¥ ìƒíƒœ ê¸°ë°˜ ì˜ˆì¸¡ ê²½í—˜ ìƒì„±
                        market_info = agent_result.get('market_info', {})
                        regime = agent_result.get('regime', 'neutral')
                        
                        # ğŸ”¥ ì „ëµ ë°©í–¥ì„±ê³¼ ë ˆì§ì„ ì¢…í•©í•˜ì—¬ ì˜ˆì¸¡ ë°©í–¥ ì¶”ì • (ë‹¤ì–‘ì„± í™•ë³´ - ê° ë°©í–¥ ìµœì†Œ 1ê°œ ë³´ì¥)
                        # ê° ì•¡ì…˜ì„ ì•ì— ë°°ì¹˜í•˜ì—¬ num_experiencesê°€ ì‘ì•„ë„ ëª¨ë“  ì•¡ì…˜ í¬í•¨ ë³´ì¥
                        if strategy_direction == 'buy':
                            # ë§¤ìˆ˜ ì „ëµ: UP ìš°ì„¸, DOWN/NEUTRAL í¬í•¨ (ê° ë°©í–¥ ìµœì†Œ 1ê°œ ë³´ì¥)
                            if 'bull' in regime.lower():
                                predicted_actions = [0, 1, 2] + [1] * 4 + [2] * 2 + [1]  # UP ìœ„ì£¼
                            elif 'bear' in regime.lower():
                                predicted_actions = [0, 1, 2] + [1] * 2 + [2] * 3 + [1]  # DOWN í˜¼í•©
                            else:
                                predicted_actions = [0, 1, 2] + [1] * 3 + [2] * 2 + [1, 2]  # ê· í˜•
                        elif strategy_direction == 'sell':
                            # ë§¤ë„ ì „ëµ: DOWN ìš°ì„¸, UP/NEUTRAL í¬í•¨ (ê° ë°©í–¥ ìµœì†Œ 1ê°œ ë³´ì¥)
                            if 'bear' in regime.lower():
                                predicted_actions = [0, 1, 2] + [2] * 4 + [1] * 2 + [2]  # DOWN ìœ„ì£¼
                            elif 'bull' in regime.lower():
                                predicted_actions = [0, 1, 2] + [2] * 2 + [1] * 3 + [2]  # UP í˜¼í•©
                            else:
                                predicted_actions = [0, 1, 2] + [2] * 3 + [1] * 2 + [2, 1]  # ê· í˜•
                        else:
                            # ì¤‘ë¦½ ì „ëµ: ê· í˜•ì¡íŒ ë¶„í¬ (ê° ë°©í–¥ ìµœì†Œ 1ê°œ ë³´ì¥)
                            if 'bull' in regime.lower():
                                predicted_actions = [0, 1, 2] + [1] * 3 + [0, 2] * 2 + [1]  # UP ìœ„ì£¼
                            elif 'bear' in regime.lower():
                                predicted_actions = [0, 1, 2] + [2] * 3 + [0, 1] * 2 + [2]  # DOWN ìœ„ì£¼
                            else:
                                predicted_actions = [0, 1, 2] + [0] * 2 + [1] * 2 + [2] * 2 + [0]  # ê· ë“±
                        
                        # ğŸ”¥ ê° ì˜ˆì¸¡ ë°©í–¥ë³„ë¡œ ê²½í—˜ ìƒì„± (ìµœì†Œ 5ê°œ, ìµœëŒ€ 20ê°œ, í•™ìŠµ ë°ì´í„° ì¦ê°€)
                        num_pred_experiences = max(5, min(len(predicted_actions), 20))
                        for action in predicted_actions[:num_pred_experiences]:
                            state = {
                                'rsi': market_info.get('rsi', 50.0),
                                'macd': market_info.get('macd', 0.0),
                                'volume_ratio': market_info.get('volume_ratio', 1.0),
                                'atr': market_info.get('atr', 0.02),
                                'adx': market_info.get('adx', 25.0),
                                'mfi': market_info.get('mfi', 50.0),
                                'bb_upper': market_info.get('bb_upper', 1.0),
                                'bb_middle': market_info.get('bb_middle', 1.0),
                                'bb_lower': market_info.get('bb_lower', 1.0),
                                'macd_signal': market_info.get('macd_signal', 0.0),
                                'close': market_info.get('close', 1.0),
                                'open': market_info.get('open', 1.0),
                                'high': market_info.get('high', 1.0),
                                'low': market_info.get('low', 1.0),
                                'volume': market_info.get('volume', 1.0),
                                'volatility': market_info.get('volatility', 0.02),
                                'regime_stage': market_info.get('regime_stage', 3),
                                'regime_confidence': market_info.get('regime_confidence', 0.5),
                                # ğŸš€ í™•ì¥ ì§€í‘œ ì¶”ê°€
                                'wave_progress': market_info.get('wave_progress', 0.5),
                                'pattern_confidence': market_info.get('pattern_confidence', predicted_conf),
                                'structure_score': market_info.get('structure_score', 0.5),
                                'sentiment': market_info.get('sentiment', 0.0),
                                'regime_transition_prob': market_info.get('regime_transition_prob', 0.05)
                            }

                            # ğŸš€ ë©”íƒ€ í•™ìŠµ: ë¶„ì„+ì „ëµ íŒŒë¼ë¯¸í„° í¬í•¨ ìƒíƒœ ë²¡í„° ìƒì„± (35ì°¨ì›)
                            enhanced_state_vec = build_state_vector_with_analysis_and_strategy(
                                state,
                                strategy_params,
                                fractal_score=fractal_score,
                                multi_timeframe_score=multi_timeframe_score,
                                indicator_cross_score=indicator_cross_score,
                                ensemble_score=ensemble_score,
                                ensemble_confidence=ensemble_confidence
                            )

                            # ğŸ”¥ ì˜ˆì¸¡ ì „ëµ: ì „ëµ ë°©í–¥ì„±, ë ˆì§, ì˜ˆì¸¡ ì‹ ë¢°ë„ ì¢…í•© ë³´ìƒ
                            base_reward = 0.0
                            
                            # ì „ëµ ë°©í–¥ê³¼ ì•¡ì…˜ ì¼ì¹˜ ì—¬ë¶€
                            if (action == 1 and strategy_direction == 'buy') or \
                               (action == 2 and strategy_direction == 'sell') or \
                               (action == 0 and strategy_direction == 'neutral'):
                                base_reward += 0.1
                            
                            # ë ˆì§ê³¼ ì•¡ì…˜ ì¼ì¹˜ ì—¬ë¶€
                            if (action == 1 and 'bull' in regime.lower()) or \
                               (action == 2 and 'bear' in regime.lower()) or \
                               (action == 0 and ('sideways' in regime.lower() or 'neutral' in regime.lower())):
                                base_reward += 0.1
                            else:
                                base_reward -= 0.05
                            
                            # ì˜ˆì¸¡ ì‹ ë¢°ë„ ë³´ì •
                            confidence_bonus = (predicted_conf - 0.5) * 0.2
                            
                            reward = base_reward + confidence_bonus
                            
                            experience = {
                                'episode': episode_num,
                                'agent_id': agent_id,
                                'state': enhanced_state_vec,  # ğŸ”¥ 25ì°¨ì› ë²¡í„°
                                'action': action,
                                'reward': reward,
                                'log_prob': -1.1,
                                'value': reward * 0.9,
                                'done': True
                            }
                            experiences.append(experience)
                        
                        break
        
        except Exception as e:
            logger.warning(f"âš ï¸ ë¶„ì„ ë°ì´í„° í¬í•¨ ê²½í—˜ ì¶”ì¶œ ì¤‘ ì¼ë¶€ ë°ì´í„° ì†ì‹¤: {e}")
            import traceback
            logger.debug(f"ìƒì„¸ ì—ëŸ¬:\n{traceback.format_exc()}")
        
        # ğŸ”¥ ë°ì´í„° ê²€ì¦ ê°•í™”: ì•¡ì…˜ ë‹¤ì–‘ì„± ì²´í¬
        if experiences:
            actions = [exp.get('action', 0) for exp in experiences]
            unique_actions = set(actions)
            action_counts = {action: actions.count(action) for action in unique_actions}
            
            # ì•¡ì…˜ ë¶„í¬ ë¡œê¹…
            logger.info(f"ğŸ“Š ê²½í—˜ ì¶”ì¶œ ì™„ë£Œ (ë¶„ì„ í¬í•¨): ì´ {len(experiences)}ê°œ, ê³ ìœ  ì•¡ì…˜: {len(unique_actions)}ê°œ")
            logger.info(f"   ì•¡ì…˜ ë¶„í¬: NEUTRAL(0)={action_counts.get(0, 0)}, UP(1)={action_counts.get(1, 0)}, DOWN(2)={action_counts.get(2, 0)}")
            
            # ğŸ”¥ ì•¡ì…˜ ë‹¤ì–‘ì„± ê²€ì¦
            if len(unique_actions) < 2:
                logger.warning(f"âš ï¸ ì•¡ì…˜ ë‹¤ì–‘ì„± ë¶€ì¡±: ê³ ìœ  ì•¡ì…˜ {len(unique_actions)}ê°œë§Œ ì¡´ì¬")
                logger.warning(f"   ì•¡ì…˜ ë¶„í¬: {action_counts}")
            elif len(unique_actions) == 2:
                logger.info(f"âœ… ì•¡ì…˜ ë‹¤ì–‘ì„± ì–‘í˜¸: ê³ ìœ  ì•¡ì…˜ {len(unique_actions)}ê°œ")
            else:
                logger.info(f"âœ… ì•¡ì…˜ ë‹¤ì–‘ì„± ìš°ìˆ˜: ê³ ìœ  ì•¡ì…˜ {len(unique_actions)}ê°œ (ëª¨ë“  ë°©í–¥ í¬í•¨)")
        else:
            logger.warning("âš ï¸ ê²½í—˜ ì¶”ì¶œ ê²°ê³¼ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤ (ë¶„ì„ í¬í•¨)")
        
        logger.debug(f"âœ… ì´ {len(experiences)}ê°œ ê²½í—˜ ì¶”ì¶œ ì™„ë£Œ (ë¶„ì„ ë°ì´í„° í¬í•¨)")
        return experiences
    
    def _create_batches(self, experiences: List[Dict], batch_size: int) -> List[List[Dict]]:
        """ê²½í—˜ì„ ë°°ì¹˜ë¡œ ë¶„í• """
        batches = []
        
        for i in range(0, len(experiences), batch_size):
            batch = experiences[i:i + batch_size]
            batches.append(batch)
        
        return batches
    
    def _update_policy(self, batch: List[Dict]) -> float:
        """
        PPO ì •ì±… ì—…ë°ì´íŠ¸ (ì‹¤ì œ í•™ìŠµ ì•Œê³ ë¦¬ì¦˜)
        
        Args:
            batch: ê²½í—˜ ë°°ì¹˜ [{state, action, reward, log_prob, value, done}, ...]
        
        Returns:
            í‰ê·  ì†ì‹¤
        """
        if not JAX_AVAILABLE or optax is None:
            logger.error("âŒ JAX ë˜ëŠ” optaxë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return 0.0
        
        try:
            # ë°°ì¹˜ ë°ì´í„° ì¶”ì¶œ ë° ê²€ì¦
            if not batch:
                return 0.0
            
            # í•„ìˆ˜ í•„ë“œ í™•ì¸ ë° ê¸°ë³¸ê°’ ì„¤ì •
            states = []
            actions = []
            rewards = []
            old_log_probs = []
            old_values = []
            
            for exp in batch:
                # State ë²¡í„° ì¶”ì¶œ
                state = exp.get('state')
                if state is None:
                    # agent_idë‚˜ ë‹¤ë¥¸ í•„ë“œì—ì„œ state ì¬êµ¬ì„± ì‹œë„
                    continue
                
                state_vec = build_state_vector(state) if isinstance(state, dict) else np.array(state, dtype=np.float32)
                states.append(state_vec)
                
                # ğŸ”¥ ì˜ˆì¸¡ ì „ëµ: Action (0=NEUTRAL, 1=UP, 2=DOWN)
                action = exp.get('action', exp.get('action_idx', 0))
                actions.append(int(action))
                
                # Reward (ì •ê·œí™” ë° ìŠ¤ì¼€ì¼ë§)
                reward = float(exp.get('reward', 0.0))
                
                # ğŸ”¥ ë³´ìƒ ì •ê·œí™”: ë§¤ìš° ìŒìˆ˜ì¸ ë³´ìƒì„ ì™„í™”
                # ë³´ìƒ ë²”ìœ„ë¥¼ -1.0 ~ 1.0ìœ¼ë¡œ ì •ê·œí™”í•˜ë˜, ì›ë˜ ë¶€í˜¸ ìœ ì§€
                if reward < -1.0:
                    # ê³¼ë„í•œ ìŒìˆ˜ ë³´ìƒì„ -1.0ìœ¼ë¡œ í´ë¦¬í•‘ (í•™ìŠµ ì•ˆì •ì„±)
                    reward = max(-1.0, reward / 10.0)  # -10.0 â†’ -1.0 ìŠ¤ì¼€ì¼ë§
                elif reward > 1.0:
                    # ê³¼ë„í•œ ì–‘ìˆ˜ ë³´ìƒë„ í´ë¦¬í•‘
                    reward = min(1.0, reward)
                
                rewards.append(reward)
                
                # Old log probability (ì—†ìœ¼ë©´ 0ìœ¼ë¡œ ì‹œì‘)
                old_log_prob = float(exp.get('log_prob', exp.get('old_log_prob', -1.1)))  # ê¸°ë³¸ê°’: log(1/3) â‰ˆ -1.1
                old_log_probs.append(old_log_prob)
                
                # Old value estimate
                old_value = float(exp.get('value', exp.get('old_value', 0.0)))
                old_values.append(old_value)
            
            if not states:
                logger.warning("âš ï¸ ë°°ì¹˜ì— ìœ íš¨í•œ stateê°€ ì—†ìŠµë‹ˆë‹¤.")
                return 0.0
            
            # ğŸ”¥ ê°œì„ : ìƒíƒœ ë²¡í„° ë°°ì¹˜ ì •ê·œí™” (í•™ìŠµ ì•ˆì •ì„± í–¥ìƒ)
            states_np = np.array(states, dtype=np.float32)
            states_np = np.nan_to_num(states_np, nan=0.0, posinf=1.0, neginf=-1.0)
            
            # ë°°ì¹˜ ë‹¨ìœ„ Z-score ì •ê·œí™” (ê° í”¼ì²˜ë³„ë¡œ)
            states_mean = np.mean(states_np, axis=0, keepdims=True)
            states_std = np.std(states_np, axis=0, keepdims=True) + 1e-8  # 0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€
            states_normalized = (states_np - states_mean) / states_std
            
            # ì •ê·œí™” í›„ í´ë¦¬í•‘ (ì´ìƒì¹˜ ì œê±°)
            states_normalized = np.clip(states_normalized, -3.0, 3.0)  # Â±3 í‘œì¤€í¸ì°¨ ë²”ìœ„
            
            states_jax = jnp.array(states_normalized, dtype=jnp.float32)
            
            actions_jax = jnp.array(actions, dtype=jnp.int32)
            
            rewards_np = np.array(rewards, dtype=np.float32)
            rewards_np = np.nan_to_num(rewards_np, nan=0.0, posinf=1e6, neginf=-1e6)
            rewards_jax = jnp.array(rewards_np, dtype=jnp.float32)
            
            old_log_probs_np = np.array(old_log_probs, dtype=np.float32)
            old_log_probs_np = np.nan_to_num(old_log_probs_np, nan=-1.1, posinf=10.0, neginf=-10.0)
            old_log_probs_jax = jnp.array(old_log_probs_np, dtype=jnp.float32)
            
            old_values_np = np.array(old_values, dtype=np.float32)
            old_values_np = np.nan_to_num(old_values_np, nan=0.0, posinf=1e6, neginf=-1e6)
            old_values_jax = jnp.array(old_values_np, dtype=jnp.float32)
            
            # PPO í•˜ì´í¼íŒŒë¼ë¯¸í„°
            clip_epsilon = self.train_config.get('clip_epsilon', 0.2)
            value_loss_coef = self.train_config.get('value_loss_coef', 0.5)
            
            # ğŸ”¥ í•™ìŠµ ì„±ëŠ¥ ê°œì„ : íƒí—˜(Exploration) ê°•í™” (ì¬í•™ìŠµ ê¶Œì¥ ë°˜ì˜)
            # ğŸ”¥ ëˆ„ì  ì¦ê°€ ë°©ì‹: ë‹¤ì–‘ì„±ì´ ë¶€ì¡±í•˜ë©´ ì ì§„ì ìœ¼ë¡œ ì¦ê°€, ê°œì„ ë˜ë©´ ì²œì²œíˆ ê°ì†Œ

            # ì•¡ì…˜ ë‹¤ì–‘ì„± ì²´í¬ (HOLDë§Œ ìˆëŠ”ì§€ í™•ì¸)
            unique_actions = len(set(actions))
            action_counts = {action: actions.count(action) for action in set(actions)}
            hold_count = action_counts.get(0, 0)
            hold_ratio = hold_count / len(actions) if actions else 0.0

            # ğŸ”¥ ì˜ˆì¸¡ ì „ëµ: NEUTRALë§Œ ìˆëŠ” ê²½ìš° íƒí—˜ ê°•í™” (ëˆ„ì  ì¦ê°€)
            # NEUTRAL(ì¤‘ë¦½ ì˜ˆì¸¡)ë§Œ í•˜ë©´ ì˜ˆì¸¡ ì •ë³´ê°€ ì—†ìœ¼ë¯€ë¡œ UP/DOWN ì˜ˆì¸¡ì„ ìœ ë„
            # ë” ì ê·¹ì ì¸ íƒí—˜ìœ¼ë¡œ ì•¡ì…˜ ë‹¤ì–‘ì„± í™•ë³´
            if unique_actions == 1:
                # ëª¨ë“  ì•¡ì…˜ì´ ë™ì¼í•˜ë©´ íƒí—˜ì„ í¬ê²Œ ì¦ê°€ (ëˆ„ì )
                self.current_entropy_coef = min(self.current_entropy_coef * 2.0, self.base_entropy_coef * 200.0)
                logger.warning(f"ğŸ” ì˜ˆì¸¡ ë‹¤ì–‘ì„± ì‹¬ê° ë¶€ì¡± (ê³ ìœ  ì•¡ì…˜: {unique_actions}), entropy_coef ëˆ„ì  ì¦ê°€: {self.current_entropy_coef:.4f}")
            elif hold_ratio > 0.95:
                # 95% ì´ìƒì´ NEUTRALì´ë©´ íƒí—˜ì„ í¬ê²Œ ì¦ê°€ (ëˆ„ì )
                self.current_entropy_coef = min(self.current_entropy_coef * 1.8, self.base_entropy_coef * 150.0)
                logger.warning(f"ğŸ” ì˜ˆì¸¡ ë‹¤ì–‘ì„± ì‹¬ê° ë¶€ì¡± (NEUTRAL ë¹„ìœ¨: {hold_ratio:.1%}), entropy_coef ëˆ„ì  ì¦ê°€: {self.current_entropy_coef:.4f}")
            elif hold_ratio > 0.9:
                # 90% ì´ìƒì´ NEUTRALì´ë©´ íƒí—˜ì„ ì¦ê°€ (ëˆ„ì )
                self.current_entropy_coef = min(self.current_entropy_coef * 1.5, self.base_entropy_coef * 75.0)
                logger.info(f"ğŸ” ì˜ˆì¸¡ ë‹¤ì–‘ì„± ë¶€ì¡± ê°ì§€ (NEUTRAL ë¹„ìœ¨: {hold_ratio:.1%}), entropy_coef ëˆ„ì  ì¦ê°€: {self.current_entropy_coef:.4f}")
            elif unique_actions == 2 or hold_ratio > 0.7:
                # 2ì¢…ë¥˜ ì•¡ì…˜ë§Œ ìˆê±°ë‚˜ 70% ì´ìƒì´ NEUTRALì´ë©´ ì¤‘ê°„ ì¦ê°€ (ëˆ„ì )
                self.current_entropy_coef = min(self.current_entropy_coef * 1.3, self.base_entropy_coef * 30.0)
                logger.info(f"ğŸ” ì˜ˆì¸¡ ë‹¤ì–‘ì„± ë¶€ì¡± ê°ì§€ (ê³ ìœ  ì•¡ì…˜: {unique_actions}, NEUTRAL ë¹„ìœ¨: {hold_ratio:.1%}), entropy_coef ëˆ„ì  ì¦ê°€: {self.current_entropy_coef:.4f}")
            elif unique_actions == 3 and hold_ratio > 0.5:
                # 3ì¢…ë¥˜ ëª¨ë‘ ìˆì§€ë§Œ NEUTRALì´ ì ˆë°˜ ì´ìƒì´ë©´ ì•½ê°„ ì¦ê°€ (ëˆ„ì )
                self.current_entropy_coef = min(self.current_entropy_coef * 1.1, self.base_entropy_coef * 8.0)
                logger.debug(f"ğŸ” ì˜ˆì¸¡ ë‹¤ì–‘ì„± ë³´í†µ (ê³ ìœ  ì•¡ì…˜: {unique_actions}, NEUTRAL ë¹„ìœ¨: {hold_ratio:.1%}), entropy_coef ì•½ê°„ ì¦ê°€: {self.current_entropy_coef:.4f}")
            else:
                # ë‹¤ì–‘í•œ ì•¡ì…˜ì´ ìˆìœ¼ë©´ ì²œì²œíˆ ê°ì†Œ (ê¸°ë³¸ê°’ìœ¼ë¡œ ë³µê·€)
                self.current_entropy_coef = max(self.current_entropy_coef * 0.95, self.base_entropy_coef)
                logger.debug(f"âœ… ì˜ˆì¸¡ ë‹¤ì–‘ì„± ì–‘í˜¸ (ê³ ìœ  ì•¡ì…˜: {unique_actions}, NEUTRAL ë¹„ìœ¨: {hold_ratio:.1%}), entropy_coef: {self.current_entropy_coef:.4f}")

            entropy_coef = self.current_entropy_coef
            
            gamma = self.train_config.get('gamma', 0.99)  # í• ì¸ìœ¨
            gae_lambda = self.train_config.get('gae_lambda', 0.95)  # GAE lambda
            
            # ğŸ”¥ í•™ìŠµ ì„±ëŠ¥ ê°œì„ : ë³´ìƒ ì •ê·œí™” ë° Shaping ê°•í™”
            # ëª¨ë“  ë³´ìƒì´ ìŒìˆ˜ì¸ ê²½ìš° í•™ìŠµì´ ì–´ë ¤ìš¸ ìˆ˜ ìˆìœ¼ë¯€ë¡œ, ë³´ìƒì„ ì ê·¹ì ìœ¼ë¡œ ì •ê·œí™”
            rewards_mean = float(jnp.mean(rewards_jax))
            rewards_std = float(jnp.std(rewards_jax))
            rewards_min = float(jnp.min(rewards_jax))
            rewards_max = float(jnp.max(rewards_jax))
            
            # í‘œì¤€í™”ë¥¼ ìœ„í•œ ìµœì†Œ std ê°’ (0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€)
            min_std = 1e-6
            
            # ğŸ”¥ ë³´ìƒ Shaping: ìŒìˆ˜ ë³´ìƒì— ë” ì ê·¹ì ì¸ ì²˜ë¦¬
            if rewards_mean < -0.1:  # í‰ê·  ë³´ìƒì´ ìŒìˆ˜ì¸ ê²½ìš°
                # 1. ë³´ìƒì„ ì–‘ìˆ˜ ì˜ì—­ìœ¼ë¡œ ì´ë™ (ìƒìˆ˜ ì¶”ê°€)
                # ëª©í‘œ: í‰ê· ì„ 0 ê·¼ì²˜ë¡œ ì´ë™ - ë” ì ê·¹ì ìœ¼ë¡œ
                shift_amount = abs(rewards_mean) * 0.8  # í‰ê· ì˜ 80%ë§Œí¼ ì´ë™ (ë” ê°•ë ¥í•˜ê²Œ)
                rewards_jax = rewards_jax + shift_amount
                rewards_mean = float(jnp.mean(rewards_jax))
                rewards_std = float(jnp.std(rewards_jax))  # std ì¬ê³„ì‚°
                logger.info(f"ğŸ” ë³´ìƒ Shaping: ìŒìˆ˜ ë³´ìƒì— {shift_amount:.4f} ì¶”ê°€ (ìƒˆ í‰ê· : {rewards_mean:.4f})")
            
            # ğŸ”¥ ë³´ìƒ ë‹¤ì–‘ì„± ê°•í™”: ì•¡ì…˜ë³„ë¡œ ë‹¤ë¥¸ ë³´ìƒ ë¶€ì—¬
            if rewards_std < min_std:
                # ë³´ìƒì´ ê±°ì˜ ë™ì¼í•˜ë©´ (ëª¨ë‘ HOLDë¡œ ì¸í•œ ìŒìˆ˜ ë³´ìƒ)
                # ë” ì ê·¹ì ì¸ ë³´ìƒ ë‹¤ì–‘ì„± ë¶€ì—¬
                # 1. ì•¡ì…˜ë³„ë¡œ ë‹¤ë¥¸ ë³´ë„ˆìŠ¤/í˜ë„í‹° ì¶”ê°€ (ê°•í™”ë¨)
                actions_jax = jnp.array(actions, dtype=jnp.int32)
                # ğŸ”¥ ê°œì„ : BUY/SELL ë³´ë„ˆìŠ¤ ì¦ê°€ (0.1 â†’ 0.3), HOLDì— í˜ë„í‹° ì¶”ê°€
                action_bonuses = jnp.where(
                    (actions_jax == 1) | (actions_jax == 2),
                    0.3,   # BUY/SELLì— ê°•í™”ëœ ë³´ë„ˆìŠ¤ (0.1 â†’ 0.3)
                    -0.1   # HOLDì— í˜ë„í‹° ì¶”ê°€ (0.0 â†’ -0.1) - ê±°ë˜ ìœ ë„
                )
                rewards_jax = rewards_jax + action_bonuses
                
                # 2. ì¶”ê°€ ì–‘ìˆ˜ ë³´ë„ˆìŠ¤ (ì „ì²´ì—) - ê°•í™”
                bonus = max(0.2, abs(rewards_mean) * 0.5)  # ğŸ”¥ ê°œì„ : ìµœì†Œ 0.2, í‰ê· ì˜ 50% (ì´ì „: 0.1, 30%)
                rewards_jax = rewards_jax + bonus

                # 3. ëœë¤ ë…¸ì´ì¦ˆ ì¶”ê°€ (ë³´ìƒ ë‹¤ì–‘ì„± ê°•í™”) - ê°•í™”
                rng_key = jax.random.PRNGKey(int(np.random.randint(0, 2**32)))
                noise = jax.random.normal(rng_key, shape=rewards_jax.shape) * 0.1  # ğŸ”¥ ê°œì„ : 10% ë…¸ì´ì¦ˆ (ì´ì „: 5%)
                rewards_jax = rewards_jax + noise

                rewards_std = float(jnp.std(rewards_jax))  # std ì¬ê³„ì‚°
                logger.warning(f"ğŸ” ë³´ìƒ ë‹¤ì–‘ì„± ì‹¬ê° ë¶€ì¡± (std={rewards_std:.6f}), ì•¡ì…˜ë³„ ë³´ë„ˆìŠ¤/í˜ë„í‹° + ì „ì²´ ë³´ë„ˆìŠ¤ {bonus:.4f} + ë…¸ì´ì¦ˆ ì¶”ê°€")
            else:
                # ë³´ìƒ ì •ê·œí™” (Z-score ì •ê·œí™” í›„ tanhë¡œ ìŠ¤ì¼€ì¼ë§)
                rewards_normalized = (rewards_jax - rewards_mean) / (rewards_std + min_std)
                # -3 ~ +3 ë²”ìœ„ë¥¼ -1 ~ +1ë¡œ ìŠ¤ì¼€ì¼ë§ (ë” ë¶€ë“œëŸ¬ìš´ ìŠ¤ì¼€ì¼ë§)
                rewards_jax = jnp.tanh(rewards_normalized * 0.5)
                logger.debug(f"ğŸ” ë³´ìƒ ì •ê·œí™” ì™„ë£Œ (mean={rewards_mean:.4f}, std={rewards_std:.4f})")
            
            # ìµœì¢… í´ë¦¬í•‘ (ì•ˆì „ì¥ì¹˜) - ë” ë„“ì€ ë²”ìœ„ í—ˆìš©
            rewards_jax = jnp.clip(rewards_jax, -2.0, 2.0)  # -1.0 ~ 1.0 â†’ -2.0 ~ 2.0
            
            # ìµœì¢… ë³´ìƒ í†µê³„ ë¡œê¹…
            final_mean = float(jnp.mean(rewards_jax))
            final_std = float(jnp.std(rewards_jax))
            logger.debug(f"ğŸ“Š ìµœì¢… ë³´ìƒ í†µê³„: mean={final_mean:.4f}, std={final_std:.4f}, range=[{rewards_min:.4f}, {rewards_max:.4f}]")
            
            # ğŸ”§ GAE (Generalized Advantage Estimation) êµ¬í˜„
            # 1. Discounted returns ê³„ì‚°
            returns = _compute_returns(rewards_jax, gamma)
            
            # returnsì™€ old_valuesì˜ shape ì¼ì¹˜ í™•ì¸
            if returns.shape != old_values_jax.shape:
                logger.warning(f"âš ï¸ Returnsì™€ Values shape ë¶ˆì¼ì¹˜: returns={returns.shape}, values={old_values_jax.shape}")
                # shape ë§ì¶”ê¸°
                if old_values_jax.ndim == 0:
                    old_values_jax = old_values_jax.reshape(-1)
                if returns.ndim == 0:
                    returns = returns.reshape(-1)
                min_len = min(len(returns), len(old_values_jax))
                returns = returns[:min_len]
                old_values_jax = old_values_jax[:min_len]
            
            # 2. GAE ê³„ì‚° (ë” ì •í™•í•œ advantage ì¶”ì •)
            advantages = _compute_gae(
                rewards=rewards_jax[:min_len] if 'min_len' in locals() else rewards_jax,
                values=old_values_jax,
                gamma=gamma,
                lam=gae_lambda
            )
            
            # advantagesì™€ returnsì˜ shape ì¼ì¹˜ í™•ì¸
            if advantages.shape != returns.shape:
                logger.warning(f"âš ï¸ Advantagesì™€ Returns shape ë¶ˆì¼ì¹˜: advantages={advantages.shape}, returns={returns.shape}")
                min_adv_len = min(len(advantages), len(returns))
                advantages = advantages[:min_adv_len]
                returns = returns[:min_adv_len]
                old_log_probs_jax = old_log_probs_jax[:min_adv_len]
                actions_jax = actions_jax[:min_adv_len]
                states_jax = states_jax[:min_adv_len]
            
            # Advantage ì •ê·œí™” (ì•ˆì „í•œ ë°©ì‹)
            try:
                advantages_mean = jnp.mean(advantages)
                advantages_std = jnp.std(advantages)
                
                # stdê°€ ë„ˆë¬´ ì‘ìœ¼ë©´ ì •ê·œí™” ìŠ¤í‚µ
                if advantages_std > 1e-6:
                    advantages_normalized = (advantages - advantages_mean) / (advantages_std + 1e-8)
                    # í´ë¦¬í•‘ (ê³¼ë„í•œ ê°’ ë°©ì§€)
                    advantages_normalized = jnp.clip(advantages_normalized, -10.0, 10.0)
                else:
                    # stdê°€ ë„ˆë¬´ ì‘ìœ¼ë©´ ì •ê·œí™” ì—†ì´ ì‚¬ìš©
                    advantages_normalized = advantages - advantages_mean
                    advantages_normalized = jnp.clip(advantages_normalized, -10.0, 10.0)
            except Exception as norm_err:
                logger.warning(f"âš ï¸ Advantage ì •ê·œí™” ì‹¤íŒ¨, ì›ë³¸ ì‚¬ìš©: {norm_err}")
                advantages_normalized = jnp.clip(advantages, -10.0, 10.0)
            
            # ğŸ”¥ ë°°ì¹˜ í¬ê¸° ì‚¬ì „ ì¡°ì • (loss_fn ì •ì˜ ì „ì— ìˆ˜í–‰)
            # í´ë¡œì € ë³€ìˆ˜ ìº¡ì²˜ë¥¼ ìœ„í•´ ì¡°ê±´ë¶€ ì¬í• ë‹¹ ì œê±°
            actual_batch_size = states_jax.shape[0] if states_jax.ndim > 0 else 0
            max_safe_batch = 256  # ì•ˆì „í•œ ë°°ì¹˜ í¬ê¸°
            if actual_batch_size > max_safe_batch:
                logger.warning(f"âš ï¸ ë°°ì¹˜ í¬ê¸° ì´ˆê³¼ ({actual_batch_size} > {max_safe_batch}), ì²˜ìŒ {max_safe_batch}ê°œë§Œ ì‚¬ìš©")
                # âœ… ë¬´ì¡°ê±´ ìŠ¬ë¼ì´ì‹±ìœ¼ë¡œ ì¬í• ë‹¹ (í´ë¡œì € ìº¡ì²˜ ë³´ì¥)
                states_jax = states_jax[:max_safe_batch]
                actions_jax = actions_jax[:max_safe_batch]
                old_log_probs_jax = old_log_probs_jax[:max_safe_batch]
                old_values_jax = old_values_jax[:max_safe_batch]
                rewards_jax = rewards_jax[:max_safe_batch]
                advantages_normalized = advantages_normalized[:max_safe_batch]
                returns = returns[:max_safe_batch]
                actual_batch_size = max_safe_batch
            
            # ì†ì‹¤ í•¨ìˆ˜ ì •ì˜ (í´ë¡œì € ë³€ìˆ˜ë“¤ì„ ëª…ì‹œì ìœ¼ë¡œ ìº¡ì²˜)
            # ëª¨ë“  ì™¸ë¶€ ë³€ìˆ˜ë¥¼ loss_fn ì •ì˜ ì‹œì ì—ì„œ ì•ˆì „í•˜ê²Œ ìº¡ì²˜
            def loss_fn(params):
                try:
                    # í˜„ì¬ ì •ì±…ìœ¼ë¡œ forward pass
                    model = self.model['model_def']
                    
                    # ì…ë ¥ shape í™•ì¸ (batch_size, feature_dim)
                    batch_size = states_jax.shape[0]
                    if batch_size == 0:
                        # ë¹ˆ ë°°ì¹˜ ì²˜ë¦¬ (í•™ìŠµ ì¤‘ë‹¨ ë°©ì§€)
                        safe_loss = jnp.array(0.0)
                        return safe_loss, (safe_loss, safe_loss, safe_loss)
                    
                    # ğŸ”§ ì…ë ¥ ë°ì´í„° ê²€ì¦ ë° í´ë¦¬í•‘
                    states_safe = jnp.clip(states_jax, -10.0, 10.0)
                    states_safe = jnp.nan_to_num(states_safe, nan=0.0, posinf=10.0, neginf=-10.0)
                    
                    # ğŸ”§ ì…ë ¥ shape í™•ì¸ (ìµœì†Œ 2D í•„ìš”: batch_size, feature_dim)
                    if states_safe.ndim == 1:
                        states_safe = states_safe.reshape(1, -1)
                    elif states_safe.ndim == 0:
                        logger.warning(f"âš ï¸ States shape ì´ìƒ: {states_safe.shape}, ë¹ˆ ë°°ì¹˜ ë°˜í™˜")
                        safe_loss = jnp.array(0.0)
                        return safe_loss, (safe_loss, safe_loss, safe_loss)
                    
                    # ğŸ”¥ ì…ë ¥ ë°ì´í„° ì¶”ê°€ ê²€ì¦
                    if not jnp.all(jnp.isfinite(states_safe)):
                        logger.warning(f"âš ï¸ Statesì— NaN/Inf ë°œê²¬, ì œê±° í›„ ì¬ê²€ì¦")
                        states_safe = jnp.nan_to_num(states_safe, nan=0.0, posinf=10.0, neginf=-10.0)
                    
                    # Feature ì°¨ì› ê²€ì¦
                    expected_feature_dim = self.model.get('obs_dim', 25)
                    if states_safe.shape[-1] != expected_feature_dim:
                        logger.error(f"âŒ Feature ì°¨ì› ë¶ˆì¼ì¹˜: {states_safe.shape[-1]} != {expected_feature_dim}")
                        safe_loss = jnp.array(0.0)
                        return safe_loss, (safe_loss, safe_loss, safe_loss)
                    
                    # ğŸ”§ params í˜•ì‹ ê²€ì¦ ë° ì •ê·œí™”
                    # Flax model.init()ì€ {"params": {...}} êµ¬ì¡°ë¥¼ ë°˜í™˜í•¨
                    # self.model['params']ëŠ” ì´ë¯¸ {"params": {...}} êµ¬ì¡°
                    # loss_fnì—ì„œ ë°›ëŠ” paramsë„ ë™ì¼í•œ êµ¬ì¡°ì´ë¯€ë¡œ, ì¤‘ë³µ ë˜í•‘ ë°©ì§€
                    if not isinstance(params, dict):
                        logger.warning(f"âš ï¸ Params í˜•ì‹ ì´ìƒ: {type(params)}, dictë¡œ ë³€í™˜ ì‹œë„")
                        try:
                            # JAX/FrozenDictë¥¼ dictë¡œ ë³€í™˜ ì‹œë„
                            if hasattr(params, '__dict__'):
                                params = dict(params)
                            else:
                                params = {'params': params}
                        except:
                            logger.error(f"âŒ Params ë³€í™˜ ì‹¤íŒ¨: {type(params)}")
                            safe_loss = jnp.array(0.0)
                            return safe_loss, (safe_loss, safe_loss, safe_loss)
                    
                    # ğŸ”§ params êµ¬ì¡° í™•ì¸ ë° ì •ê·œí™”
                    # model.init()ì€ {"params": {...}} ë°˜í™˜
                    # ì²´í¬í¬ì¸íŠ¸ì—ì„œ ë¡œë“œí•œ ê²½ìš°ë„ ë™ì¼ êµ¬ì¡°
                    # ë”°ë¼ì„œ ì´ë¯¸ ì˜¬ë°”ë¥¸ êµ¬ì¡°ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
                    if 'params' in params:
                        # ì´ë¯¸ {"params": {...}} êµ¬ì¡°ì¸ ê²½ìš° ê·¸ëŒ€ë¡œ ì‚¬ìš©
                        variables = params
                    else:
                        # íŒŒë¼ë¯¸í„° ë”•ì…”ë„ˆë¦¬ë§Œ ìˆëŠ” ê²½ìš° {"params": params}ë¡œ ë˜í•‘
                        variables = {'params': params}
                    
                    # ğŸ”¥ ëª¨ë¸ íŒŒë¼ë¯¸í„° ê²€ì¦ (NaN/Inf ì²´í¬)
                    try:
                        # Flax FrozenDictë¥¼ í™•ì¸í•˜ê³  ê²€ì¦
                        # ëª¨ë“ˆ ë ˆë²¨ì˜ jax, jnp ì‚¬ìš© (í•¨ìˆ˜ ë‚´ë¶€ì—ì„œ ë‹¤ì‹œ importí•˜ì§€ ì•ŠìŒ)
                        def check_params_finite(p):
                            """ì¬ê·€ì ìœ¼ë¡œ íŒŒë¼ë¯¸í„° ê²€ì¦"""
                            if isinstance(p, (dict, type(variables))):
                                return all(check_params_finite(v) for v in p.values())
                            elif hasattr(p, 'shape'):
                                # JAX ë°°ì—´ì¸ ê²½ìš°
                                if p.size > 0:
                                    is_finite = jnp.all(jnp.isfinite(p))
                                    if not is_finite:
                                        logger.warning(f"âš ï¸ íŒŒë¼ë¯¸í„°ì— NaN/Inf ë°œê²¬: shape={p.shape}")
                                        return False
                                return True
                            return True
                        
                        if not check_params_finite(variables):
                            logger.warning("âš ï¸ ëª¨ë¸ íŒŒë¼ë¯¸í„°ì— NaN/Inf ë°œê²¬, ì•ˆì „í•œ ê°’ìœ¼ë¡œ ëŒ€ì²´")
                            # íŒŒë¼ë¯¸í„° ì¬ì´ˆê¸°í™” ëŒ€ì‹  ì•ˆì „í•œ ê°’ ì‚¬ìš©
                            safe_loss = jnp.array(0.0)
                            return safe_loss, (safe_loss, safe_loss, safe_loss)
                    except Exception as param_check_err:
                        logger.debug(f"âš ï¸ íŒŒë¼ë¯¸í„° ê²€ì¦ ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œí•˜ê³  ê³„ì†): {param_check_err}")
                    
                    # ğŸ”§ ì•ˆì „í•œ forward pass (Flax ëª¨ë¸ apply ë°©ì‹)
                    try:
                        # variablesëŠ” {"params": {...}} êµ¬ì¡°
                        # ğŸ”¥ Flax ëª¨ë¸ apply í˜¸ì¶œ (mutable íŒŒë¼ë¯¸í„° ì—†ì´, ê¸°ë³¸ê°’ ì‚¬ìš©)
                        # JAX ì»´íŒŒì¼ ì—ëŸ¬ ë°©ì§€ë¥¼ ìœ„í•´ ëª…ì‹œì ìœ¼ë¡œ ë³€ìˆ˜ ê²€ì¦
                        outputs = model.apply(variables, states_safe)
                        
                        # ğŸ†• outputsëŠ” (action_logits, value, price_change, horizon) 4ê°œ ê°’
                        if isinstance(outputs, tuple):
                            if len(outputs) == 4:
                                action_logits, values, price_change_pred, horizon_pred = outputs
                            elif len(outputs) == 2:
                                # ì´ì „ ëª¨ë¸ í˜¸í™˜ì„± (2ê°œ ì¶œë ¥)
                                action_logits, values = outputs
                                price_change_pred = jnp.zeros((states_safe.shape[0], 1))
                                horizon_pred = jnp.ones((states_safe.shape[0], 1)) * 10
                            else:
                                logger.warning(f"âš ï¸ Model ì¶œë ¥ ê°œìˆ˜ ì˜ˆìƒê³¼ ë‹¤ë¦„: {len(outputs)}")
                                safe_loss = jnp.array(0.0)
                                return safe_loss, (safe_loss, safe_loss, safe_loss)
                        else:
                            logger.warning(f"âš ï¸ Model ì¶œë ¥ í˜•íƒœ ì˜ˆìƒê³¼ ë‹¤ë¦„: {type(outputs)}")
                            safe_loss = jnp.array(0.0)
                            return safe_loss, (safe_loss, safe_loss, safe_loss)
                    except Exception as apply_err:
                        logger.warning(f"âš ï¸ Model.apply ì‹¤íŒ¨: {apply_err}")
                        import traceback
                        logger.error(f"Model.apply ìƒì„¸ ì—ëŸ¬:\n{traceback.format_exc()}")
                        # paramsì™€ states_safe í˜•ì‹ ë¡œê¹…
                        logger.error(f"Params type: {type(params)}, States shape: {states_safe.shape}, dtype: {states_safe.dtype}")
                        
                        # ğŸ”¥ ë°°ì¹˜ê°€ ë„ˆë¬´ í¬ë©´ ìë™ìœ¼ë¡œ ë” ì‘ì€ ë°°ì¹˜ë¡œ ë¶„í• í•˜ì—¬ ì¬ì‹œë„
                        # JAX ì»´íŒŒì¼ ì—ëŸ¬ëŠ” ì¢…ì¢… í° ë°°ì¹˜ì—ì„œ ë°œìƒ
                        # XLA ì»´íŒŒì¼ ë¬¸ì œë¡œ ë” ì‘ì€ ì²­í¬ í¬ê¸° ì‚¬ìš©
                        if states_safe.shape[0] > 128:
                            logger.info(f"ğŸ”„ Model.apply ì‹¤íŒ¨, ë°°ì¹˜ ë¶„í•  ì‹œë„: {states_safe.shape[0]} â†’ 128ì”©")
                            # ì‘ì€ ë°°ì¹˜ë¡œ ë‚˜ëˆ„ì–´ ì²˜ë¦¬ (512 â†’ 128ë¡œ ì¶•ì†Œ)
                            chunk_size = 128
                            
                            # ğŸ”¥ ë” ì‘ì€ ì²­í¬ë„ ì‹œë„
                            # 128ë„ ì‹¤íŒ¨í•˜ë©´ 64ë¡œ, 64ë„ ì‹¤íŒ¨í•˜ë©´ 32ë¡œ ì¬ì‹œë„
                            chunk_sizes = [128, 64, 32]
                            success = False
                            
                            for try_chunk_size in chunk_sizes:
                                if states_safe.shape[0] <= try_chunk_size:
                                    continue  # ë°°ì¹˜ê°€ ì´ë¯¸ ì²­í¬ í¬ê¸°ë³´ë‹¤ ì‘ìœ¼ë©´ ìŠ¤í‚µ
                                
                                try:
                                    logger.info(f"  ğŸ”„ {try_chunk_size} í¬ê¸°ë¡œ ì¬ì‹œë„...")
                                    all_action_logits = []
                                    all_values = []
                                    
                                    for chunk_start in range(0, states_safe.shape[0], try_chunk_size):
                                        chunk_end = min(chunk_start + try_chunk_size, states_safe.shape[0])
                                        states_chunk = states_safe[chunk_start:chunk_end]
                                        
                                        # ê° ì²­í¬ì— ëŒ€í•´ forward pass
                                        outputs_chunk = model.apply(variables, states_chunk)

                                        # ğŸ†• 4ê°œ ì¶œë ¥ ì²˜ë¦¬
                                        if isinstance(outputs_chunk, tuple):
                                            if len(outputs_chunk) == 4:
                                                action_logits_chunk, values_chunk, pc_chunk, h_chunk = outputs_chunk
                                            elif len(outputs_chunk) == 2:
                                                action_logits_chunk, values_chunk = outputs_chunk
                                            else:
                                                raise ValueError(f"ì²­í¬ ì¶œë ¥ ê°œìˆ˜ ì˜¤ë¥˜: {len(outputs_chunk)}")
                                            all_action_logits.append(action_logits_chunk)
                                            all_values.append(values_chunk)
                                        else:
                                            raise ValueError(f"ì²­í¬ ì¶œë ¥ í˜•ì‹ ì˜¤ë¥˜: {type(outputs_chunk)}")
                                    
                                    # ëª¨ë“  ì²­í¬ ê²°ê³¼ í•©ì¹˜ê¸°
                                    if all_action_logits and all_values:
                                        action_logits = jnp.concatenate(all_action_logits, axis=0)
                                        values = jnp.concatenate(all_values, axis=0)
                                        logger.info(f"âœ… ë¶„í•  ë°°ì¹˜ ì²˜ë¦¬ ì„±ê³µ: {states_safe.shape[0]} â†’ {len(all_action_logits)}ê°œ ì²­í¬ (ê° {try_chunk_size} í¬ê¸°)")
                                        success = True
                                        break  # ì„±ê³µí•˜ë©´ ë£¨í”„ ì¢…ë£Œ
                                except Exception as chunk_try_err:
                                    logger.debug(f"  âš ï¸ {try_chunk_size} í¬ê¸°ë¡œë„ ì‹¤íŒ¨, ë‹¤ìŒ í¬ê¸° ì‹œë„: {chunk_try_err}")
                                    continue
                            
                            if not success:
                                # ëª¨ë“  ì²­í¬ í¬ê¸° ì‹¤íŒ¨
                                logger.warning(f"âš ï¸ ëª¨ë“  ë°°ì¹˜ ë¶„í•  ì‹œë„ ì‹¤íŒ¨")
                                dummy_loss = jnp.array(0.0)
                                return dummy_loss, (dummy_loss, dummy_loss, dummy_loss)
                            else:
                                # ì„±ê³µí–ˆìœ¼ë¯€ë¡œ ê³„ì† ì§„í–‰
                                pass
                        else:
                            # ì‘ì€ ë°°ì¹˜ë„ ì‹¤íŒ¨í•œ ê²½ìš°, ë” ì‘ì€ ë‹¨ìœ„ë¡œ ì¬ì‹œë„
                            if states_safe.shape[0] > 32:
                                logger.info(f"ğŸ”„ ì‘ì€ ë°°ì¹˜ë„ ì‹¤íŒ¨, ë” ì‘ê²Œ ë¶„í•  ì‹œë„: {states_safe.shape[0]} â†’ 32ì”©")
                                chunk_sizes = [32, 16, 8]
                                success = False
                                
                                for try_chunk_size in chunk_sizes:
                                    if states_safe.shape[0] <= try_chunk_size:
                                        # ë°°ì¹˜ê°€ ì²­í¬ í¬ê¸° ì´í•˜ë©´ ì§ì ‘ ì‹œë„
                                        try:
                                            logger.info(f"  ğŸ”„ {states_safe.shape[0]} í¬ê¸°ë¡œ ì§ì ‘ ì¬ì‹œë„...")
                                            outputs = model.apply(variables, states_safe)
                                            # ğŸ†• 4ê°œ ì¶œë ¥ ì²˜ë¦¬
                                            if isinstance(outputs, tuple):
                                                if len(outputs) == 4:
                                                    action_logits, values, price_change_pred, horizon_pred = outputs
                                                elif len(outputs) == 2:
                                                    action_logits, values = outputs
                                                    price_change_pred = jnp.zeros((states_safe.shape[0], 1))
                                                    horizon_pred = jnp.ones((states_safe.shape[0], 1)) * 10
                                                logger.info(f"âœ… ì§ì ‘ ì¬ì‹œë„ ì„±ê³µ: {states_safe.shape[0]}")
                                                success = True
                                                break
                                        except:
                                            continue
                                    
                                    try:
                                        logger.info(f"  ğŸ”„ {try_chunk_size} í¬ê¸°ë¡œ ë¶„í•  ì¬ì‹œë„...")
                                        all_action_logits = []
                                        all_values = []
                                        
                                        for chunk_start in range(0, states_safe.shape[0], try_chunk_size):
                                            chunk_end = min(chunk_start + try_chunk_size, states_safe.shape[0])
                                            states_chunk = states_safe[chunk_start:chunk_end]
                                            
                                            outputs_chunk = model.apply(variables, states_chunk)

                                            # ğŸ†• 4ê°œ ì¶œë ¥ ì²˜ë¦¬
                                            if isinstance(outputs_chunk, tuple):
                                                if len(outputs_chunk) == 4:
                                                    action_logits_chunk, values_chunk, pc_chunk, h_chunk = outputs_chunk
                                                elif len(outputs_chunk) == 2:
                                                    action_logits_chunk, values_chunk = outputs_chunk
                                                else:
                                                    raise ValueError(f"ì²­í¬ ì¶œë ¥ ê°œìˆ˜ ì˜¤ë¥˜: {len(outputs_chunk)}")
                                                all_action_logits.append(action_logits_chunk)
                                                all_values.append(values_chunk)
                                            else:
                                                raise ValueError(f"ì²­í¬ ì¶œë ¥ í˜•ì‹ ì˜¤ë¥˜: {type(outputs_chunk)}")
                                        
                                        if all_action_logits and all_values:
                                            action_logits = jnp.concatenate(all_action_logits, axis=0)
                                            values = jnp.concatenate(all_values, axis=0)
                                            logger.info(f"âœ… ë¶„í•  ì¬ì‹œë„ ì„±ê³µ: {states_safe.shape[0]} â†’ {len(all_action_logits)}ê°œ ì²­í¬ (ê° {try_chunk_size})")
                                            success = True
                                            break
                                    except Exception as small_chunk_err:
                                        logger.debug(f"  âš ï¸ {try_chunk_size} í¬ê¸°ë¡œë„ ì‹¤íŒ¨: {small_chunk_err}")
                                        continue
                                
                                if not success:
                                    logger.error(f"âŒ ëª¨ë“  ë°°ì¹˜ í¬ê¸° ì‹œë„ ì‹¤íŒ¨ (ìµœì†Œ í¬ê¸°: {states_safe.shape[0]})")
                                    dummy_loss = jnp.array(0.0)
                                    return dummy_loss, (dummy_loss, dummy_loss, dummy_loss)
                            else:
                                # ì´ë¯¸ ë§¤ìš° ì‘ì€ ë°°ì¹˜ (<=32)ë„ ì‹¤íŒ¨
                                logger.error(f"âŒ ë§¤ìš° ì‘ì€ ë°°ì¹˜ ({states_safe.shape[0]})ë„ ì‹¤íŒ¨ - ëª¨ë¸ ë˜ëŠ” íŒŒë¼ë¯¸í„° ë¬¸ì œ ê°€ëŠ¥")
                                dummy_loss = jnp.array(0.0)
                                return dummy_loss, (dummy_loss, dummy_loss, dummy_loss)
                    
                    # ì¶œë ¥ shape í™•ì¸ ë° ê²€ì¦
                    # valuesëŠ” (batch_size, 1) í˜•íƒœì¼ ìˆ˜ ìˆìŒ
                    values_batch_dim = values.shape[0] if values.ndim > 0 else 0
                    if action_logits.shape[0] != batch_size:
                        logger.warning(f"âš ï¸ Action logits shape ë¶ˆì¼ì¹˜: {action_logits.shape}, batch_size={batch_size}")
                        dummy_loss = jnp.array(0.0)
                        return dummy_loss, (dummy_loss, dummy_loss, dummy_loss)
                    
                    # values shape ì •ê·œí™” (2D -> 1D)
                    if values.ndim == 2 and values.shape[1] == 1:
                        # (batch_size, 1) -> (batch_size,)
                        values = values.squeeze(axis=1)
                    elif values.ndim == 2 and values.shape[1] > 1:
                        # (batch_size, value_dim) -> ì²« ë²ˆì§¸ ì°¨ì›ë§Œ ì‚¬ìš©
                        values = values[:, 0]
                    elif values.ndim == 0:
                        # ìŠ¤ì¹¼ë¼ -> ë°°ì¹˜ë¡œ í™•ì¥
                        values = jnp.broadcast_to(values, (batch_size,))
                    
                    if values.shape[0] != batch_size:
                        logger.warning(f"âš ï¸ Values shape ë¶ˆì¼ì¹˜: {values.shape}, batch_size={batch_size}")
                        dummy_loss = jnp.array(0.0)
                        return dummy_loss, (dummy_loss, dummy_loss, dummy_loss)
                    
                    # í˜„ì¬ ì •ì±…ì˜ log probability ê³„ì‚°
                    action_probs = jax.nn.softmax(action_logits)
                    log_probs = jax.nn.log_softmax(action_logits)
                    
                    # ì„ íƒëœ actionì˜ log_prob
                    action_one_hot = jax.nn.one_hot(actions_jax, num_classes=3)
                    new_log_probs = jnp.sum(log_probs * action_one_hot, axis=1)
                    
                    # Ratio ê³„ì‚° (í˜„ì¬ ì •ì±… / ì´ì „ ì •ì±…)
                    ratio = jnp.exp(new_log_probs - old_log_probs_jax)
                    
                    # ğŸ”¥ í•™ìŠµ ì„±ëŠ¥ ê°œì„ : Loss ê³„ì‚° ê°œì„ 
                    # Clipped surrogate objective (ë” ì•ˆì „í•œ í´ë¦¬í•‘)
                    ratio_clipped = jnp.clip(ratio, 1.0 - clip_epsilon, 1.0 + clip_epsilon)
                    surr1 = ratio * advantages_normalized
                    surr2 = ratio_clipped * advantages_normalized
                    policy_loss = -jnp.mean(jnp.minimum(surr1, surr2))
                    
                    # ğŸ”¥ Policy loss ì •ê·œí™” (ë„ˆë¬´ í° loss ë°©ì§€)
                    policy_loss = jnp.clip(policy_loss, -10.0, 10.0)
                    
                    # Value loss (MSE) - valuesëŠ” ì´ë¯¸ 1Dë¡œ ì •ê·œí™”ë¨
                    # returnsì™€ valuesì˜ shape ì¼ì¹˜ í™•ì¸
                    if values.shape != returns.shape:
                        min_val_len = min(len(values), len(returns))
                        values_aligned = values[:min_val_len]
                        returns_aligned = returns[:min_val_len]
                    else:
                        values_aligned = values
                        returns_aligned = returns
                    
                    # ğŸ”¥ Value lossë„ ì •ê·œí™” (Huber loss ê³ ë ¤)
                    value_error = values_aligned - returns_aligned
                    # MSE ëŒ€ì‹  Huber loss ì‚¬ìš© (ì´ìƒì¹˜ì— ëœ ë¯¼ê°)
                    delta = 1.0
                    huber_loss = jnp.where(
                        jnp.abs(value_error) < delta,
                        0.5 * value_error ** 2,
                        delta * (jnp.abs(value_error) - 0.5 * delta)
                    )
                    value_loss = jnp.mean(huber_loss)
                    
                    # ğŸ”¥ Value loss ì •ê·œí™”
                    value_loss = jnp.clip(value_loss, 0.0, 10.0)
                    
                    # Entropy bonus (íƒí—˜ ì¥ë ¤)
                    entropy = -jnp.mean(jnp.sum(action_probs * log_probs, axis=1))

                    # ğŸ”¥ Entropy ì •ê·œí™”
                    entropy = jnp.clip(entropy, 0.0, 10.0)

                    # ğŸ†• Price change loss (MSE) - íšŒê·€ ì˜ˆì¸¡
                    # price_change_pred shape: (batch_size, 1) or (batch_size,)
                    # ì‹¤ì œ ë ˆì´ë¸”ì€ í–¥í›„ orchestratorì—ì„œ ì¶”ê°€ë¨ (í˜„ì¬ëŠ” 0ìœ¼ë¡œ ì´ˆê¸°í™”)
                    # TODO: orchestratorì—ì„œ ì‹¤ì œ price_change ë ˆì´ë¸” ì œê³µ ì‹œ ì‚¬ìš©
                    price_change_target = jnp.zeros_like(price_change_pred)  # ì„ì‹œ íƒ€ê²Ÿ (0%)
                    if price_change_pred.ndim == 2 and price_change_pred.shape[1] == 1:
                        price_change_pred_flat = price_change_pred.squeeze(axis=1)
                    else:
                        price_change_pred_flat = price_change_pred
                    price_change_loss = jnp.mean((price_change_pred_flat - price_change_target.squeeze()) ** 2)
                    price_change_loss = jnp.clip(price_change_loss, 0.0, 1.0)  # í´ë¦¬í•‘ (0~1 ë²”ìœ„)

                    # ğŸ†• Horizon loss (MSE) - íšŒê·€ ì˜ˆì¸¡
                    # horizon_pred shape: (batch_size, 1) or (batch_size,)
                    # ì‹¤ì œ ë ˆì´ë¸”ì€ í–¥í›„ orchestratorì—ì„œ ì¶”ê°€ë¨ (í˜„ì¬ëŠ” 10ìœ¼ë¡œ ì´ˆê¸°í™”)
                    # TODO: orchestratorì—ì„œ ì‹¤ì œ horizon ë ˆì´ë¸” ì œê³µ ì‹œ ì‚¬ìš©
                    horizon_target = jnp.ones_like(horizon_pred) * 10.0  # ì„ì‹œ íƒ€ê²Ÿ (10 ìº”ë“¤)
                    if horizon_pred.ndim == 2 and horizon_pred.shape[1] == 1:
                        horizon_pred_flat = horizon_pred.squeeze(axis=1)
                    else:
                        horizon_pred_flat = horizon_pred
                    horizon_loss = jnp.mean((horizon_pred_flat - horizon_target.squeeze()) ** 2)
                    horizon_loss = jnp.clip(horizon_loss, 0.0, 100.0)  # í´ë¦¬í•‘ (0~100 ë²”ìœ„)

                    # ì´ ì†ì‹¤ (Loss êµ¬ì„± ìš”ì†Œë³„ ê°€ì¤‘ì¹˜ ì¡°ì •)
                    # ğŸ†• íšŒê·€ ì†ì‹¤ ì¶”ê°€ (ì‘ì€ ê°€ì¤‘ì¹˜ë¡œ ì‹œì‘)
                    regression_loss_coef = 0.1  # íšŒê·€ ì†ì‹¤ ê°€ì¤‘ì¹˜ (í–¥í›„ ì¡°ì • ê°€ëŠ¥)
                    total_loss = (
                        policy_loss +
                        value_loss_coef * value_loss -
                        entropy_coef * entropy +
                        regression_loss_coef * price_change_loss +
                        regression_loss_coef * horizon_loss
                    )
                    
                    # ğŸ”¥ Loss ì •ê·œí™” (ê³¼ë„í•œ loss ë°©ì§€)
                    total_loss = jnp.clip(total_loss, -20.0, 20.0)
                    
                    # NaN/Inf ì²´í¬
                    total_loss = jnp.nan_to_num(total_loss, nan=0.0, posinf=1e6, neginf=-1e6)
                    policy_loss = jnp.nan_to_num(policy_loss, nan=0.0, posinf=1e6, neginf=-1e6)
                    value_loss = jnp.nan_to_num(value_loss, nan=0.0, posinf=1e6, neginf=-1e6)
                    entropy = jnp.nan_to_num(entropy, nan=0.0, posinf=1e6, neginf=-1e6)
                    
                    return total_loss, (policy_loss, value_loss, entropy)
                except Exception as loss_err:
                    logger.warning(f"âš ï¸ Loss ê³„ì‚° ì‹¤íŒ¨, 0 ë°˜í™˜: {loss_err}")
                    dummy_loss = jnp.array(0.0)
                    return dummy_loss, (dummy_loss, dummy_loss, dummy_loss)
            
            # Gradient ê³„ì‚° (ì•ˆì „í•œ ë°©ì‹)
            try:
                # ğŸ”§ self.model['params']ê°€ ì´ë¯¸ ì˜¬ë°”ë¥¸ í˜•ì‹ì¸ì§€ í™•ì¸
                model_params = self.model['params']
                # Flax paramsëŠ” FrozenDict ë˜ëŠ” dict í˜•íƒœ
                (loss_value, (policy_loss_val, value_loss_val, entropy_val)), grads = jax.value_and_grad(loss_fn, has_aux=True)(model_params)
                
                # Gradient í´ë¦¬í•‘ (ê³¼ë„í•œ gradient ë°©ì§€)
                # ğŸ”§ JAX ë²„ì „ í˜¸í™˜ì„±: jax.tree_map â†’ jax.tree.map ë˜ëŠ” jax.tree_util.tree_map
                if USE_JAX_TREE and hasattr(jax, 'tree'):
                    # JAX v0.4.25+ ë˜ëŠ” ìµœì‹  ë²„ì „
                    grads = jax.tree.map(lambda g: jnp.clip(g, -1.0, 1.0), grads)
                elif JAX_TREE_UTIL is not None:
                    # êµ¬ë²„ì „ í˜¸í™˜ì„±
                    grads = JAX_TREE_UTIL.tree_map(lambda g: jnp.clip(g, -1.0, 1.0), grads)
                else:
                    # ìµœí›„ì˜ ìˆ˜ë‹¨: ìˆ˜ë™ìœ¼ë¡œ ì²˜ë¦¬
                    grads_clipped = {}
                    for k, v in grads.items():
                        if hasattr(v, '__iter__') and not isinstance(v, (str, bytes)):
                            grads_clipped[k] = jnp.clip(v, -1.0, 1.0)
                        else:
                            grads_clipped[k] = v
                    grads = grads_clipped
                
                # NaN/Inf ì²´í¬
                loss_value = jnp.nan_to_num(loss_value, nan=0.0, posinf=1e6, neginf=-1e6)
                
                # ì˜µí‹°ë§ˆì´ì € ì—…ë°ì´íŠ¸
                updates, self.opt_state = self.optimizer.update(grads, self.opt_state, self.model['params'])
                
                # Updates í´ë¦¬í•‘
                if USE_JAX_TREE and hasattr(jax, 'tree'):
                    # JAX v0.4.25+ ë˜ëŠ” ìµœì‹  ë²„ì „
                    updates = jax.tree.map(lambda u: jnp.clip(u, -0.1, 0.1), updates)
                elif JAX_TREE_UTIL is not None:
                    # êµ¬ë²„ì „ í˜¸í™˜ì„±
                    updates = JAX_TREE_UTIL.tree_map(lambda u: jnp.clip(u, -0.1, 0.1), updates)
                else:
                    # ìˆ˜ë™ ì²˜ë¦¬
                    updates_clipped = {}
                    for k, v in updates.items():
                        if hasattr(v, '__iter__') and not isinstance(v, (str, bytes)):
                            updates_clipped[k] = jnp.clip(v, -0.1, 0.1)
                        else:
                            updates_clipped[k] = v
                    updates = updates_clipped
                
                self.model['params'] = optax.apply_updates(self.model['params'], updates)
                
            except Exception as grad_err:
                logger.error(f"âŒ Gradient ê³„ì‚° ì‹¤íŒ¨: {grad_err}")
                import traceback
                logger.debug(f"Gradient ê³„ì‚° ìƒì„¸ ì—ëŸ¬:\n{traceback.format_exc()}")
                return 0.0
            
            # ì†ì‹¤ê°’ ë°˜í™˜
            loss_float = float(loss_value)
            policy_loss_float = float(policy_loss_val)
            value_loss_float = float(value_loss_val)
            entropy_float = float(entropy_val)

            # í•™ìŠµ ì§„í–‰ ìƒí™© ë¡œê¹… (ë” ìì£¼)
            if np.random.random() < 0.2:  # 20% í™•ë¥ ë¡œ ë¡œê·¸
                logger.debug(f"ğŸ“Š Loss: total={loss_float:.4f}, policy={policy_loss_float:.4f}, "
                          f"value={value_loss_float:.4f}, entropy={entropy_float:.4f}")

            # ğŸ”¥ ë””ë²„ê±° ë¡œê¹…: ë°°ì¹˜ í•™ìŠµ ìƒì„¸ ì •ë³´
            if self.debug:
                try:
                    # action_probs ê³„ì‚° (forward passì—ì„œ ì´ë¯¸ ê³„ì‚°ë¨)
                    # loss_fn ë‚´ì—ì„œ ê³„ì‚°í–ˆìœ¼ë¯€ë¡œ ë‹¤ì‹œ ê³„ì‚°í•´ì•¼ í•¨
                    if JAX_AVAILABLE:
                        model = self.model['model_def']
                        variables = self.model['params']
                        outputs = model.apply(variables, states_jax)
                        if isinstance(outputs, tuple):
                            action_logits, _ = outputs
                            action_probs = jax.nn.softmax(action_logits)
                        else:
                            action_probs = None
                    else:
                        action_probs = None

                    # KL divergence ê³„ì‚° (old vs new policy)
                    if action_probs is not None and JAX_AVAILABLE:
                        log_probs_new = jax.nn.log_softmax(action_logits)
                        # one-hot ì¸ì½”ë”©
                        action_one_hot = jax.nn.one_hot(actions_jax, num_classes=3)
                        new_log_probs = jnp.sum(log_probs_new * action_one_hot, axis=1)
                        # KL divergence: E[log(new) - log(old)]
                        kl_div = float(jnp.mean(new_log_probs - old_log_probs_jax))
                    else:
                        kl_div = 0.0

                    # í˜„ì¬ ë°°ì¹˜ ì¸ë±ìŠ¤ ì¶”ì  (ì—†ìœ¼ë©´ 0)
                    if not hasattr(self, '_debug_batch_idx'):
                        self._debug_batch_idx = 0
                    self._debug_batch_idx += 1

                    # í˜„ì¬ epoch ì¶”ì  (ì—†ìœ¼ë©´ 1)
                    current_epoch = getattr(self, '_debug_current_epoch', 1)

                    self.debug.log_batch_training(
                        epoch=current_epoch,
                        batch_idx=self._debug_batch_idx,
                        total_batches=getattr(self, '_debug_total_batches', 1),
                        loss=loss_float,
                        policy_loss=policy_loss_float,
                        value_loss=value_loss_float,
                        entropy_loss=entropy_float,
                        actions=actions,  # ì›ë³¸ actions ë¦¬ìŠ¤íŠ¸
                        action_probs=np.array(action_probs) if action_probs is not None else None,
                        entropy_coef=entropy_coef,
                        clip_ratio=clip_epsilon,
                        kl_divergence=kl_div
                    )

                    # ê·¸ë˜ë””ì–¸íŠ¸ í†µê³„ ë¡œê¹… (ì²« ë°°ì¹˜ë§Œ)
                    if self._debug_batch_idx == 1:
                        # gradsëŠ” FrozenDict í˜•íƒœ
                        grad_dict = {}
                        if 'params' in grads:
                            for layer_name, layer_params in grads['params'].items():
                                if hasattr(layer_params, 'items'):
                                    for param_name, param_grad in layer_params.items():
                                        full_name = f"{layer_name}_{param_name}"
                                        grad_dict[full_name] = np.array(param_grad) if hasattr(param_grad, 'shape') else param_grad

                        # Gradient norm ê³„ì‚°
                        grad_norm = 0.0
                        for grad_arr in grad_dict.values():
                            if hasattr(grad_arr, 'flatten'):
                                grad_norm += float(np.sum(grad_arr.flatten() ** 2))
                        grad_norm = np.sqrt(grad_norm)

                        self.debug.log_gradient_update(
                            epoch=current_epoch,
                            batch_idx=self._debug_batch_idx,
                            gradients=grad_dict,
                            learning_rate=self.current_learning_rate,
                            grad_norm=grad_norm,
                            clipped=grad_norm > 0.5  # clip_by_global_norm(0.5) ì‚¬ìš©
                        )
                except Exception as debug_err:
                    logger.debug(f"âš ï¸ ë°°ì¹˜ ë””ë²„ê·¸ ë¡œê¹… ì‹¤íŒ¨ (ë¬´ì‹œ): {debug_err}")

            # í•™ìŠµ ë°ì´í„° ê²€ì¦ ë¡œê¹… (ì²« ë°°ì¹˜ë§Œ) - ë””ë²„ê±°ë¡œ ëŒ€ì²´
            if not hasattr(self, '_first_batch_logged'):
                actual_batch_size = states_jax.shape[0] if len(states_jax.shape) > 0 else 0

                # ğŸ”¥ ë””ë²„ê±°ë¡œ í•™ìŠµ ë°ì´í„° í†µê³„ ë¡œê¹…
                if self.debug:
                    try:
                        self.debug.log_training_data_stats(
                            states=np.array(states_jax),
                            actions=actions,
                            rewards=rewards,
                            advantages=np.array(advantages)
                        )
                    except Exception as stats_err:
                        logger.debug(f"âš ï¸ í•™ìŠµ ë°ì´í„° í†µê³„ ë¡œê¹… ì‹¤íŒ¨ (ë¬´ì‹œ): {stats_err}")

                # ê¸°ì¡´ ë¡œê¹… ìœ ì§€
                logger.info(f"ğŸ” í•™ìŠµ ë°ì´í„° ê²€ì¦ (ë°°ì¹˜ í¬ê¸°: {actual_batch_size}):")
                logger.info(f"   - States shape: {states_jax.shape}, dtype: {states_jax.dtype}")
                logger.info(f"   - Actions: {dict(zip(*np.unique(actions, return_counts=True)))}")
                logger.info(f"   - Rewards ë²”ìœ„: [{np.min(rewards):.4f}, {np.max(rewards):.4f}], í‰ê· : {np.mean(rewards):.4f}")
                logger.info(f"   - Returns ë²”ìœ„: [{jnp.min(returns):.4f}, {jnp.max(returns):.4f}], í‰ê· : {jnp.mean(returns):.4f}")
                logger.info(f"   - Advantages ë²”ìœ„: [{jnp.min(advantages):.4f}, {jnp.max(advantages):.4f}], í‰ê· : {jnp.mean(advantages):.4f}")
                self._first_batch_logged = True

            return loss_float
        
        except Exception as e:
            logger.error(f"âŒ ì •ì±… ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
            import traceback
            logger.debug(f"ìƒì„¸ ì—ëŸ¬:\n{traceback.format_exc()}")
            return 0.0
    
    def _evaluate_model(self, epoch: int, sample_experiences: List[Dict]) -> Dict[str, Any]:
        """
        ëª¨ë¸ í‰ê°€
        
        Args:
            epoch: í˜„ì¬ ì—í­
            sample_experiences: ìƒ˜í”Œ ê²½í—˜ ë°ì´í„°
        
        Returns:
            í‰ê°€ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        try:
            rewards = [exp.get('reward', 0.0) for exp in sample_experiences]
            
            return {
                'epoch': epoch,
                'avg_reward': np.mean(rewards) if rewards else 0.0,
                'std_reward': np.std(rewards) if rewards else 0.0,
                'sample_size': len(sample_experiences)
            }
        
        except Exception as e:
            logger.warning(f"âš ï¸ ëª¨ë¸ í‰ê°€ ì‹¤íŒ¨: {e}")
            return {'epoch': epoch, 'avg_reward': 0.0}
    
    def _save_model(self, db_path: Optional[str] = None) -> str:
        """
        ëª¨ë¸ ì €ì¥
        
        Args:
            db_path: DB ê²½ë¡œ (Noneì´ë©´ ì„¤ì • íŒŒì¼ì—ì„œ ê°€ì ¸ì˜´)
        
        Returns:
            model_id: ì €ì¥ëœ ëª¨ë¸ ID
        """
        try:
            import uuid
            from datetime import datetime
            
            # ëª¨ë¸ ID ìƒì„±
            model_id = f"ppo_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{uuid.uuid4().hex[:8]}"
            
            # ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ
            checkpoint_dir = self.config.get('paths', {}).get('checkpoints', '/workspace/rl_pipeline/artifacts/checkpoints')
            ckpt_path = os.path.join(checkpoint_dir, f"{model_id}.ckpt")
            
            # ì²´í¬í¬ì¸íŠ¸ ì €ì¥
            save_ckpt(self.model, ckpt_path)
            
            # DBì— ê¸°ë¡
            if db_path:
                self._save_to_db(model_id, ckpt_path, db_path)
            
            logger.info(f"âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {model_id}")
            return model_id
            
        except Exception as e:
            logger.error(f"âŒ ëª¨ë¸ ì €ì¥ ì‹¤íŒ¨: {e}")
            raise
    
    def _save_to_db(self, model_id: str, ckpt_path: str, db_path: str):
        """
        ëª¨ë¸ ì •ë³´ë¥¼ DBì— ì €ì¥
        
        Args:
            model_id: ëª¨ë¸ ID
            ckpt_path: ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ
            db_path: DB ê²½ë¡œ
        """
        try:
            from rl_pipeline.db.writes import write_batch
            from rl_pipeline.hybrid.features import FEATURES_VERSION
            
            model_record = {
                'model_id': model_id,
                'algo': 'PPO',
                'features_ver': FEATURES_VERSION,
                'created_at': datetime.now().isoformat(),
                'ckpt_path': ckpt_path,
                'notes': json.dumps({
                    'hidden_dim': self.model['hidden_dim'],
                    'obs_dim': self.model['obs_dim'],
                    'action_dim': self.model['action_dim']
                })
            }
            
            write_batch([model_record], 'policy_models', db_path=db_path)
            
        except Exception as e:
            logger.warning(f"âš ï¸ DB ì €ì¥ ì‹¤íŒ¨ (ê³„ì† ì§„í–‰): {e}")


def _compute_returns(rewards: jnp.ndarray, gamma: float) -> jnp.ndarray:
    """
    Discounted returns ê³„ì‚° (ì—­ìˆœìœ¼ë¡œ ëˆ„ì )
    
    Args:
        rewards: ë³´ìƒ ë°°ì—´
        gamma: í• ì¸ìœ¨
    
    Returns:
        Discounted returns ë°°ì—´
    """
    if not JAX_AVAILABLE:
        return np.zeros_like(rewards)
    
    try:
        # ğŸ”§ JAX ì•ˆì „ ë°©ì‹: NumPyë¡œ ë¨¼ì € ê³„ì‚° í›„ ë³€í™˜
        rewards_np = np.array(rewards, dtype=np.float32)
        returns_np = np.zeros_like(rewards_np, dtype=np.float32)
        running_return = 0.0
        
        # ì—­ìˆœìœ¼ë¡œ ê³„ì‚°
        for i in range(len(rewards_np) - 1, -1, -1):
            running_return = float(rewards_np[i]) + gamma * running_return
            returns_np[i] = running_return
        
        # NaN/Inf ì²´í¬ ë° ì²˜ë¦¬
        returns_np = np.nan_to_num(returns_np, nan=0.0, posinf=1e6, neginf=-1e6)
        
        # JAX ë°°ì—´ë¡œ ë³€í™˜
        returns_jax = jnp.array(returns_np, dtype=jnp.float32)
        
        return returns_jax
    except Exception as e:
        logger.warning(f"âš ï¸ Returns ê³„ì‚° ì‹¤íŒ¨, 0 ë°˜í™˜: {e}")
        return jnp.zeros_like(rewards)


def _compute_gae(
    rewards: jnp.ndarray,
    values: jnp.ndarray,
    gamma: float,
    lam: float
) -> jnp.ndarray:
    """
    GAE (Generalized Advantage Estimation) ê³„ì‚°
    
    Args:
        rewards: ë³´ìƒ ë°°ì—´
        values: ê°€ì¹˜ ì¶”ì • ë°°ì—´
        gamma: í• ì¸ìœ¨
        lam: GAE lambda (0~1)
    
    Returns:
        GAE advantages ë°°ì—´
    """
    if not JAX_AVAILABLE:
        return np.zeros_like(rewards)
    
    try:
        # ğŸ”§ JAX ì•ˆì „ ë°©ì‹: NumPyë¡œ ë¨¼ì € ê³„ì‚° í›„ ë³€í™˜
        rewards_np = np.array(rewards, dtype=np.float32)
        values_np = np.array(values, dtype=np.float32)
        batch_size = len(rewards_np)
        
        advantages_np = np.zeros_like(rewards_np, dtype=np.float32)
        
        # ë§ˆì§€ë§‰ valueëŠ” 0ìœ¼ë¡œ ê°€ì • (ì—í”¼ì†Œë“œ ì¢…ë£Œ)
        last_gae = 0.0
        
        # ì—­ìˆœìœ¼ë¡œ GAE ê³„ì‚°
        for i in range(batch_size - 1, -1, -1):
            if i == batch_size - 1:
                next_value = 0.0  # ë§ˆì§€ë§‰ ìŠ¤í…
            else:
                next_value = float(values_np[i + 1])
            
            # TD residual: Î´_t = r_t + Î³ * V(s_{t+1}) - V(s_t)
            reward_val = float(rewards_np[i])
            value_val = float(values_np[i])
            
            delta = reward_val + gamma * next_value - value_val
            
            # GAE: A_t = Î´_t + (Î³Î») * A_{t+1}
            last_gae = delta + gamma * lam * last_gae
            advantages_np[i] = last_gae
        
        # NaN/Inf ì²´í¬ ë° ì²˜ë¦¬
        advantages_np = np.nan_to_num(advantages_np, nan=0.0, posinf=1e6, neginf=-1e6)
        
        # ê°’ ë²”ìœ„ í´ë¦¬í•‘ (ê³¼ë„í•œ ê°’ ë°©ì§€)
        advantages_np = np.clip(advantages_np, -100.0, 100.0)
        
        # JAX ë°°ì—´ë¡œ ë³€í™˜
        advantages_jax = jnp.array(advantages_np, dtype=jnp.float32)
        
        return advantages_jax
    except Exception as e:
        logger.warning(f"âš ï¸ GAE ê³„ì‚° ì‹¤íŒ¨, 0 ë°˜í™˜: {e}")
        return jnp.zeros_like(rewards)


def train(config_path: str, db_path: Optional[str] = None) -> str:
    """
    í•™ìŠµ ì‹¤í–‰ í•¨ìˆ˜
    
    Args:
        config_path: ì„¤ì • íŒŒì¼ ê²½ë¡œ
        db_path: DB ê²½ë¡œ (ì„ íƒì )
    
    Returns:
        model_id: í•™ìŠµëœ ëª¨ë¸ ID
    """
    try:
        # ì„¤ì • íŒŒì¼ ë¡œë“œ
        with open(config_path, 'r') as f:
            config = json.load(f)
        
        # DB ê²½ë¡œ ì„¤ì •
        if db_path is None:
            db_path = config.get('paths', {}).get('db')
        
        # Trainer ì´ˆê¸°í™”
        trainer = PPOTrainer(config)
        
        # Self-play ë°ì´í„°ëŠ” íŒŒë¼ë¯¸í„°ë¡œ ë°›ì•„ì•¼ í•¨
        # ì´ í•¨ìˆ˜ëŠ” ì§ì ‘ í˜¸ì¶œë˜ì§€ ì•Šê³  train_from_selfplay_dataë¥¼ í†µí•´ í˜¸ì¶œë¨
        raise NotImplementedError(
            "train() í•¨ìˆ˜ëŠ” ì§ì ‘ í˜¸ì¶œí•˜ì§€ ë§ˆì„¸ìš”. "
            "ëŒ€ì‹  PPOTrainer.train_from_selfplay_data()ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”. "
            "ë˜ëŠ” auto_train_from_selfplay() ë˜ëŠ” auto_train_from_integrated_analysis()ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”."
        )
        
    except Exception as e:
        logger.error(f"âŒ í•™ìŠµ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        raise

