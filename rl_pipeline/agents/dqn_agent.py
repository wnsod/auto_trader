"""
DQN (Deep Q-Network) Agent
ì§„ì§œ ê°•í™”í•™ìŠµ êµ¬í˜„
"""

import jax
import jax.numpy as jnp
from jax import random, jit, grad
import flax
import flax.linen as nn
import optax
import numpy as np
from typing import Dict, List, Tuple, Any
from dataclasses import dataclass
import logging

logger = logging.getLogger(__name__)


@dataclass
class DQNConfig:
    """DQN ì„¤ì •"""
    state_dim: int = 20  # ìƒíƒœ ì°¨ì› (RSI, MACD ë“±)
    action_dim: int = 3  # í–‰ë™ ê°œìˆ˜ (HOLD, BUY, SELL)
    hidden_dims: List[int] = None  # ì€ë‹‰ì¸µ ì°¨ì›
    learning_rate: float = 0.001
    gamma: float = 0.99  # í• ì¸ ê³„ìˆ˜
    epsilon_start: float = 1.0  # íƒí—˜ ì‹œì‘ í™•ë¥ 
    epsilon_end: float = 0.01  # íƒí—˜ ì¢…ë£Œ í™•ë¥ 
    epsilon_decay: float = 0.995  # íƒí—˜ ê°ì†Œìœ¨
    batch_size: int = 64
    buffer_size: int = 10000
    target_update_freq: int = 100  # íƒ€ê²Ÿ ë„¤íŠ¸ì›Œí¬ ì—…ë°ì´íŠ¸ ë¹ˆë„

    def __post_init__(self):
        if self.hidden_dims is None:
            self.hidden_dims = [128, 64, 32]


class QNetwork(nn.Module):
    """Q-Network (ê°€ì¹˜ í•¨ìˆ˜ ê·¼ì‚¬)"""
    hidden_dims: List[int]
    action_dim: int
    training: bool = False  # í•™ìŠµ ëª¨ë“œ ì—¬ë¶€

    @nn.compact
    def __call__(self, x, training: bool = False):
        """
        State â†’ Q-values for each action

        Args:
            x: State vector [batch, state_dim]
            training: í•™ìŠµ ëª¨ë“œ (Dropout í™œì„±í™”)
        Returns:
            Q-values [batch, action_dim]
        """
        for hidden_dim in self.hidden_dims:
            x = nn.Dense(hidden_dim)(x)
            x = nn.relu(x)
            # Dropoutì€ í•™ìŠµ ì‹œì—ë§Œ
            if training:
                x = nn.Dropout(0.2)(x, deterministic=False)

        # ì¶œë ¥: ê° í–‰ë™ì˜ Q-value
        q_values = nn.Dense(self.action_dim)(x)
        return q_values


class DQNAgent:
    """DQN ì—ì´ì „íŠ¸ - ì§„ì§œ ê°•í™”í•™ìŠµ!"""

    def __init__(self, config: DQNConfig, seed: int = 42):
        self.config = config
        self.rng = random.PRNGKey(seed)

        # Q-Network ì´ˆê¸°í™”
        self.q_network = QNetwork(
            hidden_dims=config.hidden_dims,
            action_dim=config.action_dim
        )

        # ë”ë¯¸ ì…ë ¥ìœ¼ë¡œ íŒŒë¼ë¯¸í„° ì´ˆê¸°í™”
        dummy_state = jnp.zeros((1, config.state_dim))
        self.rng, init_rng = random.split(self.rng)
        self.params = self.q_network.init(init_rng, dummy_state)

        # Target Network (ì•ˆì •ì ì¸ í•™ìŠµì„ ìœ„í•´)
        self.target_params = self.params

        # Optimizer
        self.optimizer = optax.adam(config.learning_rate)
        self.opt_state = self.optimizer.init(self.params)

        # íƒí—˜ í™•ë¥ 
        self.epsilon = config.epsilon_start

        # í•™ìŠµ ìŠ¤í… ì¹´ìš´í„°
        self.train_steps_count = 0

        logger.info(f"âœ… DQN Agent ì´ˆê¸°í™” ì™„ë£Œ")
        logger.info(f"   State dim: {config.state_dim}")
        logger.info(f"   Action dim: {config.action_dim}")
        logger.info(f"   Hidden dims: {config.hidden_dims}")

    def select_action(self, state: np.ndarray, training: bool = True) -> int:
        """
        Epsilon-greedy ì •ì±…ìœ¼ë¡œ í–‰ë™ ì„ íƒ

        Args:
            state: í˜„ì¬ ìƒíƒœ [state_dim]
            training: í•™ìŠµ ëª¨ë“œ ì—¬ë¶€
        Returns:
            action: ì„ íƒëœ í–‰ë™ (0=HOLD, 1=BUY, 2=SELL)
        """
        if training and np.random.random() < self.epsilon:
            # íƒí—˜: ëœë¤ í–‰ë™
            return np.random.randint(0, self.config.action_dim)

        # í™œìš©: Q-valueê°€ ê°€ì¥ ë†’ì€ í–‰ë™
        state_batch = jnp.array([state])  # [1, state_dim]
        q_values = self.q_network.apply(self.params, state_batch, training=False)[0]  # [action_dim]

        return int(jnp.argmax(q_values))

    def get_q_values(self, state: np.ndarray) -> np.ndarray:
        """
        í˜„ì¬ ìƒíƒœì˜ Q-values ë°˜í™˜

        Args:
            state: í˜„ì¬ ìƒíƒœ [state_dim]
        Returns:
            q_values: [action_dim]
        """
        state_batch = jnp.array([state])
        q_values = self.q_network.apply(self.params, state_batch, training=False)[0]
        return np.array(q_values)

    def _loss_fn(self, params, target_params, states, actions, rewards, next_states, dones, gamma, rng):
        """
        DQN Loss ê³„ì‚° (Bellman Equation)

        Loss = (Q(s,a) - (r + Î³ * max_a' Q_target(s',a')))^2
        """
        # í˜„ì¬ Q-values (í•™ìŠµ ëª¨ë“œ, RNG ì „ë‹¬)
        q_values = self.q_network.apply(params, states, training=True, rngs={'dropout': rng})  # [batch, action_dim]
        q_values_selected = jnp.take_along_axis(
            q_values,
            actions[:, None],
            axis=1
        ).squeeze()  # [batch]

        # íƒ€ê²Ÿ Q-values (ë‹¤ìŒ ìƒíƒœ, ì¶”ë¡  ëª¨ë“œ)
        next_q_values = self.q_network.apply(target_params, next_states, training=False)  # [batch, action_dim]
        next_q_max = jnp.max(next_q_values, axis=1)  # [batch]

        # TD Target
        targets = rewards + gamma * next_q_max * (1 - dones)

        # MSE Loss
        loss = jnp.mean((q_values_selected - targets) ** 2)

        return loss

    def train_step(self, batch: Dict[str, np.ndarray]) -> float:
        """
        í•œ ë²ˆì˜ í•™ìŠµ ìŠ¤í…

        Args:
            batch: {
                'states': [batch, state_dim],
                'actions': [batch],
                'rewards': [batch],
                'next_states': [batch, state_dim],
                'dones': [batch]
            }
        Returns:
            loss: í•™ìŠµ ì†ì‹¤
        """
        # NumPy â†’ JAX
        states = jnp.array(batch['states'])
        actions = jnp.array(batch['actions'], dtype=jnp.int32)
        rewards = jnp.array(batch['rewards'])
        next_states = jnp.array(batch['next_states'])
        dones = jnp.array(batch['dones'], dtype=jnp.float32)

        # RNG ìƒì„± (Dropoutìš©)
        self.rng, dropout_rng = random.split(self.rng)

        # Gradient ê³„ì‚°
        loss, grads = jax.value_and_grad(self._loss_fn)(
            self.params,
            self.target_params,
            states,
            actions,
            rewards,
            next_states,
            dones,
            self.config.gamma,
            dropout_rng
        )

        # Optimizer ì—…ë°ì´íŠ¸
        updates, self.opt_state = self.optimizer.update(grads, self.opt_state)
        self.params = optax.apply_updates(self.params, updates)

        # Epsilon ê°ì†Œ (íƒí—˜ ì¤„ì´ê¸°)
        self.epsilon = max(
            self.config.epsilon_end,
            self.epsilon * self.config.epsilon_decay
        )

        # Target Network ì—…ë°ì´íŠ¸
        self.train_steps_count += 1
        if self.train_steps_count % self.config.target_update_freq == 0:
            self.target_params = self.params
            logger.debug(f"ğŸ”„ Target network ì—…ë°ì´íŠ¸ (step {self.train_steps_count})")

        return float(loss)

    def save(self, path: str):
        """ì •ì±… ì €ì¥"""
        import pickle
        save_dict = {
            'params': self.params,
            'target_params': self.target_params,
            'opt_state': self.opt_state,
            'epsilon': self.epsilon,
            'train_steps_count': self.train_steps_count,
            'config': self.config
        }
        with open(path, 'wb') as f:
            pickle.dump(save_dict, f)
        logger.info(f"âœ… DQN Agent ì €ì¥: {path}")

    def load(self, path: str):
        """ì •ì±… ë¡œë“œ"""
        import pickle
        with open(path, 'rb') as f:
            save_dict = pickle.load(f)

        self.params = save_dict['params']
        self.target_params = save_dict['target_params']
        self.opt_state = save_dict['opt_state']
        self.epsilon = save_dict['epsilon']
        self.train_steps_count = save_dict.get('train_steps_count', save_dict.get('train_step', 0))
        logger.info(f"âœ… DQN Agent ë¡œë“œ: {path}")
        logger.info(f"   Train step: {self.train_steps_count}, Epsilon: {self.epsilon:.4f}")


if __name__ == "__main__":
    # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸
    logging.basicConfig(level=logging.INFO)

    config = DQNConfig(state_dim=20, action_dim=3)
    agent = DQNAgent(config)

    # ë”ë¯¸ ìƒíƒœë¡œ í…ŒìŠ¤íŠ¸
    state = np.random.randn(20)
    action = agent.select_action(state, training=True)
    q_values = agent.get_q_values(state)

    print(f"State: {state[:5]}...")
    print(f"Action: {action} (0=HOLD, 1=BUY, 2=SELL)")
    print(f"Q-values: {q_values}")

    # í•™ìŠµ í…ŒìŠ¤íŠ¸
    batch = {
        'states': np.random.randn(64, 20),
        'actions': np.random.randint(0, 3, 64),
        'rewards': np.random.randn(64),
        'next_states': np.random.randn(64, 20),
        'dones': np.random.randint(0, 2, 64)
    }

    loss = agent.train_step(batch)
    print(f"Training loss: {loss:.4f}")
    print("âœ… DQN Agent í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
